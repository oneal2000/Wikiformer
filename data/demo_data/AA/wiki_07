<doc id="1178" url="https://en.wikipedia.org/wiki?curid=1178" title="Afterlife">
Afterlife

The afterlife (also referred to as life after death or the world to come) is a purported existence in which the essential part of an individual's identity or their stream of consciousness continues to live after the death of their physical body. According to various ideas about the afterlife, the essential aspect of the individual that lives on after death may be some partial element, or the entire soul or spirit, of an individual, which carries with it and may confer personal identity or, on the contrary nirvana. Belief in an afterlife is in contrast to the belief in oblivion after death.
In some views, this continued existence takes place in a spiritual realm, and in other popular views, the individual may be reborn into this world and begin the life cycle over again, likely with no memory of what they have done in the past. In this latter view, such rebirths and deaths may take place over and over again continuously until the individual gains entry to a spiritual realm or otherworld. Major views on the afterlife derive from religion, esotericism and metaphysics.
Some belief systems, such as those in the Abrahamic tradition, hold that the dead go to a specific plane of existence after death, as determined by God, or other divine judgment, based on their actions or beliefs during life. In contrast, in systems of reincarnation, such as those in the Indian religions, the nature of the continued existence is determined directly by the actions of the individual in the ended life.
## Different metaphysical models.
Theists generally believe some afterlife awaits people when they die. Members of some generally non-theistic religions tend to believe in an afterlife but without reference to a deity. The Sadducees were an ancient Jewish sect that generally believed that there was a God but no existence after death.
Many religions, whether they believe in the soul's existence in another world like Christianity, Islam, and many pagan belief systems, or reincarnation like many forms of Hinduism and Buddhism, believe that one's status in the afterlife is a consequence of one's conduct during life.
### Reincarnation.
Reincarnation is the philosophical or religious concept that an aspect of a living being starts a new life in a different physical body or form after each death. It is also called rebirth or transmigration and is a part of the Saṃsāra doctrine of cyclic existence. It is a central tenet of all major Indian religions, namely Buddhism, Hinduism, Jainism, and Sikhism. The idea of reincarnation is found in many ancient cultures, and a belief in rebirth/metempsychosis was held by historic Greek figures, such as Pythagoras, Socrates, and Plato. It is also a common belief of various ancient and modern religions such as Spiritism, Theosophy, and Eckankar. It is found as well in many tribal societies around the world, in places such as Australia, East Asia, Siberia, and South America.
Although the majority of denominations within the Abrahamic religions of Judaism, Christianity, and Islam do not believe that individuals reincarnate, particular groups within these religions do refer to reincarnation; these groups include the mainstream historical and contemporary followers of Kabbalah, the Cathars, Alawites, the Druze, and the Rosicrucians. The historical relations between these sects and the beliefs about reincarnation that were characteristic of Neoplatonism, Orphism, Hermeticism, Manicheanism, and Gnosticism of the Roman era as well as the Indian religions have been the subject of recent scholarly research. Unity Church and its founder Charles Fillmore teach reincarnation.
Rosicrucians speak of a life review period occurring immediately after death and before entering the afterlife's planes of existence (before the silver cord is broken), followed by a judgment, more akin to a final review or end report over one's life.
### Heaven and Hell.
Heaven, the heavens, Seven Heavens, pure lands, Tian, Jannah, Valhalla, or the Summerland, is a common religious, cosmological, or transcendent place where beings such as gods, angels, jinn, saints, or venerated ancestors are said to originate, be enthroned, or live. According to the beliefs of some religions, heavenly beings can descend to earth or incarnate, and earthly beings can ascend to heaven in the afterlife, or in exceptional cases enter heaven alive.
Heaven is often described as a "higher place", the holiest place, a paradise, in contrast to hell or the underworld or the "low places", and universally or conditionally accessible by earthly beings according to various standards of divinity, goodness, piety, faith or other virtues or right beliefs or simply the will of God. Some believe in the possibility of a heaven on Earth in a world to come.
In Hinduism, heaven is considered as "Svarga loka". There are seven positive regions the soul can go to after death and seven negative regions. After completing its stay in the respective region, the soul is subjected to rebirth in different living forms according to its "karma". This cycle can be broken after a soul achieves "Moksha" or "Nirvana". Any place of existence, either of humans, souls or deities, outside the tangible world (heaven, hell, or other) is referred to as otherworld.
Hell, in many religious and folkloric traditions, is a place of torment and punishment in the afterlife. Religions with a linear divine history often depict hell as an eternal destination, while religions with a cyclic history often depict a hell as an intermediary period between incarnations. Typically, these traditions locate hell in another dimension or under the earth's surface and often include entrances to hell from the land of the living. Other afterlife destinations include purgatory and limbo.
Traditions that do not conceive of the afterlife as a place of punishment or reward merely describe hell as an abode of the dead, the grave, a neutral place (for example, Sheol or Hades) located under the surface of earth.
## Ancient religions.
### Ancient Egyptian religion.
The afterlife played an important role in Ancient Egyptian religion, and its belief system is one of the earliest known in recorded history. When the body died, parts of its soul known as "ka" (body double) and the "ba" (personality) would go to the Kingdom of the Dead. While the soul dwelt in the Fields of Aaru, Osiris demanded work as restitution for the protection he provided. Statues were placed in the tombs to serve as substitutes for the deceased.
Arriving at one's reward in afterlife was a demanding ordeal, requiring a sin-free heart and the ability to recite the spells, passwords and formulae of the Book of the Dead. In the Hall of Two Truths, the deceased's heart was weighed against the "Shu" feather of truth and justice taken from the headdress of the goddess Ma'at. If the heart was lighter than the feather, they could pass on, but if it were heavier they would be devoured by the demon Ammit.
Egyptians also believed that being mummified and put in a sarcophagus (an ancient Egyptian "coffin" carved with complex symbols and designs, as well as pictures and hieroglyphs) was the only way to have an afterlife. What are referred to as the Coffin Texts, are inscribed on a coffin and serve as a guide for the challenges in the afterlife. The Coffin texts are more or less a duplication of the Pyramid Texts, which would serve as a guide for Egyptian pharaohs or queens in the afterlife. Only if the corpse had been properly embalmed and entombed in a mastaba, could the dead live again in the Fields of Yalu and accompany the Sun on its daily ride. Due to the dangers the afterlife posed, the Book of the Dead was placed in the tomb with the body as well as food, jewelry, and 'curses'. They also used the "opening of the mouth". 
Ancient Egyptian civilization was based on religion; their belief in the rebirth after death became the driving force behind their funeral practices. Death was simply a temporary interruption, rather than complete cessation, of life, and that eternal life could be ensured by means like piety to the gods, preservation of the physical form through mummification, and the provision of statuary and other funerary equipment. Each human consisted of the physical body, the "ka", the "ba", and the "akh". The Name and Shadow were also living entities. To enjoy the afterlife, all these elements had to be sustained and protected from harm.
On 30 March 2010, a spokesman for the Egyptian Culture Ministry claimed it had unearthed a large red granite door in Luxor with inscriptions by User, a powerful adviser to the 18th Dynasty Queen Hatshepsut who ruled between 1479 BC and 1458 BC, the longest of any woman. It believes the false door is a 'door to the Afterlife'. According to the archaeologists, the door was reused in a structure in Roman Egypt.
### Ancient Greek and Roman religions.
The Greek god Hades is known in Greek mythology as the king of the underworld, a place where souls live after death. The Greek god Hermes, the messenger of the gods, would take the dead soul of a person to the underworld (sometimes called Hades or the House of Hades). Hermes would leave the soul on the banks of the River Styx, the river between life and death.
Charon, also known as the ferry-man, would take the soul across the river to Hades, if the soul had gold: Upon burial, the family of the dead soul would put coins under the deceased's tongue. Once crossed, the soul would be judged by Aeacus, Rhadamanthus and King Minos. The soul would be sent to Elysium, Tartarus, or Asphodel Fields. The Elysian Fields were for the ones that lived pure lives. It consisted of green fields, valleys and mountains, everyone there was peaceful and contented, and the Sun always shone there. Tartarus was for the people that blasphemed against the gods, or were simply rebellious and consciously evil.
The Asphodel Fields were for a varied selection of human souls: Those whose sins equalled their goodness, were indecisive in their lives, or were not judged. Those who had sinned went to the deepest pit, Tartarus. In Tartarus, the soul would be punished by being burned in lava, or stretched on racks. Some heroes of Greek legend are allowed to visit the underworld. The Romans had a similar belief system about the afterlife, with Hades becoming known as Pluto. In the ancient Greek myth about the Labours of Heracles, the hero Heracles had to travel to the underworld to capture Cerberus, the three-headed guard dog, as one of his tasks.
In "Dream of Scipio", Cicero describes what seems to be an out of body experience, of the soul traveling high above the Earth, looking down at the small planet, from far away.
In Book VI of Virgil's "Aeneid", the hero, Aeneas, travels to the underworld to see his father. By the River Styx, he sees the souls of those not given a proper burial, forced to wait by the river until someone buries them. While down there, along with the dead, he is shown the place where the wrongly convicted reside, the fields of sorrow where those who committed suicide and now regret it reside, including Aeneas' former lover, the warriors and shades, Tartarus (where the titans and powerful non-mortal enemies of the Olympians reside) where he can hear the groans of the imprisoned, the palace of Pluto, and the fields of Elysium where the descendants of the divine and bravest heroes reside. He sees the river of forgetfulness, Lethe, which the dead must drink to forget their life and begin anew. Lastly, his father shows him all of the future heroes of Rome who will live if Aeneas fulfills his destiny in founding the city.
### Norse religion.
The Poetic and Prose Eddas, the oldest sources for information on the Norse concept of the afterlife, vary in their description of the several realms that are described as falling under this topic. The most well-known are:
## Abrahamic religions.
### Baháʼí Faith.
The teachings of the Baháʼí Faith state that the nature of the afterlife is beyond the understanding of those living, just as an unborn fetus cannot understand the nature of the world outside of the womb. The Baháʼí writings state that the soul is immortal and after death it will continue to progress until it finally attains God's presence. In Baháʼí belief, souls in the afterlife will continue to retain their individuality and consciousness and will be able to recognize and communicate spiritually with other souls whom they have made deep profound friendships with, such as their spouses.
The Baháʼí scriptures also state there are distinctions between souls in the afterlife, and that souls will recognize the worth of their own deeds and understand the consequences of their actions. It is explained that those souls that have turned toward God will experience gladness, while those who have lived in error will become aware of the opportunities they have lost. Also, in the Baháʼí view, souls will be able to recognize the accomplishments of the souls that have reached the same level as themselves, but not those that have achieved a rank higher than them.
### Christianity.
Mainstream Christianity professes belief in the Nicene Creed, and English versions of the Nicene Creed in current use include the phrase: "We look for the resurrection of the dead, and the life of the world to come."
When questioned by the Sadducees about the resurrection of the dead (in a context relating to who one's spouse would be if one had been married several times in life), Jesus said that marriage will be irrelevant after the resurrection as the resurrected will be like the angels in heaven.
Jesus also maintained that the time would come when the dead would hear the voice of the Son of God, and all who were in the tombs would come out; those who have heard His "[commandments] and believes in the one who sent [Him]" to the resurrection of life, but those who do not to the resurrection of condemnation.
The Book of Enoch describes Sheol as divided into four compartments for four types of the dead: the faithful saints who await resurrection in Paradise, the merely virtuous who await their reward, the wicked who await punishment, and the wicked who have already been punished and will not be resurrected on Judgment Day. The Book of Enoch is considered apocryphal by most denominations of Christianity and all denominations of Judaism.
The book of 2 Maccabees gives a clear account of the dead awaiting a future resurrection and judgment, plus prayers and offerings for the dead to remove the burden of sin.
The author of Luke recounts the story of Lazarus and the rich man, which shows people in Hades awaiting the resurrection either in comfort or torment. The author of the Book of Revelation writes about God and the angels versus Satan and demons in an epic battle at the end of times when all souls are judged. There is mention of ghostly bodies of past prophets, and the transfiguration.
The non-canonical Acts of Paul and Thecla speak of the efficacy of prayer for the dead, so that they might be "translated to a state of happiness".
Hippolytus of Rome pictures the underworld (Hades) as a place where the righteous dead, awaiting in the bosom of Abraham their resurrection, rejoice at their future prospect, while the unrighteous are tormented at the sight of the "lake of unquenchable fire" into which they are destined to be cast.
Gregory of Nyssa discusses the long-before believed possibility of purification of souls after death.
Pope Gregory I repeats the concept, articulated over a century earlier by Gregory of Nyssa that the saved suffer purification after death, in connection with which he wrote of "purgatorial flames".
The noun "purgatorium" (Latin: place of cleansing) is used for the first time to describe a state of painful purification of the saved after life. The same word in adjectival form ("purgatorius -a -um", cleansing), which appears also in non-religious writing, was already used by Christians such as Augustine of Hippo and Pope Gregory I to refer to an after-death cleansing.
During the Age of Enlightenment, theologians and philosophers presented various philosophies and beliefs. A notable example is Emanuel Swedenborg who wrote some 18 theological works which describe in detail the nature of the afterlife according to his claimed spiritual experiences, the most famous of which is "Heaven and Hell". His report of life there covers a wide range of topics, such as marriage in heaven (where all angels are married), children in heaven (where they are raised by angel parents), time and space in heaven (there are none), the after-death awakening process in the World of Spirits (a place halfway between Heaven and Hell and where people first wake up after death), the allowance of a free will choice between Heaven or Hell (as opposed to being sent to either one by God), the eternity of Hell (one could leave but would never want to), and that all angels or devils were once people on earth.
#### The Catholic Church.
The "Spiritual Combat", a written work by Lorenzo Scupoli, states that four assaults are attempted by the "evil one" at the hour of death. The Catholic conception of the afterlife teaches that after the body dies, the soul is judged, the righteous and free of sin enter Heaven. However, those who die in unrepented mortal sin go to hell. In the 1990s, the Catechism of the Catholic Church defined hell not as punishment imposed on the sinner but rather as the sinner's self-exclusion from God. Unlike other Christian groups, the Catholic Church teaches that those who die in a state of grace, but still carry venial sin go to a place called Purgatory where they undergo purification to enter Heaven.
#### Limbo.
Despite popular opinion, Limbo, which was elaborated upon by theologians beginning in the Middle Ages, was never recognized as a dogma of the Catholic Church, yet, at times, it has been a very popular theological theory within the Church. Limbo is a theory that unbaptized but innocent souls, such as those of infants, virtuous individuals who lived before Jesus Christ was born on earth, or those that die before baptism exist in neither Heaven or Hell proper. Therefore, these souls neither merit the beatific vision, nor are subjected to any punishment, because they are not guilty of any personal sin although they have not received baptism, so still bear original sin. So they are generally seen as existing in a state of natural, but not supernatural, happiness, until the end of time.
In other Christian denominations it has been described as an intermediate place or state of confinement in oblivion and neglect.
#### Purgatory.
The notion of purgatory is associated particularly with the Catholic Church. In the Catholic Church, all those who die in God's grace and friendship, but still imperfectly purified, are indeed assured of their eternal salvation; but after death they undergo purification, so as to achieve the holiness necessary to enter the joy of heaven or the final purification of the elect, which is entirely different from the punishment of the damned. The tradition of the church, by reference to certain texts of scripture, speaks of a "cleansing fire" although it is not always called purgatory.
Anglicans of the Anglo-Catholic tradition generally also hold to the belief. John Wesley, the founder of Methodism, believed in an intermediate state between death and the resurrection of the dead and in the possibility of "continuing to grow in holiness there", but Methodism does not officially affirm this belief and denies the possibility of helping by prayer any who may be in that state.
#### Orthodox Christianity.
The Orthodox Church is intentionally reticent on the afterlife, as it acknowledges the mystery especially of things that have not yet occurred. Beyond the second coming of Jesus, bodily resurrection, and final judgment, all of which is affirmed in the Nicene Creed (325 CE), Orthodoxy does not teach much else in any definitive manner. Unlike Western forms of Christianity, however, Orthodoxy is traditionally non-dualist and does not teach that there are two separate literal locations of heaven and hell, but instead acknowledges that "the 'location' of one's final destiny—heaven or hell—as being figurative."
Instead, Orthodoxy teaches that the final judgment is simply one's uniform encounter with divine love and mercy, but this encounter is experienced multifariously depending on the extent to which one has been transformed, partaken of divinity, and is therefore compatible or incompatible with God. "The monadic, immutable, and ceaseless object of eschatological encounter is therefore the love and mercy of God, his glory which infuses the heavenly temple, and it is the subjective human reaction which engenders multiplicity or any division of experience." For instance, St. Isaac the Syrian observes that "those who are punished in Gehenna, are scourged by the scourge of love. ... The power of love works in two ways: it torments sinners ... [as] bitter regret. But love inebriates the souls of the sons of Heaven by its delectability." In this sense, the divine action is always, immutably, and uniformly love and if one experiences this love negatively, the experience is then one of self-condemnation because of free will rather than condemnation by God.
Orthodoxy therefore uses the description of Jesus' judgment in John 3:19–21 as their model: "19 And this is the judgment: the light has come into the world, and people loved the darkness rather than the light because their works were evil. 20 For everyone who does wicked things hates the light and does not come to the light, lest his works should be exposed. 21 But whoever does what is true comes to the light, so that it may be clearly seen that his works have been carried out in God." As a characteristically Orthodox understanding, then, Fr. Thomas Hopko writes, "[I]t is precisely the presence of God's mercy and love which cause the torment of the wicked. God does not punish; he forgives... . In a word, God has mercy on all, whether all like it or not. If we like it, it is paradise; if we do not, it is hell. Every knee will bend before the Lord. Everything will be subject to Him. God in Christ will indeed be "all and in all," with boundless mercy and unconditional pardon. But not all will rejoice in God's gift of forgiveness, and that choice will be judgment, the self-inflicted source of their sorrow and pain."
Moreover, Orthodoxy includes a prevalent tradition of "apokatastasis", or the restoration of all things in the end. This has been taught most notably by Origen, but also many other Church fathers and Saints, including Gregory of Nyssa. The Second Council of Constantinople (553 CE) affirmed the orthodoxy of Gregory of Nyssa while simultaneously condemning Origen's brand of universalism because it taught the restoration back to our pre-existent state, which Orthodoxy doesn't teach. It is also a teaching of such eminent Orthodox theologians as Olivier Clément, Metropolitan Kallistos Ware, and Bishop Hilarion Alfeyev. Although apokatastasis is not a dogma of the church but instead a theologoumenon, it is no less a teaching of the Orthodox Church than its rejection. As Met. Kallistos Ware explains, "It is heretical to say that all must be saved, for this is to deny free will; but, it is legitimate to hope that all may be saved," as insisting on torment without end also denies free will.
#### The Church of Jesus Christ of Latter-day Saints.
Joseph F. Smith of The Church of Jesus Christ of Latter-day Saints presents an elaborate vision of the afterlife. It is revealed as the scene of an extensive missionary effort by righteous spirits in paradise to redeem those still in darkness—a spirit prison or "hell" where the spirits of the dead remain until judgment. It is divided into two parts: Spirit Prison and Paradise. Together these are also known as the Spirit World (also Abraham's Bosom; see Luke 16:19–25). They believe that Christ visited spirit prison (1 Peter 3:18–20) and opened the gate for those who repent to cross over to Paradise. This is similar to the Harrowing of Hell doctrine of some mainstream Christian faiths. Both Spirit Prison and Paradise are temporary according to Latter-day Saint beliefs. After the resurrection, spirits are assigned "permanently" to three degrees of heavenly glory, determined by how they lived – Celestial, Terrestrial, and Telestial. (1 Cor 15:44–42; Doctrine and Covenants, Section 76) Sons of Perdition, or those who have known and seen God and deny it, will be sent to the realm of Satan, which is called Outer Darkness, where they shall live in misery and agony forever. However, according to Mormon faith, since most persons lack the amount of knowledge to commit the Eternal sin, they are incapable of becoming sons of perdition.
The Celestial Kingdom is believed to be a place where the righteous can live eternally with their families. Progression does not end once one has entered the Celestial Kingdom, but it extends eternally. According to "True to the Faith" (a handbook on doctrines in the LDS faith), "The celestial kingdom is the place prepared for those who have "received the testimony of Jesus" and been "made perfect through Jesus the mediator of the new covenant, who wrought out this perfect atonement through the shedding of his own blood" (Doctrine and Covenants, 76:51, 69). To inherit this gift, we must receive the ordinances of salvation, keep the commandments, and repent of our sins."
#### Jehovah's Witnesses.
Jehovah's Witnesses occasionally use terms such as "afterlife" to refer to any hope for the dead, but they understand Ecclesiastes 9:5 to preclude belief in an immortal soul. Individuals judged by God to be wicked, such as in the Great Flood or at Armageddon, are given no hope of an afterlife. However, they believe that after Armageddon there will be a bodily resurrection of "both righteous and unrighteous" dead (but not the "wicked"). Survivors of Armageddon and those who are resurrected are then to gradually restore earth to a paradise. After Armageddon, unrepentant sinners are punished with eternal death (non-existence).
#### Seventh-day Adventists.
The Seventh-day Adventist Church's beliefs regarding the afterlife differ from other Christian churches. Rather than ascend to Heaven or descend to Hell, Adventists believe the dead "remain unconscious until the return of Christ in judgement". The concept that the dead remain dead until resurrection is one of the fundamental beliefs of Seventh-day Adventist. Adventists believe that death is an unconscious state (a “sleep”). This is based on Matt. 9:24; Mark 5:39; John 11:11-14; 1 Cor. 15:51, 52; 1 Thess. 4:13-17; 2 Peter 3:4; Eccl. 9:5, 6, 10. At death, all consciousness ends. The dead person does not know anything and does not do anything. They believe that death is creation, only in reverse. Ecclesiastes 12:7. When a person dies, the body turns to dust again, and the spirit goes back to God, who gave it. The spirit of every person who dies—whether saved or unsaved—returns to God at death. The spirit that returns to God at death is the breath of life. 
### Islam.
The Islamic belief in the afterlife, called "al-ākhira" () and as stated in the Quran, is descriptive. Mankind (as well as the "jinn") is destined for either the "garden/gardens" ("janna/jannāt") for the righteous, or the "hellfire" ("an-nār") for the wicked, respectively also referred to as "Paradise" ("al-firdaws") and "Gehenna" or "Gehinnom" ("jahannam").
The level of comfort in the immediate afterlife of the "al-qabr" or "the grave" (compare the Jewish concept of Sheol), according to some commentators, depends wholly on their level of "iʾmān" or faith in God. In order for one to achieve proper and firm "iʾmān" one must have a righteous conduct, lest his level of faith dwindles and eventually withers away if one does not practice Islam long enough, and be led on the straight path ("aṣ-ṣirāṭ al-mustaqīm") which is walked on by those who have been divinely graced and not have incurred divine wrath. In the Quran, God warns of grievous punishment to those who do not believe in the afterlife, and admonishes mankind that hellfire is prepared for the disbelievers.
Islam teaches that the purpose of Man's creation is entirely to worship God alone, which includes being kind to other humans as well as animals and plants, by not oppressing them. The Quran repeatedly reminds the reader that the worldly life ("ḥayāt ad-dunyā") is nothing but a test and to determine each individual's ultimate abode, which is eternal and everlasting. Some Quranic verses describing the Islamic paradise refer to perpetually youthful attendants which inhabit it, and they are described as both male and female servants: the females are referred to as "ḥūr" whereas the males are referred to as "ghilmān", "wildān", and "suqāh". The slave boys are referred to in the Quran as "immortal boys" (, ) or "young men" () who serve wine and meals to the blessed.
In the 20th century, discussions about the afterlife address the interconnection between human action and divine judgment, the need for moral rectitude, and the eternal consequences of human action in this life and world.
A central doctrine of Islamic faith is the Last Day ("al-yawm al-ākhir"), on which the world will come to an end and God will raise all mankind (as well as the "jinn") from the dead and evaluate their worldly actions. The Last Day is also called the Encompassing Day ("al-yawm al-muḥīṭ"), more commonly known as the "Day of Resurrection" ("yawm al-qiyāma"), "Day of Judgment" ("yawm ad-dīn"), and "Day of Reckoning" ("yawm al-ḥisāb"), as well as both the "Day of Separation" ("yawm al-faṣl") and "Day of Gathering" ("yawm al-jamʿ"), and is also referred to as "as-Sāʿah", meaning "the Hour" signaled by the blowing of the horn/trumpet.
Until the Last Day, deceased souls remain in their graves awaiting the resurrection and judgment. However, they will begin to feel immediately a taste of their destiny to come. Those bound for hell will suffer in their graves, while those bound for heaven will be in peace until that time. 
"Jannah" and "Jahannam" both have different levels. "Jannah" has eight gates and eight levels. The higher the level the better it is and the happier you are. "Jahannam" possess 7 deep terrible layers. The lower the layer the worse it is. Individuals will arrive at both everlasting places during Judgment Day, which commences after the angel Israfil blows the trumpet the second time. Islam teaches the continued existence of the soul and a transformed physical existence after death. The resurrection that will take place on the Last Day is physical, and is explained by suggesting that God will re-create the decayed body (17:100: "Could they not see that God who created the heavens and the earth is able to create the like of them?").
On the Last Day, resurrected humans and jinn will be judged by God according to their deeds. One's eternal destination depends on balance of good to bad deeds in life. They are either granted admission to Paradise, where they will enjoy spiritual and physical pleasures forever, or condemned to Hell to suffer spiritual and physical torment for eternity. The day of judgment is described as passing over Hell on a narrow bridge (as thin as human hair and sharper than a razor) in order to enter Paradise. Those who fall, weighted by their bad deeds, will go to Hell.
In Islam, believers are those who believed in oneness of God and did not associate any partners with him or did not give the attributes of God to any other entity. It is an established belief that if a believer goes to hell for his sins being greater than his good deeds, he will not remain in hell forever. When punishment for his sins will be over, God will forgive him and grant him heaven.
#### Ahmadiyya.
Ahmadi Muslims believe that the afterlife is not material but of a spiritual nature. According to Mirza Ghulam Ahmad, founder of the Ahmadiyya Muslim Community, the soul will give birth to another rarer entity and will resemble the life on this earth in the sense that this entity will bear a similar relationship to the soul as the soul bears relationship with the human existence on earth. On earth, if a person leads a righteous life and submits to the will of God, his or her tastes become attuned to enjoying spiritual pleasures as opposed to carnal desires. With this, an "embryonic soul" begins to take shape. Different tastes are said to be born which a person given to carnal passions finds no enjoyment. For example, sacrifice of one's own rights over that of others becomes enjoyable, or that forgiveness becomes second nature. In such a state a person finds contentment and peace at heart and at this stage, according to Ahmadiyya beliefs, it can be said that a soul within the soul has begun to take shape.
#### Sufism.
The Sufi Muslim scholar Ibn 'Arabi defined Barzakh as the intermediate realm or "isthmus". It is between the world of corporeal bodies and the world of spirits, and is a means of contact between the two worlds. Without it, there would be no contact between the two and both would cease to exist. He described it as simple and luminous, like the world of spirits, but also able to take on many different forms just like the world of corporeal bodies can. In broader terms Barzakh, "is anything that separates two things". It has been called the dream world in which the dreamer is in both life and death.
### Judaism.
#### Sheol.
Sheol, in the Hebrew Bible, is a place of darkness (Job x. 21, 22) to which all the dead go, both the righteous and the unrighteous, regardless of the moral choices made in life, (Gen. xxxvii. 36; Ezek. xxxii.; Isa. xiv.; Job xxx. 23), a place of stillness, (Ps. lxxxviii. 13, xciv. 17; Eccl. ix. 10), at the longest possible distance from heaven (Job xi. 8; Amos ix. 2; Ps. cxxxix. 8).
The inhabitants of Sheol are the "shades" ("rephaim"), entities without personality or strength. Under some circumstances they are thought to be able to be contacted by the living, as the Witch of Endor contacts the shade of Samuel for Saul, but such practices are forbidden (Deuteronomy 18:10).
While the Hebrew Bible appears to describe Sheol as the permanent place of the dead, in the Second Temple period (roughly 500 BC – 70 AD) a more diverse set of ideas developed. In some texts, Sheol is considered to be the home of both the righteous and the wicked, separated into respective compartments; in others, it was considered a place of punishment, meant for the wicked dead alone. When the Hebrew scriptures were translated into Greek in ancient Alexandria around 200 BC, the word "Hades" (the Greek underworld) was substituted for Sheol. This is reflected in the New Testament where Hades is both the underworld of the dead and the personification of the evil it represents.
#### World to Come.
The Talmud offers a number of thoughts relating to the afterlife. After death, the soul is brought for judgment. Those who have led pristine lives enter immediately into the "Olam Haba" or world to come. Most do not enter the world to come immediately, but now experience a period of review of their earthly actions and they are made aware of what they have done wrong. Some view this period as being a "re-schooling", with the soul gaining wisdom as one's errors are reviewed. Others view this period to include spiritual discomfort for past wrongs. At the end of this period, not longer than one year, the soul then takes its place in the world to come. Although discomforts are made part of certain Jewish conceptions of the afterlife, the concept of eternal damnation is not a tenet of the Jewish afterlife. According to the Talmud, extinction of the soul is reserved for a far smaller group of malicious and evil leaders, either whose very evil deeds go way beyond norms, or who lead large groups of people to utmost evil. This is also part of Maimonides' 13 principles of faith.
Maimonides describes the "Olam Haba" in spiritual terms, relegating the prophesied physical resurrection to the status of a future miracle, unrelated to the afterlife or the Messianic era. According to Maimonides, an afterlife continues for the soul of every human being, a soul now separated from the body in which it was "housed" during its earthly existence.
The Zohar describes Gehenna not as a place of punishment for the wicked but as a place of spiritual purification for souls.
#### Reincarnation in Jewish tradition.
Although there is no reference to reincarnation in the Talmud or any prior writings, according to rabbis such as Avraham Arieh Trugman, reincarnation is recognized as being part and parcel of Jewish tradition. Trugman explains that it is through oral tradition that the meanings of the Torah, its commandments and stories, are known and understood. The classic work of Jewish mysticism, the Zohar, is quoted liberally in all Jewish learning; in the Zohar the idea of reincarnation is mentioned repeatedly. Trugman states that in the last five centuries the concept of reincarnation, which until then had been a much hidden tradition within Judaism, was given open exposure.
Shraga Simmons commented that within the Bible itself, the idea [of reincarnation] is intimated in Deut. 25:5–10, Deut. 33:6 and Isaiah 22:14, 65:6.
Yirmiyahu Ullman wrote that reincarnation is an "ancient, mainstream belief in Judaism". The Zohar makes frequent and lengthy references to reincarnation. Onkelos, a righteous convert and authoritative commentator of the same period, explained the verse, "Let Reuben live and not die ..." (Deuteronomy 33:6) to mean that Reuben should merit the World to Come directly, and not have to die again as a result of being reincarnated. Torah scholar, commentator and kabbalist, Nachmanides (Ramban 1195–1270), attributed Job's suffering to reincarnation, as hinted in Job's saying "God does all these things twice or three times with a man, to bring back his soul from the pit to... the light of the living' (Job 33:29, 30)."
Reincarnation, called "gilgul", became popular in folk belief, and is found in much Yiddish literature among Ashkenazi Jews. Among a few kabbalists, it was posited that some human souls could end up being reincarnated into non-human bodies. These ideas were found in a number of Kabbalistic works from the 13th century, and also among many mystics in the late 16th century. Martin Buber's early collection of stories of the Baal Shem Tov's life includes several that refer to people reincarnating in successive lives.
Among well known (generally non-kabbalist or anti-kabbalist) rabbis who rejected the idea of reincarnation are Saadia Gaon, David Kimhi, Hasdai Crescas, Yedayah Bedershi (early 14th century), Joseph Albo, Abraham ibn Daud, the Rosh and Leon de Modena. Saadia Gaon, in Emunoth ve-Deoth (Hebrew: "beliefs and opinions") concludes Section VI with a refutation of the doctrine of metempsychosis (reincarnation). While rebutting reincarnation, Saadia Gaon further states that Jews who hold to reincarnation have adopted non-Jewish beliefs. By no means do all Jews today believe in reincarnation, but belief in reincarnation is not uncommon among many Jews, including Orthodox.
Other well-known rabbis who are reincarnationists include Yonassan Gershom, Abraham Isaac Kook, Talmud scholar Adin Steinsaltz, DovBer Pinson, David M. Wexelman, Zalman Schachter, and many others. Reincarnation is cited by authoritative biblical commentators, including Ramban (Nachmanides), Menachem Recanti and Rabbenu Bachya.
Among the many volumes of Yitzchak Luria, most of which come down from the pen of his primary disciple, Chaim Vital, are insights explaining issues related to reincarnation. His "Shaar HaGilgulim", "The Gates of Reincarnation", is a book devoted exclusively to the subject of reincarnation in Judaism.
Rabbi Naftali Silberberg of The Rohr Jewish Learning Institute notes that "Many ideas that originate in other religions and belief systems have been popularized in the media and are taken for granted by unassuming Jews."
## Indian religions.
### Buddhism.
Buddhists maintain that rebirth takes place without an unchanging self or soul passing from one form to another. The type of rebirth will be conditioned by the moral tone of the person's actions (kamma or karma). For example, if a person has committed harmful actions by body, speech and mind based on greed, hate and delusion, would have his/her rebirth in a lower realm, i.e. an animal, a hungry ghost or a hell realm, is to be expected. On the other hand, where a person has performed skillful actions based on generosity, loving-kindness (metta), compassion and wisdom, rebirth in a happy realm, i.e. human or one of the many heavenly realms, can be expected.
Yet the mechanism of rebirth with kamma is not deterministic. It depends on various levels of kamma. The most important moment that determines where a person is reborn into is the last thought moment. At that moment, heavy kamma would ripen if there were performed, if not then near death kamma, if not then habitual kamma, finally if none of the above happened, then residual kamma from previous actions can ripen. According to Theravada Buddhism, there are 31 realms of existence that one can be reborn into.
Pure Land Buddhism of Mahayana believes in a special place apart from the 31 planes of existence called Pure Land. It is believed that each Buddha has their own pure land, created out of their merits for the sake of sentient beings who recall them mindfully to be able to be reborn in their pure land and train to become a Buddha there. Thus the main practice of pure land Buddhism is to chant a Buddha's name.
In Tibetan Buddhism the Tibetan Book of the Dead explains the intermediate state of humans between death and reincarnation. The deceased will find the bright light of wisdom, which shows a straightforward path to move upward and leave the cycle of reincarnation. There are various reasons why the deceased do not follow that light. Some had no briefing about the intermediate state in the former life. Others only used to follow their basic instincts like animals. And some have fear, which results from foul deeds in the former life or from insistent haughtiness. In the intermediate state the awareness is very flexible, so it is important to be virtuous, adopt a positive attitude, and avoid negative ideas. Ideas which are rising from subconsciousness can cause extreme tempers and cowing visions. In this situation they have to understand, that these manifestations are just reflections of the inner thoughts. No one can really hurt them, because they have no more material body. The deceased get help from different Buddhas who show them the path to the bright light. The ones who do not follow the path after all will get hints for a better reincarnation. They have to release the things and beings on which or whom they still hang from the life before. It is recommended to choose a family where the parents trust in the Dharma and to reincarnate with the will to care for the welfare of all beings.
"Life is cosmic energy of the universe and after death it merges in universe again and as the time comes to find the suitable place for the entity died in the life condition it gets born. There are 10 life states of any life: Hell, hunger, anger, animality, rapture, humanity, learning, realization, bodhisatva and buddhahood. The life dies in which life condition it reborn in the same life condition."
### Hinduism.
There are two major views of afterlife in Hinduism: mythical and philosophical. The philosophies of Hinduism consider each individual consists of 3 bodies: physical body compose of water and bio-matter ("sthūla śarīra"), an energetic/psychic/mental/subtle body ("sūkṣma-śarīra") and a causal body ("kāraṇa śarīra") comprising subliminal stuff i.e. mental impressions etc.
The individual is a stream of consciousness ("Ātman") which flows through all the physical changes of the body and at the death of the physical body, flows on into another physical body. The two components that transmigrate are the subtle body and the causal body.
The thought that occupies the mind at the time of death determines the quality of our rebirth (antim smaraṇa), hence Hinduism advises to be mindful of one's thoughts and cultivate positive wholesome thoughts - Mantra chanting (Japa) is commonly practiced for this.
The mythical includes the philosophical but adds heaven and hell myths.
When one leaves the physical body at death he appears in the court of Lord Yama, the God of Death for an exit interview. The panel consists of Yama and Chitragupta - the cosmic accountant, and Varuna the cosmic intelligence officer. He is counseled about his life, achievements and failures and is shown a mirror in which his entire life is reflected. (Philosophically these three men are projections of one's mind) Yama the Lord of Justice then sends him to a heavenly realm (svarga) if he has been exceptionally benevolent and beneficent for a period of Rest and Recreation. his period is limited in time by the weight of his good deeds. If he has been exceptionally malevolent and caused immense suffering to other beings then he is sent to a cosmic gulag (naraka) for his sins. After one has exhausted his karmas, he takes birth again to continue his spiritual evolution.
Rebirth can take place as a god (deva), a human (manuṣya) an animal (tiryak) — but it is generally taught that the spiritual evolution takes place from lower to higher species. In certain cases of traumatic death a person can take the form of a Preta or Hungry Ghost - and remains in an earth-bound state interminably - until certain ceremonies are done to liberate them. This mythological part is extensively elaborated in the Hindu Puranas especially in the Garuda Purana.
The Upanishads are the first scriptures in Hinduism which explicitly mention about Afterlife, The Bhagavad Gita, a famous Hindu script, says that just as a man discards his old clothes and wears new ones; similarly the Atman discards the old body and takes on a new one. In Hinduism, the belief is that the body is nothing but a shell, the consciousness inside is immutable and indestructible and takes on different lives in a cycle of birth and death. The end of this cycle is called "mukti" (Sanskrit: मुक्ति) and staying finally with the ultimate reality forever; is "moksha" (Sanskrit: मोक्ष) or liberation
### Jainism.
Jainism also believes in the afterlife. They believe that the soul takes on a body form based on previous karmas or actions performed by that soul through eternity. Jains believe the soul is eternal and that the freedom from the cycle of reincarnation is the means to attain eternal bliss.
### Sikhism.
The essential doctrine of Sikhism is to experience the divine through simple living, meditation and contemplation while being alive. Sikhism also has the belief of being in union with God while living. Accounts of afterlife are considered to be aimed at the popular prevailing views of the time so as to provide a referential framework without necessarily establishing a belief in the afterlife. Thus while it is also acknowledged that living the life of a householder is above the metaphysical truth, Sikhism can be considered agnostic to the question of an afterlife. Some scholars also interpret the mention of reincarnation to be naturalistic akin to the biogeochemical cycles.
But if one analyses the Sikh Scriptures carefully, one may find that on many occasions the afterlife and the existence of heaven and hell are mentioned in "Guru Granth Sahib" and in "Dasam Granth", so from that it can be concluded that Sikhism does believe in the existence of heaven and hell; however, heaven and hell are created to temporarily reward and punish, and one will then take birth again until one merges in God. According to the Sikh scriptures, the human form is the closet form to God and the best opportunity for a human being to attain salvation and merge back with God. Sikh Gurus said that nothing dies, nothing is born, everything is ever present, and it just changes forms. Like standing in front of a wardrobe, you pick up a dress and wear it and then you discard it. You wear another one. Thus, in the view of Sikhism, your soul is never born and never dies. Your soul is a part of God and hence lives forever.
## Others.
### Traditional African religions.
Traditional African religions are diverse in their beliefs in an afterlife. Hunter-gatherer societies such as the Hadza have no particular belief in an afterlife, and the death of an individual is a straightforward end to their existence. Ancestor cults are found throughout Sub-Saharan Africa, including cultures like the Yombe, Beng, Yoruba and Ewe, "[T]he belief that the dead come back into life and are reborn into their families is given concrete expression in the personal names that are given to children...What is reincarnated are some of the dominant characteristics of the ancestor and not his soul. For each soul remains distinct and each birth represents a new soul." The Yoruba, Dogon and LoDagoa have eschatological ideas similar to Abrahamic religions, "but in most African societies, there is a marked absence of such clear-cut notions of heaven and hell, although there are notions of God judging the soul after death." In some societies like the Mende, multiple beliefs coexist. The Mende believe that people die twice: once during the process of joining the secret society, and again during biological death after which they become ancestors. However, some Mende also believe that after people are created by God they live ten consecutive lives, each in progressively descending worlds. One cross-cultural theme is that the ancestors are part of the world of the living, interacting with it regularly.
### Shinto.
It is common for families to participate in ceremonies for children at a shrine, yet have a Buddhist funeral at the time of death. In old Japanese legends, it is often claimed that the dead go to a place called "yomi" (黄泉), a gloomy underground realm with a river separating the living from the dead mentioned in the legend of Izanami and Izanagi. This "yomi" very closely resembles the Greek Hades; however, later myths include notions of resurrection and even Elysium-like descriptions such as in the legend of Okuninushi and Susanoo. Shinto tends to hold negative views on death and corpses as a source of pollution called "kegare". However, death is also viewed as a path towards apotheosis in Shintoism as can be evidenced by how legendary individuals become enshrined after death. Perhaps the most famous would be Emperor Ojin who was enshrined as Hachiman the God of War after his death.
### Unitarian Universalism.
Some Unitarian Universalists believe in universalism: that all souls will ultimately be saved and that there are no torments of hell. Unitarian Universalists differ widely in their theology hence there is no exact same stance on the issue. Although Unitarians historically believed in a literal hell, and Universalists historically believed that everyone goes to heaven, modern Unitarian Universalists can be categorized into those believing in a heaven, reincarnation and oblivion. Most Unitarian Universalists believe that heaven and hell are symbolic places of consciousness and the faith is largely focused on the worldly life rather than any possible afterlife.
### Spiritualism.
According to Edgar Cayce, the afterlife consisted of nine realms equated with the nine planets of astrology. The first, symbolized by Saturn, was a level for the purification of the souls. The second, Mercury's realm, gives us the ability to consider problems as a whole. The third of the nine soul realms is ruled by Earth and is associated with the Earthly pleasures. The fourth realm is where we find out about love and is ruled by Venus. The fifth realm is where we meet our limitations and is ruled by Mars. The sixth realm is ruled by Neptune, and is where we begin to use our creative powers and free ourselves from the material world. The seventh realm is symbolized by Jupiter, which strengthens the soul's ability to depict situations, to analyze people and places, things, and conditions. The eighth afterlife realm is ruled by Uranus and develops psychic ability. The ninth afterlife realm is symbolized by Pluto, the astrological realm of the unconscious. This afterlife realm is a transient place where souls can choose to travel to other realms or other solar systems, it is the souls liberation into eternity, and is the realm that opens the doorway from our solar system into the cosmos point of view.
Mainstream Spiritualists postulate a series of seven realms that are not unlike Edgar Cayce's nine realms ruled by the planets. As it evolves, the soul moves higher and higher until it reaches the ultimate realm of spiritual oneness. The first realm, equated with hell, is the place where troubled souls spend a long time before they are compelled to move up to the next level. The second realm, where most souls move directly, is thought of as an intermediate transition between the lower planes of life and hell and the higher perfect realms of the universe. The third level is for those who have worked with their karmic inheritance. The fourth level is that from which evolved souls teach and direct those on Earth. The fifth level is where the soul leaves human consciousness behind. At the sixth plane, the soul is finally aligned with the cosmic consciousness and has no sense of separateness or individuality. Finally, the seventh level, the goal of each soul, is where the soul transcends its own sense of "soulfulness" and reunites with the World Soul and the universe.
### Wicca.
The Wiccan afterlife is most commonly described as The Summerland. Here, souls rest, recuperate from life, and reflect on the experiences they had during their lives. After a period of rest, the souls are reincarnated, and the memory of their previous lives is erased. Many Wiccans see The Summerland as a place to reflect on their life actions. It is not a place of reward, but rather the end of a life journey at an end point of incarnations.
### Zoroastrianism.
Zoroastrianism states that the "urvan", the disembodied spirit, lingers on earth for three days before departing downward to the kingdom of the dead that is ruled by Yima. For the three days that it rests on Earth, righteous souls sit at the head of their body, chanting the Ustavaiti Gathas with joy, while a wicked person sits at the feet of the corpse, wails and recites the Yasna. Zoroastrianism states that for the righteous souls, a beautiful maiden, which is the personification of the soul's good thoughts, words and deeds, appears. For a wicked person, a very old, ugly, naked hag appears. After three nights, the soul of the wicked is taken by the demon Vizaresa (Vīzarəša), to Chinvat bridge, and is made to go to darkness (hell).
Yima is believed to have been the first king on earth to rule, as well as the first man to die. Inside of Yima's realm, the spirits live a shadowy existence, and are dependent on their own descendants which are still living on Earth. Their descendants are to satisfy their hunger and clothe them, through rituals done on earth.
Rituals which are done on the first three days are vital and important, as they protect the soul from evil powers and give it strength to reach the underworld. After three days, the soul crosses Chinvat bridge which is the Final Judgment of the soul. Rashnu and Sraosha are present at the final judgment. The list is expanded sometimes, and include Vahman and Ormazd. Rashnu is the yazata who holds the scales of justice. If the good deeds of the person outweigh the bad, the soul is worthy of paradise. If the bad deeds outweigh the good, the bridge narrows down to the width of a blade-edge, and a horrid hag pulls the soul in her arms, and takes it down to hell with her.
Misvan Gatu is the "place of the mixed ones" where the souls lead a gray existence, lacking both joy and sorrow. A soul goes here if his/her good deeds and bad deeds are equal, and Rashnu's scale is equal.
## Parapsychology.
The Society for Psychical Research was founded in 1882 with the express intention of investigating phenomena relating to Spiritualism and the afterlife. Its members continue to conduct scientific research on the paranormal to this day. Some of the earliest attempts to apply scientific methods to the study of phenomena relating to an afterlife were conducted by this organization. Its earliest members included noted scientists like William Crookes, and philosophers such as Henry Sidgwick and William James.
Parapsychological investigation of the afterlife includes the study of haunting, apparitions of the deceased, instrumental trans-communication, electronic voice phenomena, and mediumship. 
A study conducted in 1901 by physician Duncan MacDougall sought to measure the weight lost by a human when the soul "departed the body" upon death. MacDougall weighed dying patients in an attempt to prove that the soul was material, tangible and thus measurable. Although MacDougall's results varied considerably from "21 grams", for some people this figure has become synonymous with the measure of a soul's mass. The title of the 2003 movie "21 Grams" is a reference to MacDougall's findings. His results have never been reproduced, and are generally regarded either as meaningless or considered to have had little if any scientific merit.
Frank Tipler has argued that physics can explain immortality, although such arguments are not falsifiable and, in Karl Popper's views, they do not qualify as science.
After 25 years of parapsychological research Susan Blackmore came to the conclusion that, according to her experiences, there is not enough empirical evidence for many of these cases.
### Mediumship.
Mediums purportedly act as a vessel for communications from spirits in other realms. Mediumship is not specific to one culture or religion; it can be identified in several belief systems, most notably Spiritualism. While the practice gained popularity in Europe and North America in the 19th century, evidence of mediumship dates back thousands of years in Asia. Mediums who claim to have contact with deceased people include Tyler Henry and Pascal Voggenhuber.
### Near death research.
Research also includes the study of the near death experience. Scientists who have worked in this area include Elisabeth Kübler-Ross, Raymond Moody, Sam Parnia, Michael Sabom, Bruce Greyson, Peter Fenwick, Jeffrey Long, Susan Blackmore, Charles Tart, William James, Ian Stevenson, Michael Persinger, Pim van Lommel, Penny Sartori, Walter van Laack among others.
## Philosophy.
### Modern philosophy.
There is a view based on the philosophical question of personal identity, termed open individualism by Daniel Kolak. It concludes that individual conscious experience is illusory, and because consciousness continues after death in all conscious beings, "you" do not die. This position has been supported by notable physicists such as Erwin Schrödinger and Freeman Dyson.
Certain problems arise with the idea of a particular person continuing after death. Peter van Inwagen, in his argument regarding resurrection, notes that the materialist must have some sort of physical continuity. John Hick also raises questions regarding personal identity in his book, "Death and Eternal Life", using an example of a person ceasing to exist in one place while an exact replica appears in another. If the replica had all the same experiences, traits, and physical appearances of the first person, we would all attribute the same identity to the second, according to Hick.
### Process philosophy.
In the panentheistic model of process philosophy and theology the writers Alfred North Whitehead and Charles Hartshorne rejected the idea that the universe was made of substance, instead saying reality is composed of living experiences (occasions of experience). According to Hartshorne people do not experience subjective (or personal) immortality in the afterlife, but they do have objective immortality because their experiences live on forever in God, who contains all that was. However other process philosophers such as David Ray Griffin have written that people may have subjective experience after death.
## Science.
Psychological proposals for the origin of a belief in an afterlife include cognitive disposition, cultural learning, and as an intuitive religious idea. In one study, children were able to recognize the ending of physical, mental, and perceptual activity in death, but were hesitant to conclude the ending of will, self, or emotion in death.
In 2008, a large-scale study conducted by the University of Southampton involving 2060 patients from 15 hospitals in the United Kingdom, United States and Austria was launched. The AWARE (AWAreness during REsuscitation) study examined the broad range of mental experiences in relation to death. In a large study, researchers also tested the validity of conscious experiences for the first time using objective markers, to determine whether claims of awareness compatible with out-of-body experiences correspond with real or hallucinatory events. The results revealed that 40% of those who survived a cardiac arrest were aware during the time that they were clinically dead and before their hearts were restarted. One patient also had a verified out-of-body experience (over 80% of patients did not survive their cardiac arrest or were too sick to be interviewed), but his cardiac arrest occurred in a room without markers. Dr. Parnia in the interview stated, "The evidence thus far suggests that in the first few minutes after death, consciousness is not annihilated." The study continues in AWARE II, which is set to be completed in September 2020.
Studies have also been done on the widely reported phenomenon of Near Death Experiences. Experiencers commonly report being transported to a different “realm” or “plane of existence” and they have been shown to display a lasting positive aftereffect on most experiencers.
## External links.
 

</doc>
<doc id="1181" url="https://en.wikipedia.org/wiki?curid=1181" title="Astrometry">
Astrometry

Astrometry is a branch of astronomy that involves precise measurements of the positions and movements of stars and other celestial bodies. It provides the kinematics and physical origin of the Solar System and our galaxy, the Milky Way.
## History.
The history of astrometry is linked to the history of star catalogues, which gave astronomers reference points for objects in the sky so they could track their movements. This can be dated back to Hipparchus, who around 190 BC used the catalogue of his predecessors Timocharis and Aristillus to discover Earth's precession. In doing so, he also developed the brightness scale still in use today. Hipparchus compiled a catalogue with at least 850 stars and their positions. Hipparchus's successor, Ptolemy, included a catalogue of 1,022 stars in his work the "Almagest", giving their location, coordinates, and brightness.
In the 10th century, Abd al-Rahman al-Sufi carried out observations on the stars and described their positions, magnitudes and star color; furthermore, he provided drawings for each constellation, which are depicted in his "Book of Fixed Stars". Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe with a diameter of nearly 1.4 metres. His observations on eclipses were still used centuries later in Simon Newcomb's investigations on the motion of the Moon, while his other observations of the motions of the planets Jupiter and Saturn inspired Laplace's "Obliquity of the Ecliptic" and "Inequalities of Jupiter and Saturn". In the 15th century, the Timurid astronomer Ulugh Beg compiled the "Zij-i-Sultani", in which he catalogued 1,019 stars. Like the earlier catalogs of Hipparchus and Ptolemy, Ulugh Beg's catalogue is estimated to have been precise to within approximately 20 minutes of arc.
In the 16th century, Tycho Brahe used improved instruments, including large mural instruments, to measure star positions more accurately than previously, with a precision of 15–35 arcsec. Taqi al-Din measured the right ascension of the stars at the Constantinople Observatory of Taqi ad-Din using the "observational clock" he invented. When telescopes became commonplace, setting circles sped measurements
James Bradley first tried to measure stellar parallaxes in 1729. The stellar movement proved too insignificant for his telescope, but he instead discovered the aberration of light and the nutation of the Earth's axis. His cataloguing of 3222 stars was refined in 1807 by Friedrich Bessel, the father of modern astrometry. He made the first measurement of stellar parallax: 0.3 arcsec for the binary star 61 Cygni.
Being very difficult to measure, only about 60 stellar parallaxes had been obtained by the end of the 19th century, mostly by use of the filar micrometer. Astrographs using astronomical photographic plates sped the process in the early 20th century. Automated plate-measuring machines and more sophisticated computer technology of the 1960s allowed more efficient compilation of star catalogues. Started in the late 19th century, the project Carte du Ciel to improve star mapping couldn't be finished but made photography a common technique for astrometry. In the 1980s, charge-coupled devices (CCDs) replaced photographic plates and reduced optical uncertainties to one milliarcsecond. This technology made astrometry less expensive, opening the field to an amateur audience.
In 1989, the European Space Agency's Hipparcos satellite took astrometry into orbit, where it could be less affected by mechanical forces of the Earth and optical distortions from its atmosphere. Operated from 1989 to 1993, Hipparcos measured large and small angles on the sky with much greater precision than any previous optical telescopes. During its 4-year run, the positions, parallaxes, and proper motions of 118,218 stars were determined with an unprecedented degree of accuracy. A new "Tycho catalog" drew together a database of 1,058,332 stars to within 20-30 mas (milliarcseconds). Additional catalogues were compiled for the 23,882 double and multiple stars and 11,597 variable stars also analyzed during the Hipparcos mission.
In 2013, the Gaia satellite was launched and improved the accuracy of Hipparcos.
The precision was improved by a factor of 100 and enabled the mapping of a billion stars.
Today, the catalogue most often used is USNO-B1.0, an all-sky catalogue that tracks proper motions, positions, magnitudes and other characteristics for over one billion stellar objects. During the past 50 years, 7,435 Schmidt camera plates were used to complete several sky surveys that make the data in USNO-B1.0 accurate to within 0.2 arcsec.
## Applications.
Apart from the fundamental function of providing astronomers with a reference frame to report their observations in, astrometry is also fundamental for fields like celestial mechanics, stellar dynamics and galactic astronomy. In observational astronomy, astrometric techniques help identify stellar objects by their unique motions. It is instrumental for keeping time, in that UTC is essentially the atomic time synchronized to Earth's rotation by means of exact astronomical observations. Astrometry is an important step in the cosmic distance ladder because it establishes parallax distance estimates for stars in the Milky Way.
Astrometry has also been used to support claims of extrasolar planet detection by measuring the displacement the proposed planets cause in their parent star's apparent position on the sky, due to their mutual orbit around the center of mass of the system. Astrometry is more accurate in space missions that are not affected by the distorting effects of the Earth's atmosphere. NASA's planned Space Interferometry Mission (SIM PlanetQuest) (now cancelled) was to utilize astrometric techniques to detect terrestrial planets orbiting 200 or so of the nearest solar-type stars. The European Space Agency's Gaia Mission, launched in 2013, applies astrometric techniques in its stellar census. In addition to the detection of exoplanets, it can also be used to determine their mass.
Astrometric measurements are used by astrophysicists to constrain certain models in celestial mechanics. By measuring the velocities of pulsars, it is possible to put a limit on the asymmetry of supernova explosions. Also, astrometric results are used to determine the distribution of dark matter in the galaxy.
Astronomers use astrometric techniques for the tracking of near-Earth objects. Astrometry is responsible for the detection of many record-breaking Solar System objects. To find such objects astrometrically, astronomers use telescopes to survey the sky and large-area cameras to take pictures at various determined intervals. By studying these images, they can detect Solar System objects by their movements relative to the background stars, which remain fixed. Once a movement per unit time is observed, astronomers compensate for the parallax caused by Earth's motion during this time and the heliocentric distance to this object is calculated. Using this distance and other photographs, more information about the object, including its orbital elements, can be obtained.
50000 Quaoar and 90377 Sedna are two Solar System objects discovered in this way by Michael E. Brown and others at Caltech using the Palomar Observatory's Samuel Oschin telescope of and the Palomar-Quest large-area CCD camera. The ability of astronomers to track the positions and movements of such celestial bodies is crucial to the understanding of the Solar System and its interrelated past, present, and future with others in the Universe.
## Statistics.
A fundamental aspect of astrometry is error correction. Various factors introduce errors into the measurement of stellar positions, including atmospheric conditions, imperfections in the instruments and errors by the observer or the measuring instruments. Many of these errors can be reduced by various techniques, such as through instrument improvements and compensations to the data. The results are then analyzed using statistical methods to compute data estimates and error ranges.

</doc>
<doc id="1182" url="https://en.wikipedia.org/wiki?curid=1182" title="Athena">
Athena

Athena or Athene, often given the epithet Pallas, is an ancient Greek goddess associated with wisdom, handicraft, and warfare who was later syncretized with the Roman goddess Minerva. Athena was regarded as the patron and protectress of various cities across Greece, particularly the city of Athens, from which she most likely received her name. The Parthenon on the Acropolis of Athens is dedicated to her. Her major symbols include owls, olive trees, snakes, and the Gorgoneion. In art, she is generally depicted wearing a helmet and holding a spear.
From her origin as an Aegean palace goddess, Athena was closely associated with the city. She was known as "Polias" and "Poliouchos" (both derived from "polis", meaning "city-state"), and her temples were usually located atop the fortified acropolis in the central part of the city. The Parthenon on the Athenian Acropolis is dedicated to her, along with numerous other temples and monuments. As the patron of craft and weaving, Athena was known as "Ergane". She was also a warrior goddess, and was believed to lead soldiers into battle as "Athena Promachos". Her main festival in Athens was the Panathenaia, which was celebrated during the month of Hekatombaion in midsummer and was the most important festival on the Athenian calendar.
In Greek mythology, Athena was believed to have been born from the forehead of her father Zeus. In some versions of the story, Athena has no mother and is born from Zeus' forehead by parthenogenesis. In others, such as Hesiod's "Theogony", Zeus swallows his consort Metis, who was pregnant with Athena; in this version, Athena is first born within Zeus and then escapes from his body through his forehead. In the founding myth of Athens, Athena bested Poseidon in a competition over patronage of the city by creating the first olive tree. She was known as "Athena Parthenos" "Athena the Virgin," but in one archaic Attic myth, the god Hephaestus tried and failed to rape her, resulting in Gaia giving birth to Erichthonius, an important Athenian founding hero. Athena was the patron goddess of heroic endeavor; she was believed to have aided the heroes Perseus, Heracles, Bellerophon, and Jason. Along with Aphrodite and Hera, Athena was one of the three goddesses whose feud resulted in the beginning of the Trojan War.
She plays an active role in the "Iliad", in which she assists the Achaeans and, in the "Odyssey", she is the divine counselor to Odysseus. In the later writings of the Roman poet Ovid, Athena was said to have competed against the mortal Arachne in a weaving competition, afterward transforming Arachne into the first spider; Ovid also describes how she transformed Medusa into a Gorgon after witnessing her being raped by Poseidon in her temple. Since the Renaissance, Athena has become an international symbol of wisdom, the arts, and classical learning. Western artists and allegorists have often used Athena as a symbol of freedom and democracy.
## Etymology.
Athena is associated with the city of Athens. The name of the city in ancient Greek is (), a plural toponym, designating the place where—according to myth—she presided over the "Athenai", a sisterhood devoted to her worship. In ancient times, scholars argued whether Athena was named after Athens or Athens after Athena. Now scholars generally agree that the goddess takes her name from the city; the ending -"ene" is common in names of locations, but rare for personal names. Testimonies from different cities in ancient Greece attest that similar city goddesses were worshipped in other cities and, like Athena, took their names from the cities where they were worshipped. For example, in Mycenae there was a goddess called Mykene, whose sisterhood was known as "Mykenai", whereas at Thebes an analogous deity was called Thebe, and the city was known under the plural form "Thebai" (or Thebes, in English, where the 's' is the plural formation). The name "Athenai" is likely of Pre-Greek origin because it contains the presumably Pre-Greek morpheme "*-ān-".
In his dialogue "Cratylus", the ancient Greek philosopher Plato (428–347 BC) gives some rather imaginative etymologies of Athena's name, based on the theories of the ancient Athenians and his own etymological speculations:
Thus, Plato believed that Athena's name was derived from Greek , —which the later Greeks rationalised as from the deity's (, ) mind (, ). The second-century AD orator Aelius Aristides attempted to derive natural symbols from the etymological roots of Athena's names to be "aether", "air", "earth", and "moon".
## Origins.
Athena was originally the Aegean goddess of the palace, who presided over household crafts and protected the king. A single Mycenaean Greek inscription appears at Knossos in the Linear B tablets from the Late Minoan II-era "Room of the Chariot Tablets"; these comprise the earliest Linear B archive anywhere. Although "Athana potnia" is often translated as "Mistress Athena", it could also mean "the "Potnia" of Athana", or "the Lady of Athens". However, any connection to the city of Athens in the Knossos inscription is uncertain. A sign series appears in the still undeciphered corpus of Linear A tablets, written in the unclassified Minoan language. This could be connected with the Linear B Mycenaean expressions and or ("Diwia", "of Zeus" or, possibly, related to a homonymous goddess), resulting in a translation "Athena of Zeus" or "divine Athena". Similarly, in the Greek mythology and epic tradition, Athena figures as a daughter of Zeus (; "cfr." Dyeus). However, the inscription quoted seems to be very similar to "", quoted as SY Za 1 by Jan Best. Best translates the initial , which is recurrent in line beginnings, as "I have given".
A Mycenean fresco depicts two women extending their hands towards a central figure, who is covered by an enormous figure-eight shield; this may depict the warrior-goddess with her "palladion", or her palladion in an aniconic representation. In the "Procession Fresco" at Knossos, which was reconstructed by the Mycenaeans, two rows of figures carrying vessels seem to meet in front of a central figure, which is probably the Minoan precursor to Athena. The early twentieth-century scholar Martin Persson Nilsson argued that the Minoan snake goddess figurines are early representations of Athena.
Nilsson and others have claimed that, in early times, Athena was either an owl herself or a bird goddess in general. In the third book of the "Odyssey", she takes the form of a sea-eagle. Proponents of this view argue that she dropped her prophylactic owl-mask before she lost her wings. "Athena, by the time she appears in art," Jane Ellen Harrison remarks, "has completely shed her animal form, has reduced the shapes she once wore of snake and bird to attributes, but occasionally in black-figure vase-paintings she still appears with wings."
It is generally agreed that the cult of Athena preserves some aspects of the Proto-Indo-European transfunctional goddess. The cult of Athena may have also been influenced by those of Near Eastern warrior goddesses such as the East Semitic Ishtar and the Ugaritic Anat, both of whom were often portrayed bearing arms. Classical scholar Charles Penglase notes that Athena resembles Inanna in her role as a "terrifying warrior goddess" and that both goddesses were closely linked with creation. Athena's birth from the head of Zeus may be derived from the earlier Sumerian myth of Inanna's descent into and return from the Underworld.
Plato notes that the citizens of Sais in Egypt worshipped a goddess known as Neith, whom he identifies with Athena. Neith was the ancient Egyptian goddess of war and hunting, who was also associated with weaving; her worship began during the Egyptian Pre-Dynastic period. In Greek mythology, Athena was reported to have visited mythological sites in North Africa, including Libya's Triton River and the Phlegraean plain. Based on these similarities, the Sinologist Martin Bernal created the "Black Athena" hypothesis, which claimed that Neith was brought to Greece from Egypt, along with "an enormous number of features of civilization and culture in the third and second millennia". The "Black Athena" hypothesis stirred up widespread controversy near the end of the twentieth century, but it has now been widely rejected by modern scholars.
## Cult and patronages.
### Panhellenic and Athenian cult.
In her aspect of "Athena Polias", Athena was venerated as the goddess of the city and the protectress of the citadel. In Athens, the Plynteria, or "Feast of the Bath", was observed every year at the end of the month of Thargelion. The festival lasted for five days. During this period, the priestesses of Athena, or "plyntrídes", performed a cleansing ritual within the Erechtheion, a sanctuary devoted to Athena and Poseidon. Here Athena's statue was undressed, her clothes washed, and body purified. Athena was worshipped at festivals such as Chalceia as "Athena Ergane", the patroness of various crafts, especially weaving. She was also the patron of metalworkers and was believed to aid in the forging of armor and weapons. During the late fifth century BC, the role of goddess of philosophy became a major aspect of Athena's cult.
As "Athena Promachos", she was believed to lead soldiers into battle. Athena represented the disciplined, strategic side of war, in contrast to her brother Ares, the patron of violence, bloodlust, and slaughter—"the raw force of war". Athena was believed to only support those fighting for a just cause and was thought to view war primarily as a means to resolve conflict. The Greeks regarded Athena with much higher esteem than Ares. Athena was especially worshipped in this role during the festivals of the Panathenaea and Pamboeotia, both of which prominently featured displays of athletic and military prowess. As the patroness of heroes and warriors, Athena was believed to favor those who used cunning and intelligence rather than brute strength.
In her aspect as a warrior maiden, Athena was known as "Parthenos" ( "virgin"), because, like her fellow goddesses Artemis and Hestia, she was believed to remain perpetually a virgin. Athena's most famous temple, the Parthenon on the Athenian Acropolis, takes its name from this title. According to Karl Kerényi, a scholar of Greek mythology, the name "Parthenos" is not merely an observation of Athena's virginity, but also a recognition of her role as enforcer of rules of sexual modesty and ritual mystery. Even beyond recognition, the Athenians allotted the goddess value based on this pureness of virginity, which they upheld as a rudiment of female behavior. Kerényi's study and theory of Athena explains her virginal epithet as a result of her relationship to her father Zeus and a vital, cohesive piece of her character throughout the ages. This role is expressed in a number of stories about Athena. Marinus of Neapolis reports that when Christians removed the statue of the goddess from the Parthenon, a beautiful woman appeared in a dream to Proclus, a devotee of Athena, and announced that the "Athenian Lady" wished to dwell with him.
### Regional cults.
Athena was not only the patron goddess of Athens, but also other cities, including Argos, Sparta, Gortyn, Lindos, and Larisa. The various cults of Athena were all branches of her panhellenic cult and often proctored various initiation rites of Grecian youth, such as the passage into citizenship by young men or the passage of young women into marriage. These cults were portals of a uniform socialization, even beyond mainland Greece. Athena was frequently equated with Aphaea, a local goddess of the island of Aegina, originally from Crete and also associated with Artemis and the nymph Britomartis. In Arcadia, she was assimilated with the ancient goddess Alea and worshiped as Athena Alea. Sanctuaries dedicated to Athena Alea were located in the Laconian towns of Mantineia and Tegea. The temple of Athena Alea in Tegea was an important religious center of ancient Greece. The geographer Pausanias was informed that the "temenos" had been founded by Aleus.
Athena had a major temple on the Spartan Acropolis, where she was venerated as Poliouchos and "Khalkíoikos" ("of the Brazen House", often latinized as "Chalcioecus"). This epithet may refer to the fact that cult statue held there may have been made of bronze, that the walls of the temple itself may have been made of bronze, or that Athena was the patron of metal-workers. Bells made of terracotta and bronze were used in Sparta as part of Athena's cult. An Ionic-style temple to Athena Polias was built at Priene in the fourth century BC. It was designed by Pytheos of Priene, the same architect who designed the Mausoleum at Halicarnassus. The temple was dedicated by Alexander the Great and an inscription from the temple declaring his dedication is now held in the British Museum.
## Epithets and attributes.
Athena was known as "Atrytone" ( "the Unwearying"), "Parthenos" ( "Virgin"), and "Promachos" ( "she who fights in front"). The epithet "Polias" (Πολιάς "of the city"), refers to Athena's role as protectress of the city. The epithet "Ergane" (Εργάνη "the Industrious") pointed her out as the patron of craftsmen and artisans. Burkert notes that the Athenians sometimes simply called Athena "the Goddess", "hē theós" (ἡ θεός), certainly an ancient title. After serving as the judge at the trial of Orestes in which he was acquitted of having murdered his mother Clytemnestra, Athena won the epithet "Areia" (Αρεία).
Athena was sometimes given the epithet "Hippia" (Ἵππια "of the horses", "equestrian"), referring to her invention of the bit, bridle, chariot, and wagon. The Greek geographer Pausanias mentions in his "Guide to Greece" that the temple of Athena "Chalinitis" ("the bridler") in Corinth was located near the tomb of Medea's children. Other epithets include Ageleia, Itonia and "Aethyia", under which she was worshiped in Megara. The word "aíthyia" () signifies a "diver", also some diving bird species (possibly the shearwater) and figuratively, a "ship", so the name must reference Athena teaching the art of shipbuilding or navigation. In a temple at Phrixa in Elis, reportedly built by Clymenus, she was known as "Cydonia" (Κυδωνία). Pausanias wrote that at Buporthmus there was a sanctuary of Athena Promachorma (Προμαχόρμα), meaning "protector of the anchorage".
The Greek biographer Plutarch (AD 46–120) refers to an instance during the Parthenon's construction of her being called "Athena Hygieia" (Ὑγίεια, i. e. personified "Health") after inspiring a physician to a successful course of treatment.
In Homer's epic works, Athena's most common epithet is "Glaukopis" (), which usually is translated as, "bright-eyed" or "with gleaming eyes". The word is a combination of "glaukós" (, meaning "gleaming, silvery", and later, "bluish-green" or "gray") and "ṓps" (, "eye, face"). The word "glaúx" (, "little owl") is from the same root, presumably according to some, because of the bird's own distinctive eyes. Athena was clearly associated with the owl from very early on; in archaic images, she is frequently depicted with an owl perched on her hand. Through its association with Athena, the owl evolved into the national mascot of the Athenians and eventually became a symbol of wisdom.
In the "Iliad" (4.514), the "Odyssey" (3.378), the "Homeric Hymns", and in Hesiod's "Theogony", Athena is also given the curious epithet "Tritogeneia" (Τριτογένεια), whose significance remains unclear. It could mean various things, including "Triton-born", perhaps indicating that the homonymous sea-deity was her parent according to some early myths. One myth relates the foster father relationship of this Triton towards the half-orphan Athena, whom he raised alongside his own daughter Pallas. Kerényi suggests that "Tritogeneia did not mean that she came into the world on any particular river or lake, but that she was born of the water itself; for the name Triton seems to be associated with water generally." In Ovid's "Metamorphoses", Athena is occasionally referred to as "Tritonia".
Another possible meaning may be "triple-born" or "third-born", which may refer to a triad or to her status as the third daughter of Zeus or the fact she was born from Metis, Zeus, and herself; various legends list her as being the first child after Artemis and Apollo, though other legends identify her as Zeus' first child. Several scholars have suggested a connection to the Rigvedic god Trita, who was sometimes grouped in a body of three mythological poets. Michael Janda has connected the myth of Trita to the scene in the "Iliad" in which the "three brothers" Zeus, Poseidon, and Hades divide the world between them, receiving the "broad sky", the sea, and the underworld respectively. Janda further connects the myth of Athena being born of the head (i. e. the uppermost part) of Zeus, understanding "Trito-" (which perhaps originally meant "the third") as another word for "the sky". In Janda's analysis of Indo-European mythology, this heavenly sphere is also associated with the mythological body of water surrounding the inhabited world ("cfr." Triton's mother, Amphitrite).
Yet another possible meaning is mentioned in Diogenes Laertius' biography of Democritus, that Athena was called "Tritogeneia" because three things, on which all mortal life depends, come from her.
## Mythology.
### Birth.
She was the daughter of Zeus, produced without a mother, so that she emerged full-grown from his forehead. There was an alternative story that Zeus swallowed Metis, the goddess of counsel, while she was pregnant with Athena, so that Athena finally emerged from Zeus. Being the favourite child of Zeus, she had great power.
In the classical Olympian pantheon, Athena was regarded as the favorite daughter of Zeus, born fully armed from his forehead. The story of her birth comes in several versions. The earliest mention is in Book V of the "Iliad", when Ares accuses Zeus of being biased in favor of Athena because "autos egeinao" (literally "you fathered her", but probably intended as "you gave birth to her").
She was essentially urban and civilized, the antithesis in many respects of Artemis, goddess of the outdoors. Athena was probably a pre-Hellenic goddess and was later taken over by the Greeks.
In the version recounted by Hesiod in his "Theogony", Zeus married the goddess Metis, who is described as the "wisest among gods and mortal men", and engaged in sexual intercourse with her. After learning that Metis was pregnant, however, he became afraid that the unborn offspring would try to overthrow him, because Gaia and Ouranos had prophesied that Metis would bear children wiser than their father. In order to prevent this, Zeus tricked Metis into letting him swallow her, but it was too late because Metis had already conceived. A later account of the story from the "Bibliotheca" of Pseudo-Apollodorus, written in the second century AD, makes Metis Zeus's unwilling sexual partner, rather than his wife. According to this version of the story, Metis transformed into many different shapes in effort to escape Zeus, but Zeus successfully raped her and swallowed her.
After swallowing Metis, Zeus took six more wives in succession until he married his seventh and present wife, Hera. Then Zeus experienced an enormous headache. He was in such pain that he ordered someone (either Prometheus, Hephaestus, Hermes, Ares, or Palaemon, depending on the sources examined) to cleave his head open with the "labrys", the double-headed Minoan axe. Athena leaped from Zeus's head, fully grown and armed. The "First Homeric Hymn to Athena" states in lines 9–16 that the gods were awestruck by Athena's appearance and even Helios, the god of the sun, stopped his chariot in the sky. Pindar, in his "Seventh Olympian Ode", states that she "cried aloud with a mighty shout" and that "the Sky and mother Earth shuddered before her."
Hesiod states that Hera was so annoyed at Zeus for having given birth to a child on his own that she conceived and bore Hephaestus by herself, but in "Imagines" 2. 27 (trans. Fairbanks), the third-century AD Greek rhetorician Philostratus the Elder writes that Hera "rejoices" at Athena's birth "as though Athena were her daughter also." The second-century AD Christian apologist Justin Martyr takes issue with those pagans who erect at springs images of Kore, whom he interprets as Athena: "They said that Athena was the daughter of Zeus not from intercourse, but when the god had in mind the making of a world through a word ("logos") his first thought was Athena." According to a version of the story in a scholium on the "Iliad" (found nowhere else), when Zeus swallowed Metis, she was pregnant with Athena by the Cyclops Brontes. The "Etymologicum Magnum" instead deems Athena the daughter of the Daktyl Itonos. Fragments attributed by the Christian Eusebius of Caesarea to the semi-legendary Phoenician historian Sanchuniathon, which Eusebius thought had been written before the Trojan war, make Athena instead the daughter of Cronus, a king of Byblos who visited "the inhabitable world" and bequeathed Attica to Athena.
### Pallas Athena.
Athena's epithet "Pallas" is derived either from , meaning "to brandish [as a weapon]", or, more likely, from and related words, meaning "youth, young woman". On this topic, Walter Burkert says "she is the Pallas of Athens, "Pallas Athenaie", just as Hera of Argos is "Here Argeie"." In later times, after the original meaning of the name had been forgotten, the Greeks invented myths to explain its origin, such as those reported by the Epicurean philosopher Philodemus and the "Bibliotheca" of Pseudo-Apollodorus, which claim that "Pallas" was originally a separate entity, whom Athena had slain in combat.
In one version of the myth, Pallas was the daughter of the sea-god Triton; she and Athena were childhood friends, but Athena accidentally killed her during a friendly sparring match. Distraught over what she had done, Athena took the name Pallas for herself as a sign of her grief. In another version of the story, Pallas was a Gigante; Athena slew him during the Gigantomachy and flayed off his skin to make her cloak, which she wore as a victory trophy. In an alternative variation of the same myth, Pallas was instead Athena's father, who attempted to assault his own daughter, causing Athena to kill him and take his skin as a trophy.
The "palladion" was a statue of Athena that was said to have stood in her temple on the Trojan Acropolis. Athena was said to have carved the statue herself in the likeness of her dead friend Pallas. The statue had special talisman-like properties and it was thought that, as long as it was in the city, Troy could never fall. When the Greeks captured Troy, Cassandra, the daughter of Priam, clung to the palladion for protection, but Ajax the Lesser violently tore her away from it and dragged her over to the other captives. Athena was infuriated by this violation of her protection. Although Agamemnon attempted to placate her anger with sacrifices, Athena sent a storm at Cape Kaphereos to destroy almost the entire Greek fleet and scatter all of the surviving ships across the Aegean.
### Lady of Athens.
In Homer's "Iliad", Athena, as a war goddess, inspired and fought alongside the Greek heroes; her aid was synonymous with military prowess. Also in the Iliad, Zeus, the chief god, specifically assigned the sphere of war to Ares, the god of war, and Athena. Athena's moral and military superiority to Ares derived in part from the fact that she represented the intellectual and civilized side of war and the virtues of justice and skill, whereas Ares represented mere blood lust. Her superiority also derived in part from the vastly greater variety and importance of her functions and from the patriotism of Homer's predecessors, Ares being of foreign origin. In the Iliad, Athena was the divine form of the heroic, martial ideal: she personified excellence in close combat, victory, and glory. The qualities that led to victory were found on the aegis, or breastplate, that Athena wore when she went to war: fear, strife, defense, and assault. Athena appears in Homer's Odyssey as the tutelary deity of Odysseus, and myths from later sources portray her similarly as helper of Perseus and Heracles (Hercules). As the guardian of the welfare of kings, Athena became the goddess of good counsel, of prudent restraint and practical insight, as well as of war.
In a founding myth reported by Pseudo-Apollodorus, Athena competed with Poseidon for the patronage of Athens. They agreed that each would give the Athenians one gift and that Cecrops, the king of Athens, would determine which gift was better. Poseidon struck the ground with his trident and a salt water spring sprang up; this gave the Athenians access to trade and water. Athens at its height was a significant sea power, defeating the Persian fleet at the Battle of Salamis—but the water was salty and undrinkable. In an alternative version of the myth from Vergil's "Georgics", Poseidon instead gave the Athenians the first horse. Athena offered the first domesticated olive tree. Cecrops accepted this gift and declared Athena the patron goddess of Athens. The olive tree brought wood, oil, and food, and became a symbol of Athenian economic prosperity. Robert Graves was of the opinion that "Poseidon's attempts to take possession of certain cities are political myths", which reflect the conflict between matriarchal and patriarchal religions.
Pseudo-Apollodorus records an archaic legend, which claims that Hephaestus once attempted to rape Athena, but she pushed him away, causing him to ejaculate on her thigh. Athena wiped the semen off using a tuft of wool, which she tossed into the dust, impregnating Gaia and causing her to give birth to Erichthonius. Athena adopted Erichthonius as her son and raised him. The Roman mythographer Hyginus records a similar story in which Hephaestus demanded Zeus to let him marry Athena since he was the one who had smashed open Zeus's skull, allowing Athena to be born. Zeus agreed to this and Hephaestus and Athena were married, but, when Hephaestus was about to consummate the union, Athena vanished from the bridal bed, causing him to ejaculate on the floor, thus impregnating Gaia with Erichthonius.
The geographer Pausanias records that Athena placed the infant Erichthonius into a small chest ("cista"), which she entrusted to the care of the three daughters of Cecrops: Herse, Pandrosos, and Aglauros of Athens. She warned the three sisters not to open the chest, but did not explain to them why or what was in it. Aglauros, and possibly one of the other sisters, opened the chest. Differing reports say that they either found that the child itself was a serpent, that it was guarded by a serpent, that it was guarded by two serpents, or that it had the legs of a serpent. In Pausanias's story, the two sisters were driven mad by the sight of the chest's contents and hurled themselves off the Acropolis, dying instantly, but an Attic vase painting shows them being chased by the serpent off the edge of the cliff instead.
Erichthonius was one of the most important founding heroes of Athens and the legend of the daughters of Cecrops was a cult myth linked to the rituals of the Arrhephoria festival. Pausanias records that, during the Arrhephoria, two young girls known as the "Arrhephoroi", who lived near the temple of Athena Polias, would be given hidden objects by the priestess of Athena, which they would carry on their heads down a natural underground passage. They would leave the objects they had been given at the bottom of the passage and take another set of hidden objects, which they would carry on their heads back up to the temple. The ritual was performed in the dead of night and no one, not even the priestess, knew what the objects were. The serpent in the story may be the same one depicted coiled at Athena's feet in Pheidias's famous statue of the "Athena Parthenos" in the Parthenon. Many of the surviving sculptures of Athena show this serpent.
Herodotus records that a serpent lived in a crevice on the north side of the summit of the Athenian Acropolis and that the Athenians left a honey cake for it each month as an offering. On the eve of the Second Persian invasion of Greece in 480 BC, the serpent did not eat the honey cake and the Athenians interpreted it as a sign that Athena herself had abandoned them. Another version of the myth of the Athenian maidens is told in "Metamorphoses" by the Roman poet Ovid (43 BC17 AD); in this late variant Hermes falls in love with Herse. Herse, Aglaulus, and Pandrosus go to the temple to offer sacrifices to Athena. Hermes demands help from Aglaulus to seduce Herse. Aglaulus demands money in exchange. Hermes gives her the money the sisters have already offered to Athena. As punishment for Aglaulus's greed, Athena asks the goddess Envy to make Aglaulus jealous of Herse. When Hermes arrives to seduce Herse, Aglaulus stands in his way instead of helping him as she had agreed. He turns her to stone.
### Patron of heroes.
According to Pseudo-Apollodorus's "Bibliotheca", Athena advised Argos, the builder of the "Argo", the ship on which the hero Jason and his band of Argonauts sailed, and aided in the ship's construction. Pseudo-Apollodorus also records that Athena guided the hero Perseus in his quest to behead Medusa. She and Hermes, the god of travelers, appeared to Perseus after he set off on his quest and gifted him with tools he would need to kill the Gorgon. Athena gave Perseus a polished bronze shield to view Medusa's reflection rather than looking at her directly and thereby avoid being turned to stone. Hermes gave him an adamantine scythe to cut off Medusa's head. When Perseus swung his blade to behead Medusa, Athena guided it, allowing his scythe to cut it clean off. According to Pindar's "Thirteenth Olympian Ode", Athena helped the hero Bellerophon tame the winged horse Pegasus by giving him a bit.
In ancient Greek art, Athena is frequently shown aiding the hero Heracles. She appears in four of the twelve metopes on the Temple of Zeus at Olympia depicting Heracles's Twelve Labors, including the first, in which she passively watches him slay the Nemean lion, and the tenth, in which she is shown actively helping him hold up the sky. She is presented as his "stern ally", but also the "gentle... acknowledger of his achievements." Artistic depictions of Heracles's apotheosis show Athena driving him to Mount Olympus in her chariot and presenting him to Zeus for his deification. In Aeschylus's tragedy "Orestes", Athena intervenes to save Orestes from the wrath of the Erinyes and presides over his trial for the murder of his mother Clytemnestra. When half the jury votes to acquit and the other half votes to convict, Athena casts the deciding vote to acquit Orestes and declares that, from then on, whenever a jury is tied, the defendant shall always be acquitted.
In "The Odyssey", Odysseus' cunning and shrewd nature quickly wins Athena's favour. For the first part of the poem, however, she largely is confined to aiding him only from "afar", mainly by implanting thoughts in his head during his journey home from Troy. Her guiding actions reinforce her role as the "protectress of heroes," or, as mythologian Walter Friedrich Otto dubbed her, the "goddess of nearness," due to her mentoring and motherly probing. It is not until he washes up on the shore of the island of the Phaeacians, where Nausicaa is washing her clothes that Athena arrives personally to provide more tangible assistance. She appears in Nausicaa's dreams to ensure that the princess rescues Odysseus and plays a role in his eventual escort to Ithaca. Athena appears to Odysseus upon his arrival, disguised as a herdsman; she initially lies and tells him that Penelope, his wife, has remarried and that he is believed to be dead, but Odysseus lies back to her, employing skillful prevarications to protect himself. Impressed by his resolve and shrewdness, she reveals herself and tells him what he needs to know in order to win back his kingdom. She disguises him as an elderly beggar so that he will not be recognized by the suitors or Penelope, and helps him to defeat the suitors. Athena also appears to Odysseus's son Telemachus. Her actions lead him to travel around to Odysseus's comrades and ask about his father. He hears stories about some of Odysseus's journey. Athena's push for Telemachos's journey helps him grow into the man role, that his father once held. She also plays a role in ending the resultant feud against the suitors' relatives. She instructs Laertes to throw his spear and to kill Eupeithes, the father of Antinous.
### Punishment myths.
The Gorgoneion appears to have originated as an apotropaic symbol intended to ward off evil. In a late myth invented to explain the origins of the Gorgon, Medusa is described as having been a young priestess who served in the temple of Athena in Athens. Poseidon lusted after Medusa, and raped her in the temple of Athena, refusing to allow her vow of chastity to stand in his way. Upon discovering the desecration of her temple, Athena transformed Medusa into a hideous monster with serpents for hair whose gaze would turn any mortal to stone.
In his "Twelfth Pythian Ode", Pindar recounts the story of how Athena invented the "aulos", a kind of flute, in imitation of the lamentations of Medusa's sisters, the Gorgons, after she was beheaded by the hero Perseus. According to Pindar, Athena gave the aulos to mortals as a gift. Later, the comic playwright Melanippides of Melos ( 480-430 BC) embellished the story in his comedy "Marsyas", claiming that Athena looked in the mirror while she was playing the aulos and saw how blowing into it puffed up her cheeks and made her look silly, so she threw the aulos away and cursed it so that whoever picked it up would meet an awful death. The aulos was picked up by the satyr Marsyas, who was later killed by Apollo for his hubris. Later, this version of the story became accepted as canonical and the Athenian sculptor Myron created a group of bronze sculptures based on it, which was installed before the western front of the Parthenon in around 440 BC.
A myth told by the early third-century BC Hellenistic poet Callimachus in his "Hymn" 5 begins with Athena bathing in a spring on Mount Helicon at midday with one of her favorite companions, the nymph Chariclo. Chariclo's son Tiresias happened to be hunting on the same mountain and came to the spring searching for water. He inadvertently saw Athena naked, so she struck him blind to ensure he would never again see what man was not intended to see. Chariclo intervened on her son's behalf and begged Athena to have mercy. Athena replied that she could not restore Tiresias's eyesight, so, instead, she gave him the ability to understand the language of the birds and thus foretell the future.
The fable of Arachne appears in Ovid's "Metamorphoses" (8 AD) (vi.5–54 and 129–145), which is nearly the only extant source for the legend. The story does not appear to have been well known prior to Ovid's rendition of it and the only earlier reference to it is a brief allusion in Virgil's "Georgics", (29 BC) (iv, 246) that does not mention Arachne by name. According to Ovid, Arachne (whose name means "spider" in ancient Greek) was the daughter of a famous dyer in Tyrian purple in Hypaipa of Lydia, and a weaving student of Athena. She became so conceited of her skill as a weaver that she began claiming that her skill was greater than that of Athena herself. Athena gave Arachne a chance to redeem herself by assuming the form of an old woman and warning Arachne not to offend the deities. Arachne scoffed and wished for a weaving contest, so she could prove her skill.
Athena wove the scene of her victory over Poseidon in the contest for the patronage of Athens. Athena's tapestry also depicted the 12 Olympian gods and defeat of mythological figures who challenged their authority. Arachne's tapestry featured twenty-one episodes of the deities' infidelity, including Zeus being unfaithful with Leda, with Europa, and with Danaë. It represented the unjust and discrediting behavior of the gods towards mortals. Athena admitted that Arachne's work was flawless, but was outraged at Arachne's offensive choice of subject, which displayed the failings and transgressions of the deities. Finally, losing her temper, Athena destroyed Arachne's tapestry and loom, striking it with her shuttle. Athena then struck Arachne across the face with her staff four times. Arachne hanged herself in despair, but Athena took pity on her and brought her back from the dead in the form of a spider.
### Trojan War.
The myth of the Judgement of Paris is mentioned briefly in the "Iliad", but is described in depth in an epitome of the "Cypria", a lost poem of the Epic Cycle, which records that all the gods and goddesses as well as various mortals were invited to the marriage of Peleus and Thetis (the eventual parents of Achilles). Only Eris, goddess of discord, was not invited. She was annoyed at this, so she arrived with a golden apple inscribed with the word καλλίστῃ (kallistēi, "for the fairest"), which she threw among the goddesses. Aphrodite, Hera, and Athena all claimed to be the fairest, and thus the rightful owner of the apple.
The goddesses chose to place the matter before Zeus, who, not wanting to favor one of the goddesses, put the choice into the hands of Paris, a Trojan prince. After bathing in the spring of Mount Ida where Troy was situated, the goddesses appeared before Paris for his decision. In the extant ancient depictions of the Judgement of Paris, Aphrodite is only occasionally represented nude, and Athena and Hera are always fully clothed. Since the Renaissance, however, Western paintings have typically portrayed all three goddesses as completely naked.
All three goddesses were ideally beautiful and Paris could not decide between them, so they resorted to bribes. Hera tried to bribe Paris with power over all Asia and Europe, and Athena offered fame and glory in battle, but Aphrodite promised Paris that, if he were to choose her as the fairest, she would let him marry the most beautiful woman on earth. This woman was Helen, who was already married to King Menelaus of Sparta. Paris selected Aphrodite and awarded her the apple. The other two goddesses were enraged and, as a direct result, sided with the Greeks in the Trojan War.
In Books V–VI of the "Iliad", Athena aids the hero Diomedes, who, in the absence of Achilles, proves himself to be the most effective Greek warrior. Several artistic representations from the early sixth century BC may show Athena and Diomedes, including an early sixth-century BC shield band depicting Athena and an unidentified warrior riding on a chariot, a vase painting of a warrior with his charioteer facing Athena, and an inscribed clay plaque showing Diomedes and Athena riding in a chariot. Numerous passages in the "Iliad" also mention Athena having previously served as the patron of Diomedes's father Tydeus. When the Trojan women go to the temple of Athena on the Acropolis to plead her for protection from Diomedes, Athena ignores them.
Athena also gets into a duel with Ares, the god of the brutal wars, and her male counterpart. Ares blames her for encouraging Diomedes to tear his beautiful flesh. He curses her and strikes with all his strength. Athena, however, cleverly deflects his blow with her aegis, a powerful shield which even Zeus's thunderbolt and lightning cannot blast through. Athena picked up a massive boulder and threw it at Ares, who immediately crumpled to the ground. Aphrodite, who was then a lover of the war god came down from Olympus to carry Ares away but was struck by Athena's golden spear and fell. Athena taunted the gods who supported Troy, saying that they will too eventually end up like Ares and Aphrodite, which scared them, therefore proving her power and reputation among the other gods.
In Book XXII of the "Iliad", while Achilles is chasing Hector around the walls of Troy, Athena appears to Hector disguised as his brother Deiphobus and persuades him to hold his ground so that they can fight Achilles together. Then, Hector throws his spear at Achilles and misses, expecting Deiphobus to hand him another, but Athena disappears instead, leaving Hector to face Achilles alone without his spear. In Sophocles's tragedy "Ajax", she punishes Odysseus's rival Ajax the Great, driving him insane and causing him to massacre the Achaeans' cattle, thinking that he is slaughtering the Achaeans themselves. Even after Odysseus himself expresses pity for Ajax, Athena declares, "To laugh at your enemies - what sweeter laughter can there be than that?" (lines 78–9). Ajax later commits suicide as a result of his humiliation.
## Classical art.
Athena appears frequently in classical Greek art, including on coins and in paintings on ceramics. She is especially prominent in works produced in Athens. In classical depictions, Athena is usually portrayed standing upright, wearing a full-length chiton. She is most often represented dressed in armor like a male soldier and wearing a Corinthian helmet raised high atop her forehead. Her shield bears at its centre the aegis with the head of the gorgon ("gorgoneion") in the center and snakes around the edge. Sometimes she is shown wearing the aegis as a cloak. As Athena Promachos, she is shown brandishing a spear. Scenes in which Athena was represented include her birth from the head of Zeus, her battle with the Gigantes, the birth of Erichthonius, and the Judgement of Paris.
The "Mourning Athena" or "Athena Meditating" is a famous relief sculpture dating to around 470-460 BC that has been interpreted to represent Athena Polias. The most famous classical depiction of Athena was the "Athena Parthenos", a now-lost gold and ivory statue of her in the Parthenon created by the Athenian sculptor Phidias. Copies reveal that this statue depicted Athena holding her shield in her left hand with Nike, the winged goddess of victory, standing in her right. Athena Polias is also represented in a Neo-Attic relief now held in the Virginia Museum of Fine Arts, which depicts her holding an owl in her hand and wearing her characteristic Corinthian helmet while resting her shield against a nearby "herma". The Roman goddess Minerva adopted most of Athena's Greek iconographical associations, but was also integrated into the Capitoline Triad.
## Post-classical culture.
### Art and symbolism.
Early Christian writers, such as Clement of Alexandria and Firmicus, denigrated Athena as representative of all the things that were detestable about paganism; they condemned her as "immodest and immoral". During the Middle Ages, however, many attributes of Athena were given to the Virgin Mary, who, in fourth century portrayals, was often depicted wearing the Gorgoneion. Some even viewed the Virgin Mary as a warrior maiden, much like Athena Parthenos; one anecdote tells that the Virgin Mary once appeared upon the walls of Constantinople when it was under siege by the Avars, clutching a spear and urging the people to fight. During the Middle Ages, Athena became widely used as a Christian symbol and allegory, and she appeared on the family crests of certain noble houses.
During the Renaissance, Athena donned the mantle of patron of the arts and human endeavor; allegorical paintings involving Athena were a favorite of the Italian Renaissance painters. In Sandro Botticelli's painting "Pallas and the Centaur", probably painted sometime in the 1480s, Athena is the personification of chastity, who is shown grasping the forelock of a centaur, who represents lust. Andrea Mantegna's 1502 painting "Minerva Expelling the Vices from the Garden of Virtue" uses Athena as the personification of Graeco-Roman learning chasing the vices of medievalism from the garden of modern scholarship. Athena is also used as the personification of wisdom in Bartholomeus Spranger's 1591 painting "The Triumph of Wisdom" or "Minerva Victorious over Ignorance".
During the sixteenth and seventeenth centuries, Athena was used as a symbol for female rulers. In his book "A Revelation of the True Minerva" (1582), Thomas Blennerhassett portrays Queen Elizabeth I of England as a "new Minerva" and "the greatest goddesse nowe on earth". A series of paintings by Peter Paul Rubens depict Athena as Marie de' Medici's patron and mentor; the final painting in the series goes even further and shows Marie de' Medici with Athena's iconography, as the mortal incarnation of the goddess herself. The Flemish sculptor Jean-Pierre-Antoine Tassaert (Jan Peter Anton Tassaert) later portrayed Catherine II of Russia as Athena in a marble bust in 1774. During the French Revolution, statues of pagan gods were torn down all throughout France, but statues of Athena were not. Instead, Athena was transformed into the personification of freedom and the republic and a statue of the goddess stood in the center of the Place de la Revolution in Paris. In the years following the Revolution, artistic representations of Athena proliferated.
A statue of Athena stands directly in front of the Austrian Parliament Building in Vienna, and depictions of Athena have influenced other symbols of Western freedom, including the Statue of Liberty and Britannia. For over a century, a full-scale replica of the Parthenon has stood in Nashville, Tennessee. In 1990, the curators added a gilded forty-two-foot (12.5 m) tall replica of Phidias's "Athena Parthenos", built from concrete and fiberglass. The Great Seal of California bears the image of Athena kneeling next to a brown grizzly bear. Athena has occasionally appeared on modern coins, as she did on the ancient Athenian drachma. Her head appears on the $50 1915-S Panama-Pacific commemorative coin.
### Modern interpretations.
One of Sigmund Freud's most treasured possessions was a small, bronze sculpture of Athena, which sat on his desk. Freud once described Athena as "a woman who is unapproachable and repels all sexual desires - since she displays the terrifying genitals of the Mother." Feminist views on Athena are sharply divided; some feminists regard her as a symbol of female empowerment, while others regard her as "the ultimate patriarchal sell out... who uses her powers to promote and advance men rather than others of her sex." In contemporary Wicca, Athena is venerated as an aspect of the Goddess and some Wiccans believe that she may bestow the "Owl Gift" ("the ability to write and communicate clearly") upon her worshippers. Due to her status as one of the twelve Olympians, Athena is a major deity in Hellenismos, a Neopagan religion which seeks to authentically revive and recreate the religion of ancient Greece in the modern world.
Athena is a natural patron of universities: At Bryn Mawr College in Pennsylvania a statue of Athena (a replica of the original bronze one in the arts and archaeology library) resides in the Great Hall. It is traditional at exam time for students to leave offerings to the goddess with a note asking for good luck, or to repent for accidentally breaking any of the college's numerous other traditions. Pallas Athena is the tutelary goddess of the international social fraternity Phi Delta Theta. Her owl is also a symbol of the fraternity.

</doc>
<doc id="1183" url="https://en.wikipedia.org/wiki?curid=1183" title="Amber Diceless Roleplaying Game">
Amber Diceless Roleplaying Game

The Amber Diceless Roleplaying Game is a role-playing game created and written by Erick Wujcik, set in the fictional universe created by author Roger Zelazny for his "Chronicles of Amber". The game is unusual in that no dice are used in resolving conflicts or player actions; instead a simple diceless system of comparative ability, and narrative description of the action by the players and gamemaster, is used to determine how situations are resolved.
"Amber DRPG" was created in the 1980s, and is much more focused on relationships and roleplaying than most of the roleplaying games of that era. Most "Amber" characters are members of the two ruling classes in the "Amber" multiverse, and are much more advanced in matters of strength, endurance, psyche, warfare and sorcery than ordinary beings. This often means that the only individuals who are capable of opposing a character are from his or her family, a fact that leads to much suspicion and intrigue.
## History.
Erick Wujcik offered to design an Amber role-playing game for West End Games, who agreed to look at his work. Wujcik intended to integrate the feel of the "Amber" setting from the novels into a role-playing game, and playtested his system for a few months at the Michigan Gaming Center where he decided to try it out as a diceless game. West End was not interested in a diceless role-playing game, so Wujcik acquired the RPG rights to "Amber" and took the game to R. Talsorian Games, until he withdrew over creative differences. Wujcik then founded Phage Press, and published "Amber Diceless Role-playing" in 1991.
The original 256-page game book was published in 1991 by Phage Press, covering material from the first five novels (the "Corwin Cycle") and some details – sorcery and the Logrus – from the remaining five novels (the "Merlin Cycle"), in order to allow players to roleplay characters from the Courts of Chaos. Some details were changed slightly to allow more player choice – for example, players can be full Trump Artists without having walked the Pattern or the Logrus, which Merlin says is impossible; and players' psychic abilities are far greater than those shown in the books.
A 256-page companion volume, "Shadow Knight", was published in 1993. This supplemental rule book includes the remaining elements from the Merlin novels, such as Broken Patterns, and allows players to create Constructs such as Merlin's Ghostwheel. The book presents the second series of novels not as additions to the series' continuity but as an example of a roleplaying campaign with Merlin, Luke, Julia, Jurt and Coral as the PCs. The remainder of the book is a collection of essays on the game, statistics for the new characters and an update of the older ones in light of their appearance in the second series, and (perhaps most usefully for GMs) plot summaries of each of the ten books. The book includes some material from the short story "The Salesman's Tale," and some unpublished material cut from "Prince of Chaos", notably Coral's pregnancy by Merlin.
Both books were translated into French and published by Jeux Descartes in 1994 and 1995.
A third book, "Rebma", was promised. Cover art was commissioned and pre-orders were taken, but it was never published. Wujcik also expressed a desire to create a book giving greater detail to the Courts of Chaos. The publishing rights to the "Amber DRPG" games were acquired in 2004 by Guardians of Order, who took over sales of the game and announced their intention to release a new edition of the game. However, no new edition was released before Guardians of Order went out of business in 2006. The two existing books are now out-of-print, but they have been made available as PDF downloads.
In June 2007 a new publishing company, headed by Edwin Voskamp and Eric Todd, was formed with the express purpose of bringing "Amber DRPG" back into print. The new company is named "Diceless by Design".
In May 2010, "Rite Publishing" secured a license from Diceless by Design to use the rules system with a new setting in the creation of a new product to be written by industry and system veteran Jason Durall. The project Lords of Gossamer &amp; Shadow (Diceless) was funded via Kickstarter in May 2013. In Sept 2013 the project was completed, and on in Nov 2013 Lords of Gossamer and Shadow (Diceless) was released publicly in full-color Print and PDF, along with additional supplements and continued support.
## Setting.
The game is set in the multiverse described in Zelazny's "Chronicles of Amber". The first book assumes that gamemasters will set their campaigns after the Patternfall war; that is, after the end of the fifth book in the series, "The Courts of Chaos", but uses material from the following books to describe those parts of Zelazny's cosmology that were featured there in more detail. The "Amber" multiverse consists of Amber, a city at one pole of the universe wherein is found the Pattern, the symbol of Order; The Courts of Chaos, an assembly of worlds at the other pole where can be found the Logrus, the manifestation of Chaos, and the Abyss, the source or end of all reality; and Shadow, the collection of all possible universes (shadows) between and around them. Inhabitants of either pole can use one or both of the Pattern and the Logrus to travel through Shadow.
It is assumed that players will portray the children of the main characters from the books – the ruling family of Amber, known as the Elder Amberites – or a resident of the Courts. However, since some feel that being the children of the main characters is too limiting, it is fairly common to either start with King Oberon's death "before" the book begins and roleplay the Elder Amberites as they vie for the throne; or to populate Amber from scratch with a different set of Elder Amberites. The former option is one presented in the book; the latter is known in the Amber community as an "Amethyst" game. A third option is to have the players portray Corwin's children, in an Amber-like city built around Corwin's pattern; this is sometimes called an "Argent" game, since one of Corwin's heraldic colours is Silver.
## System.
### Attributes.
Characters in "Amber DRPG" are represented by four attributes: "Psyche", "Strength", "Endurance" and "Warfare".
The attributes run from −25 (normal human level), through −10 (normal level for a denizen of the Courts of Chaos) and 0 (normal level for an inhabitant of Amber), upwards without limit. Scores above 0 are "ranked", with the highest score being ranked 1st, the next-highest 2nd, and so on. The character with 1st rank in each attribute is considered "superior" in that attribute, being considered to be substantially better than the character with 2nd rank even if the difference in scores is small. All else being equal, a character with a higher rank in an attribute will always win a contest based on that attribute.
#### The Attribute Auction.
A character's ability scores are purchased during character creation in an auction; players get 100 character points, and bid on each attribute in turn. The character who bids the most for an attribute is "ranked" first and is considered superior to all other characters in that attribute. Unlike conventional auctions, bids are non-refundable; if one player bids 65 for psyche and another wins with a bid of 66, then the character with 66 is "superior" to the character with 65 even though there is only one bid difference. Instead, lower bidding characters are ranked in ascending order according to how much they have bid, the characters becoming progressively weaker in that attribute as they pay less for it. After the auction, players can secretly pay extra points to raise their ranks, but they can only pay to raise their scores to an existing rank. Further, a character with a bid-for rank is considered to have a slight advantage over character with a bought-up rank.
The Auction simulates a 'history' of competition between the descendants of Oberon for player characters who have not had dozens of decades to get to know each other. Through the competitive Auction, characters may begin the game vying for standings. The auction serves to introduce some unpredictability into character creation without the need to resort to dice, cards, or other randomizing devices. A player may intend, for example, to create a character who is a strong, mighty warrior, but being "outplayed" in the auction may result in lower attribute scores than anticipated, therefore necessitating a change of character concept. Since a player cannot control another player's bids, and since all bids are non-refundable, the auction involves a considerable amount of strategizing and prioritization by players. A willingness to spend as many points as possible on an attribute may improve your chances of a high ranking, but too reckless a spending strategy could leave a player with few points to spend on powers and objects. In a hotly contested auction, such as for the important attribute of warfare, the most valuable skill is the ability to force one's opponents to back down. With two or more equally determined players, this can result in a "bidding war," in which the attribute is driven up by increments to large sums. An alternative strategy is to try to cow other players into submission with a high opening bid. Most players bid low amounts between one and ten points in an initial bid in order to feel out the competition and to save points for other uses. A high enough opening bid could signal a player's determination to be first ranked in that attribute, thereby dissuading others from competing.
#### Psyche in "Amber DRPG" compared to the "Chronicles".
Characters with high psyche are presented as having strong telepathic abilities, being able to hypnotise and even mentally dominate any character with lesser psyche with whom they can make eye-contact. This is likely due to three scenes in the "Chronicles": first, when Eric paralyzes Corwin with an attack across the Trump and refuses to desist because one or the other would be dominated; second, when Corwin faces the demon Strygalldwir, it is able to wrestle mentally with him when their gazes meet; and third, when Fiona is able to keep Brand immobile in the final battle at the Courts of Chaos. However, in general, the books only feature mental battles when there is some reason for mind-to-mind contact (for example, Trump contact) and magic or Trump is involved in all three of the above conflicts, so it is not clear whether Zelazny intended his characters to have such a power; the combination of Brand's "living trump" powers and his high Psyche (as presented in the roleplaying game) would have guaranteed him victory over Corwin. "Shadow Knight" does address this inconsistency somewhat, by presenting the "living trump" abilities as somewhat limited.
### Powers.
Characters in "Amber DRPG" have access to the powers seen in the "Chronicles of Amber": "Pattern", "Logrus", "Shape-shifting", "Trump", and "magic".
Each of the first four powers is available in an advanced form.
### Artifacts, Personal shadows and Constructs.
While a character with Pattern, Logrus or Conjuration can acquire virtually any object, players can choose to spend character points to obtain objects with particular virtues – unbreakability, or a mind of their own. Since they have paid points for the items, they are a part of the character's legend, and cannot lightly be destroyed. Similarly, a character can find any possible universe, but they can spend character points to know of or inhabit shadows which are (in some sense) "real" and therefore useful. The expansion, "Shadow Knight", adds Constructs – artifacts with connections to shadows.
### Stuff.
Unspent character points become good stuff – a good luck for the character. Players are also allowed to overspend (in moderation), with the points becoming bad stuff – bad luck which the Gamemaster should inflict on the character. Stuff governs how non-player characters perceive and respond to the character: characters with good stuff will often receive friendly or helpful reactions, while characters with bad stuff are often treated with suspicion or hostility.
As well as representing luck, stuff can be seen as representing a character's outlook on the universe: characters with good stuff seeing the multiverse as a cheerful place, while characters with bad stuff see it as hostile.
### Conflict resolution.
In any given fair conflict between two characters, the character with the higher score in the relevant attribute will eventually win. The key words here are "fair" and "eventually" – if characters' ranks are close, and the weaker character has obtained some advantage, then the weaker character can escape defeat or perhaps prevail. Close ranks result in longer contests while greater difference between ranks result in fast resolution. Alternatively, if characters' attribute ranks are close, the weaker character can try to change the relevant attribute by changing the nature of the conflict. For example, if two characters are wrestling the relevant attribute is Strength; a character could reveal a weapon, changing it to Warfare; they could try to overcome the other character's mind using a power, changing it to Psyche; or they could concentrate their strength on defense, changing it to Endurance. If there is a substantial difference between characters' ranks, the conflict is generally over before the weaker character can react.
### The "Golden Rule".
"Amber DRPG" advises gamemasters to change rules as they see fit, even to the point of adding or removing powers or attributes.
## Reception.
In the June 1992 edition of "Dragon" (Issue 182), both Lester Smith and Allen Varney published reviews of this game.
Loyd Blankenship reviewed "Amber" in "Pyramid" #2 (July/Aug., 1993), and stated that ""Amber" is a valuable resource to a GM - even if he isn't running an "Amber" game. For gamers who have an aspiring actor or actress lurking within their breast, or for someone running a campaign via electronic mail or message base, "Amber" should be given serious consideration."
## Community.
Despite the game's out-of-print status, a thriving convention scene exists supporting the game. Amber conventions, known as "Ambercons", are held yearly in Massachusetts, Michigan, Portland (United States), Milton Keynes (England), Belfast (Northern Ireland) and Modena, Italy. Additionally, Phage Press published 12 volumes of a dedicated "Amber DRPG" magazine called "Amberzine". Some "Amberzine" issues are still available from Phage Press.

</doc>
<doc id="1184" url="https://en.wikipedia.org/wiki?curid=1184" title="Athene (disambiguation)">
Athene (disambiguation)

Athene or Athena is the shrewd companion of heroes and the goddess of heroic endeavour in Greek mythology.
Athene may also refer to:

</doc>
<doc id="1186" url="https://en.wikipedia.org/wiki?curid=1186" title="AphexTwin">
AphexTwin



</doc>
<doc id="1187" url="https://en.wikipedia.org/wiki?curid=1187" title="Alloy">
Alloy

An alloy is an admixture of metals, or a metal combined with one or more other elements. For example, combining the metallic elements gold and copper produces red gold, gold and silver becomes white gold, and silver combined with copper produces sterling silver. Combining iron with non-metallic carbon or silicon produces alloys called steel or silicon steel. The resulting mixture forms a substance with properties that often differ from those of the pure metals, such as increased strength or hardness. Unlike other substances that may contain metallic bases but do not behave as metals, such as aluminium oxide (sapphire), beryllium aluminium silicate (emerald) or sodium chloride (salt), an alloy will retain all the properties of a metal in the resulting material, such as electrical conductivity, ductility, opacity, and luster. Alloys are used in a wide variety of applications, from the steel alloys, used in everything from buildings to automobiles to surgical tools, to exotic titanium alloys used in the aerospace industry, to beryllium-copper alloys for non-sparking tools. In some cases, a combination of metals may reduce the overall cost of the material while preserving important properties. In other cases, the combination of metals imparts synergistic properties to the constituent metal elements such as corrosion resistance or mechanical strength. Examples of alloys are steel, solder, brass, pewter, duralumin, bronze, and amalgams.
An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal). Intermetallic compounds are alloys with a defined stoichiometry and crystal structure. Zintl phases are also sometimes considered alloys depending on bond types (see Van Arkel–Ketelaar triangle for information on classifying bonding in binary compounds).
Alloys are defined by a metallic bonding character. The alloy constituents are usually measured by mass percentage for practical applications, and in atomic fraction for basic science studies. Alloys are usually classified as substitutional or interstitial alloys, depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous (consisting of a single phase), or heterogeneous (consisting of two or more phases) or intermetallic.
## Introduction.
An alloy is a mixture of chemical elements, which forms an impure substance (admixture) that retains the characteristics of a metal. An alloy is distinct from an impure metal in that, with an alloy, the added elements are well controlled to produce desirable properties, while impure metals such as wrought iron are less controlled, but are often considered useful. Alloys are made by mixing two or more elements, at least one of which is a metal. This is usually called the primary metal or the base metal, and the name of this metal may also be the name of the alloy. The other constituents may or may not be metals but, when mixed with the molten base, they will be soluble and dissolve into the mixture.
The mechanical properties of alloys will often be quite different from those of its individual constituents. A metal that is normally very soft (malleable), such as aluminium, can be altered by alloying it with another soft metal, such as copper. Although both metals are very soft and ductile, the resulting aluminium alloy will have much greater strength. Adding a small amount of non-metallic carbon to iron trades its great ductility for the greater strength of an alloy called steel. Due to its very-high strength, but still substantial toughness, and its ability to be greatly altered by heat treatment, steel is one of the most useful and common alloys in modern use. By adding chromium to steel, its resistance to corrosion can be enhanced, creating stainless steel, while adding silicon will alter its electrical characteristics, producing silicon steel.
Like oil and water, a molten metal may not always mix with another element. For example, pure iron is almost completely insoluble with copper. Even when the constituents are soluble, each will usually have a saturation point, beyond which no more of the constituent can be added. Iron, for example, can hold a maximum of 6.67% carbon. Although the elements of an alloy usually must be soluble in the liquid state, they may not always be soluble in the solid state. If the metals remain soluble when solid, the alloy forms a solid solution, becoming a homogeneous structure consisting of identical crystals, called a phase. If as the mixture cools the constituents become insoluble, they may separate to form two or more different types of crystals, creating a heterogeneous microstructure of different phases, some with more of one constituent than the other. However, in other alloys, the insoluble elements may not separate until after crystallization occurs. If cooled very quickly, they first crystallize as a homogeneous phase, but they are supersaturated with the secondary constituents. As time passes, the atoms of these supersaturated alloys can separate from the crystal lattice, becoming more stable, and forming a second phase that serves to reinforce the crystals internally.
Some alloys, such as electrum—an alloy of silver and gold—occur naturally. Meteorites are sometimes made of naturally occurring alloys of iron and nickel, but are not native to the Earth. One of the first alloys made by humans was bronze, which is a mixture of the metals tin and copper. Bronze was an extremely useful alloy to the ancients, because it is much stronger and harder than either of its components. Steel was another common alloy. However, in ancient times, it could only be created as an accidental byproduct from the heating of iron ore in fires (smelting) during the manufacture of iron. Other ancient alloys include pewter, brass and pig iron. In the modern age, steel can be created in many forms. Carbon steel can be made by varying only the carbon content, producing soft alloys like mild steel or hard alloys like spring steel. Alloy steels can be made by adding other elements, such as chromium, molybdenum, vanadium or nickel, resulting in alloys such as high-speed steel or tool steel. Small amounts of manganese are usually alloyed with most modern steels because of its ability to remove unwanted impurities, like phosphorus, sulfur and oxygen, which can have detrimental effects on the alloy. However, most alloys were not created until the 1900s, such as various aluminium, titanium, nickel, and magnesium alloys. Some modern superalloys, such as incoloy, inconel, and hastelloy, may consist of a multitude of different elements.
## Terminology.
As a noun, the term alloy is used to describe a mixture of atoms in which the primary constituent is a metal. When used as a verb, the term refers to the act of mixing a metal with other elements. The primary metal is called the "base", the "matrix", or the "solvent". The secondary constituents are often called "solutes". If there is a mixture of only two types of atoms (not counting impurities) such as a copper-nickel alloy, then it is called a "binary alloy." If there are three types of atoms forming the mixture, such as iron, nickel and chromium, then it is called a "ternary alloy." An alloy with four constituents is a "quaternary alloy," while a five-part alloy is termed a "quinary alloy." Because the percentage of each constituent can be varied, with any mixture the entire range of possible variations is called a "system". In this respect, all of the various forms of an alloy containing only two constituents, like iron and carbon, is called a "binary system," while all of the alloy combinations possible with a ternary alloy, such as alloys of iron, carbon and chromium, is called a "ternary system".
An alloy is technically an impure metal, but when referring to alloys, the term "impurities" usually denotes undesirable elements. Such impurities are introduced from the base metals and alloying elements, but are removed during processing. For instance, sulfur is a common impurity in steel. Sulfur combines readily with iron to form iron sulfide, which is very brittle, creating weak spots in the steel. Lithium, sodium and calcium are common impurities in aluminium alloys, which can have adverse effects on the structural integrity of castings. Conversely, otherwise pure-metals that simply contain unwanted impurities are often called "impure metals" and are not usually referred to as alloys. Oxygen, present in the air, readily combines with most metals to form metal oxides; especially at higher temperatures encountered during alloying. Great care is often taken during the alloying process to remove excess impurities, using fluxes, chemical additives, or other methods of extractive metallurgy.
In practice, some alloys are used so predominantly with respect to their base metals that the name of the primary constituent is also used as the name of the alloy. For example, 14 karat gold is an alloy of gold with other elements. Similarly, the silver used in jewelry and the aluminium used as a structural building material are also alloys.
The term "alloy" is sometimes used in everyday speech as a synonym for a particular alloy. For example, automobile wheels made of an aluminium alloy are commonly referred to as simply "alloy wheels", although in point of fact steels and most other metals in practical use are also alloys. Steel is such a common alloy that many items made from it, like wheels, barrels, or girders, are simply referred to by the name of the item, assuming it is made of steel. When made from other materials, they are typically specified as such, (i.e.: "bronze wheel", "plastic barrel", or "wood girder").
## Theory.
Alloying a metal is done by combining it with one or more other elements. The most common and oldest alloying process is performed by heating the base metal beyond its melting point and then dissolving the solutes into the molten liquid, which may be possible even if the melting point of the solute is far greater than that of the base. For example, in its liquid state, titanium is a very strong solvent capable of dissolving most metals and elements. In addition, it readily absorbs gases like oxygen and burns in the presence of nitrogen. This increases the chance of contamination from any contacting surface, and so must be melted in vacuum induction-heating and special, water-cooled, copper crucibles. However, some metals and solutes, such as iron and carbon, have very high melting-points and were impossible for ancient people to melt. Thus, alloying (in particular, interstitial alloying) may also be performed with one or more constituents in a gaseous state, such as found in a blast furnace to make pig iron (liquid-gas), nitriding, carbonitriding or other forms of case hardening (solid-gas), or the cementation process used to make blister steel (solid-gas). It may also be done with one, more, or all of the constituents in the solid state, such as found in ancient methods of pattern welding (solid-solid), shear steel (solid-solid), or crucible steel production (solid-liquid), mixing the elements via solid-state diffusion.
By adding another element to a metal, differences in the size of the atoms create internal stresses in the lattice of the metallic crystals; stresses that often enhance its properties. For example, the combination of carbon with iron produces steel, which is stronger than iron, its primary element. The electrical and thermal conductivity of alloys is usually lower than that of the pure metals. The physical properties, such as density, reactivity, Young's modulus of an alloy may not differ greatly from those of its base element, but engineering properties such as tensile strength, ductility, and shear strength may be substantially different from those of the constituent materials. This is sometimes a result of the sizes of the atoms in the alloy, because larger atoms exert a compressive force on neighboring atoms, and smaller atoms exert a tensile force on their neighbors, helping the alloy resist deformation. Sometimes alloys may exhibit marked differences in behavior even when small amounts of one element are present. For example, impurities in semiconducting ferromagnetic alloys lead to different properties, as first predicted by White, Hogan, Suhl, Tian Abrie and Nakamura.
Some alloys are made by melting and mixing two or more metals. Bronze, an alloy of copper and tin, was the first alloy discovered, during the prehistoric period now known as the Bronze Age. It was harder than pure copper and originally used to make tools and weapons, but was later superseded by metals and alloys with better properties. In later times bronze has been used for ornaments, bells, statues, and bearings. Brass is an alloy made from copper and zinc.
Unlike pure metals, most alloys do not have a single melting point, but a melting range during which the material is a mixture of solid and liquid phases (a slush). The temperature at which melting begins is called the solidus, and the temperature when melting is just complete is called the liquidus. For many alloys there is a particular alloy proportion (in some cases more than one), called either a eutectic mixture or a peritectic composition, which gives the alloy a unique and low melting point, and no liquid/solid slush transition.
## Heat-treatable alloys.
Alloying elements are added to a base metal, to induce hardness, toughness, ductility, or other desired properties. Most metals and alloys can be work hardened by creating defects in their crystal structure. These defects are created during plastic deformation by hammering, bending, extruding, et cetera, and are permanent unless the metal is recrystallized. Otherwise, some alloys can also have their properties altered by heat treatment. Nearly all metals can be softened by annealing, which recrystallizes the alloy and repairs the defects, but not as many can be hardened by controlled heating and cooling. Many alloys of aluminium, copper, magnesium, titanium, and nickel can be strengthened to some degree by some method of heat treatment, but few respond to this to the same degree as does steel.
The base metal iron of the iron-carbon alloy known as steel, undergoes a change in the arrangement (allotropy) of the atoms of its crystal matrix at a certain temperature (usually between and , depending on carbon content). This allows the smaller carbon atoms to enter the interstices of the iron crystal. When this diffusion happens, the carbon atoms are said to be in "solution" in the iron, forming a particular single, homogeneous, crystalline phase called austenite. If the steel is cooled slowly, the carbon can diffuse out of the iron and it will gradually revert to its low temperature allotrope. During slow cooling, the carbon atoms will no longer be as soluble with the iron, and will be forced to precipitate out of solution, nucleating into a more concentrated form of iron carbide (Fe3C) in the spaces between the pure iron crystals. The steel then becomes heterogeneous, as it is formed of two phases, the iron-carbon phase called cementite (or carbide), and pure iron ferrite. Such a heat treatment produces a steel that is rather soft. If the steel is cooled quickly, however, the carbon atoms will not have time to diffuse and precipitate out as carbide, but will be trapped within the iron crystals. When rapidly cooled, a diffusionless (martensite) transformation occurs, in which the carbon atoms become trapped in solution. This causes the iron crystals to deform as the crystal structure tries to change to its low temperature state, leaving those crystals very hard but much less ductile (more brittle).
While the high strength of steel results when diffusion and precipitation is prevented (forming martensite), most heat-treatable alloys are precipitation hardening alloys, that depend on the diffusion of alloying elements to achieve their strength. When heated to form a solution and then cooled quickly, these alloys become much softer than normal, during the diffusionless transformation, but then harden as they age. The solutes in these alloys will precipitate over time, forming intermetallic phases, which are difficult to discern from the base metal. Unlike steel, in which the solid solution separates into different crystal phases (carbide and ferrite), precipitation hardening alloys form different phases within the same crystal. These intermetallic alloys appear homogeneous in crystal structure, but tend to behave heterogeneously, becoming hard and somewhat brittle.
## Substitutional and interstitial alloys.
When a molten metal is mixed with another substance, there are two mechanisms that can cause an alloy to form, called "atom exchange" and the "interstitial mechanism". The relative size of each element in the mix plays a primary role in determining which mechanism will occur. When the atoms are relatively similar in size, the atom exchange method usually happens, where some of the atoms composing the metallic crystals are substituted with atoms of the other constituent. This is called a "substitutional alloy". Examples of substitutional alloys include bronze and brass, in which some of the copper atoms are substituted with either tin or zinc atoms respectively. 
In the case of the interstitial mechanism, one atom is usually much smaller than the other and can not successfully substitute for the other type of atom in the crystals of the base metal. Instead, the smaller atoms become trapped in the spaces between the atoms of the crystal matrix, called the "interstices". This is referred to as an "interstitial alloy". Steel is an example of an interstitial alloy, because the very small carbon atoms fit into interstices of the iron matrix. 
Stainless steel is an example of a combination of interstitial and substitutional alloys, because the carbon atoms fit into the interstices, but some of the iron atoms are substituted by nickel and chromium atoms.
## History and examples.
### Meteoric iron.
The use of alloys by humans started with the use of meteoric iron, a naturally occurring alloy of nickel and iron. It is the main constituent of iron meteorites. As no metallurgic processes were used to separate iron from nickel, the alloy was used as it was. Meteoric iron could be forged from a red heat to make objects such as tools, weapons, and nails. In many cultures it was shaped by cold hammering into knives and arrowheads. They were often used as anvils. Meteoric iron was very rare and valuable, and difficult for ancient people to work.
### Bronze and brass.
Iron is usually found as iron ore on Earth, except for one deposit of native iron in Greenland, which was used by the Inuit people. Native copper, however, was found worldwide, along with silver, gold, and platinum, which were also used to make tools, jewelry, and other objects since Neolithic times. Copper was the hardest of these metals, and the most widely distributed. It became one of the most important metals to the ancients. Around 10,000 years ago in the highlands of Anatolia (Turkey), humans learned to smelt metals such as copper and tin from ore. Around 2500 BC, people began alloying the two metals to form bronze, which was much harder than its ingredients. Tin was rare, however, being found mostly in Great Britain. In the Middle East, people began alloying copper with zinc to form brass. Ancient civilizations took into account the mixture and the various properties it produced, such as hardness, toughness and melting point, under various conditions of temperature and work hardening, developing much of the information contained in modern alloy phase diagrams. For example, arrowheads from the Chinese Qin dynasty (around 200 BC) were often constructed with a hard bronze-head, but a softer bronze-tang, combining the alloys to prevent both dulling and breaking during use.
### Amalgams.
Mercury has been smelted from cinnabar for thousands of years. Mercury dissolves many metals, such as gold, silver, and tin, to form amalgams (an alloy in a soft paste or liquid form at ambient temperature). Amalgams have been used since 200 BC in China for gilding objects such as armor and mirrors with precious metals. The ancient Romans often used mercury-tin amalgams for gilding their armor. The amalgam was applied as a paste and then heated until the mercury vaporized, leaving the gold, silver, or tin behind. Mercury was often used in mining, to extract precious metals like gold and silver from their ores.
### Precious-metal alloys.
Many ancient civilizations alloyed metals for purely aesthetic purposes. In ancient Egypt and Mycenae, gold was often alloyed with copper to produce red-gold, or iron to produce a bright burgundy-gold. Gold was often found alloyed with silver or other metals to produce various types of colored gold. These metals were also used to strengthen each other, for more practical purposes. Copper was often added to silver to make sterling silver, increasing its strength for use in dishes, silverware, and other practical items. Quite often, precious metals were alloyed with less valuable substances as a means to deceive buyers. Around 250 BC, Archimedes was commissioned by the King of Syracuse to find a way to check the purity of the gold in a crown, leading to the famous bath-house shouting of "Eureka!" upon the discovery of Archimedes' principle.
### Pewter.
The term pewter covers a variety of alloys consisting primarily of tin. As a pure metal, tin is much too soft to use for most practical purposes. However, during the Bronze Age, tin was a rare metal in many parts of Europe and the Mediterranean, so it was often valued higher than gold. To make jewellery, cutlery, or other objects from tin, workers usually alloyed it with other metals to increase strength and hardness. These metals were typically lead, antimony, bismuth or copper. These solutes were sometimes added individually in varying amounts, or added together, making a wide variety of objects, ranging from practical items such as dishes, surgical tools, candlesticks or funnels, to decorative items like ear rings and hair clips.
The earliest examples of pewter come from ancient Egypt, around 1450 BC. The use of pewter was widespread across Europe, from France to Norway and Britain (where most of the ancient tin was mined) to the Near East. The alloy was also used in China and the Far East, arriving in Japan around 800 AD, where it was used for making objects like ceremonial vessels, tea canisters, or chalices used in shinto shrines.
### Steel and pig iron.
The first known smelting of iron began in Anatolia, around 1800 BC. Called the bloomery process, it produced very soft but ductile wrought iron. By 800 BC, iron-making technology had spread to Europe, arriving in Japan around 700 AD. Pig iron, a very hard but brittle alloy of iron and carbon, was being produced in China as early as 1200 BC, but did not arrive in Europe until the Middle Ages. Pig iron has a lower melting point than iron, and was used for making cast-iron. However, these metals found little practical use until the introduction of crucible steel around 300 BC. These steels were of poor quality, and the introduction of pattern welding, around the 1st century AD, sought to balance the extreme properties of the alloys by laminating them, to create a tougher metal. Around 700 AD, the Japanese began folding bloomery-steel and cast-iron in alternating layers to increase the strength of their swords, using clay fluxes to remove slag and impurities. This method of Japanese swordsmithing produced one of the purest steel-alloys of the ancient world.
While the use of iron started to become more widespread around 1200 BC, mainly because of interruptions in the trade routes for tin, the metal was much softer than bronze. However, very small amounts of steel, (an alloy of iron and around 1% carbon), was always a byproduct of the bloomery process. The ability to modify the hardness of steel by heat treatment had been known since 1100 BC, and the rare material was valued for the manufacture of tools and weapons. Because the ancients could not produce temperatures high enough to melt iron fully, the production of steel in decent quantities did not occur until the introduction of blister steel during the Middle Ages. This method introduced carbon by heating wrought iron in charcoal for long periods of time, but the absorption of carbon in this manner is extremely slow thus the penetration was not very deep, so the alloy was not homogeneous. In 1740, Benjamin Huntsman began melting blister steel in a crucible to even out the carbon content, creating the first process for the mass production of tool steel. Huntsman's process was used for manufacturing tool steel until the early 1900s.
The introduction of the blast furnace to Europe in the Middle Ages meant that people could produce pig iron in much higher volumes than wrought iron. Because pig iron could be melted, people began to develop processes to reduce carbon in liquid pig iron to create steel. Puddling had been used in China since the first century, and was introduced in Europe during the 1700s, where molten pig iron was stirred while exposed to the air, to remove the carbon by oxidation. In 1858, Henry Bessemer developed a process of steel-making by blowing hot air through liquid pig iron to reduce the carbon content. The Bessemer process led to the first large scale manufacture of steel.
#### Alloy steels.
Steel is an alloy of iron and carbon, but the term "alloy steel" usually only refers to steels that contain other elements— like vanadium, molybdenum, or cobalt—in amounts sufficient to alter the properties of the base steel. Since ancient times, when steel was used primarily for tools and weapons, the methods of producing and working the metal were often closely guarded secrets. Even long after the Age of reason, the steel industry was very competitive and manufacturers went through great lengths to keep their processes confidential, resisting any attempts to scientifically analyze the material for fear it would reveal their methods. For example, the people of Sheffield, a center of steel production in England, were known to routinely bar visitors and tourists from entering town to deter industrial espionage. Thus, almost no metallurgical information existed about steel until 1860. Because of this lack of understanding, steel was not generally considered an alloy until the decades between 1930 and 1970 (primarily due to the work of scientists like William Chandler Roberts-Austen, Adolf Martens, and Edgar Bain), so "alloy steel" became the popular term for ternary and quaternary steel-alloys.
After Benjamin Huntsman developed his crucible steel in 1740, he began experimenting with the addition of elements like manganese (in the form of a high-manganese pig-iron called "spiegeleisen"), which helped remove impurities such as phosphorus and oxygen; a process adopted by Bessemer and still used in modern steels (albeit in concentrations low enough to still be considered carbon steel). Afterward, many people began experimenting with various alloys of steel without much success. However, in 1882, Robert Hadfield, being a pioneer in steel metallurgy, took an interest and produced a steel alloy containing around 12% manganese. Called mangalloy, it exhibited extreme hardness and toughness, becoming the first commercially viable alloy-steel. Afterward, he created silicon steel, launching the search for other possible alloys of steel.
Robert Forester Mushet found that by adding tungsten to steel it could produce a very hard edge that would resist losing its hardness at high temperatures. "R. Mushet's special steel" (RMS) became the first high-speed steel. Mushet's steel was quickly replaced by tungsten carbide steel, developed by Taylor and White in 1900, in which they doubled the tungsten content and added small amounts of chromium and vanadium, producing a superior steel for use in lathes and machining tools. In 1903 the Wright brothers used a chromium-nickel steel to make the crankshaft for their airplane engine, while in 1908 Henry Ford began using vanadium steels for parts like crankshafts and valves in his Model T Ford, due to their higher strength and resistance to high temperatures. In 1912, the Krupp Ironworks in Germany developed a rust-resistant steel by adding 21% chromium and 7% nickel, producing the first stainless steel.
### Aluminium and other nonferrous-alloys.
Nonferrous alloys contain no appreciable amounts of iron. The first alloys, bronze and brass, were used for thousands of years, along with lead alloys, pewter and others—but these were all made from metals that were fairly non-reactive and could be smelted over open flames. In the 18th century, Antoine Lavoisier helped to establish the oxygen theory of combustion, displacing the defunct phlogiston theory that had ruled since the late Middle Ages. The oxygen theory helped correctly explain the phenomenon of things like oxidation of metals (i.e., rust) and how rocky ores transform into metals when heated. Lavoisier predicted that many of the earths, salts, and alkalis—for example in alum, a salt used since antiquity—contained metallic bases that were too reactive to oxygen to smelt by the usual methods. His work eventually led to the periodic table of elements, which helped confirm the existence of these "missing metals."
Due to their high reactivity, most metals were not discovered until the 19th century. A method for extracting aluminium from bauxite was proposed by Humphry Davy in 1807, using an electric arc. Although his attempts were unsuccessful, by 1855 the first sales of pure aluminium reached the market. However, as extractive metallurgy was still in its infancy, most aluminium extraction-processes produced unintended alloys contaminated with other elements found in the ore; the most abundant of which was copper. These aluminium-copper alloys (at the time termed "aluminum bronze") preceded pure aluminium, offering greater strength and hardness over the soft, pure metal, and to a slight degree were found to be heat treatable. However, due to their softness and limited hardenability these alloys found little practical use, and were more of a novelty, until the Wright brothers used an aluminium alloy to construct the first airplane engine in 1903. During the time between 1865 and 1910, processes for extracting many other metals were discovered, such as chromium, vanadium, tungsten, iridium, cobalt, and molybdenum, and various alloys were developed.
Prior to 1910, research mainly consisted of private individuals tinkering in their own laboratories. However, as the aircraft and automotive industries began growing, research into alloys became an industrial effort in the years following 1910, as new magnesium alloys were developed for pistons and wheels in cars, and pot metal for levers and knobs, and aluminium alloys developed for airframes and aircraft skins were put into use.
### Precipitation-hardening alloys.
In 1906, precipitation hardening alloys were discovered by Alfred Wilm. Precipitation hardening alloys, such as certain alloys of aluminium, titanium, and copper, are heat-treatable alloys that soften when quenched (cooled quickly), and then harden over time. Wilm had been searching for a way to harden aluminium alloys for use in machine-gun cartridge cases. Knowing that aluminium-copper alloys were heat-treatable to some degree, Wilm tried quenching a ternary alloy of aluminium, copper, and the addition of magnesium, but was initially disappointed with the results. However, when Wilm retested it the next day he discovered that the alloy increased in hardness when left to age at room temperature, and far exceeded his expectations. Although an explanation for the phenomenon was not provided until 1919, duralumin was one of the first "age hardening" alloys used, becoming the primary building material for the first Zeppelins, and was soon followed by many others. Because they often exhibit a combination of high strength and low weight, these alloys became widely used in many forms of industry, including the construction of modern aircraft.

</doc>
<doc id="1189" url="https://en.wikipedia.org/wiki?curid=1189" title="Articles of Faith">
Articles of Faith



</doc>
<doc id="1190" url="https://en.wikipedia.org/wiki?curid=1190" title="Alternative history">
Alternative history



</doc>
<doc id="1192" url="https://en.wikipedia.org/wiki?curid=1192" title="Artistic revolution">
Artistic revolution

Throughout history, forms of art have gone through periodic abrupt changes called artistic revolutions. Movements have come to an end to be replaced by a new movement markedly different in striking ways. See also cultural movements.
## Scientific and technological.
Not all artistic revolutions were political. Sometimes, science and technological innovations have brought about unforeseen transformations in the works of artists. The stylistic revolution known as Impressionism, by painters eager to more accurately capture the changing colors of light and shadow, is inseparable from discoveries and inventions in the mid-19th century in which the style was born.
Eugene Chevreul, a French chemist hired as director of dyes at a French tapestry works, began to investigate the optical nature of color in order to improve color in fabrics. Chevreul realized It was the eye, and not the dye, that had the greatest influence on color, and from this, he revolutionized color theory by grasping what came to be called the law of simultaneous contrast: that colors mutually influence one another when juxtaposed, each imposing its own complementary color on the other. The French painter Eugène Delacroix, who had been experimenting with what he called broken tones, embraced Chevreul's book, "The Law of Contrast of Color (1839) with its explanations of how juxtaposed colors can enhance or diminish each other, and his exploration of all the visible colors of the spectrum. Inspired by Chevreul’s 1839 treatise, Delacroix passed his enthusiasm on to the young artists who were inspired by him. It was Chevreul who led the Impressionists to grasp that they should apply separate brushstrokes of pure color to a canvas and allow the viewer’s eye to combine them optically.
They were aided greatly in this by innovations in oil paint itself. Since the Renaissance, painters had to grind pigment, add oil and thus create their own paints; these time-consuming paints also quickly dried out, making studio painting a necessity for large works, and limiting painters to mix one or two colors at a time and fill in an entire area using just that one color before it dried out. in 1841, a little-known American painter named John G. Rand invented a simple improvement without which the Impressionist movement could not have occurred: the small, flexible tin tube with removable cap in which oil paints could be stored. Oil paints kept in such tubes stayed moist and usable -- and quite portable. For the first time since the Renaissance, painters were not trapped by the time frame of how quickly oil paint dried.
Paints in tubes could be easily loaded up and carried out into the real world, to directly observe the play of color and natural light, in shadow and movement, to paint in the moment. Selling the oil paint in tubes also brought about the arrival of dazzling new pigments - chrome yellow, cadmium blue - invented by 19th century industrial chemists. The tubes freed the Impressionists to paint quickly, and across an entire canvas, rather than carefully delineated single-color sections at a time; in short, to sketch directly in oil - racing across the canvas in every color that came to hand and thus inspiring their name of "impressionists" - since such speedy, bold brushwork and dabs of separate colors made contemporary critics think their paintings were mere impressions, not finished paintings, which were to have no visible brush marks at all, seamless under layers of varnish.
Pierre-Auguste Renoir said, “Without colors in tubes, there would be no Cézanne, no Monet, no Pissarro, and no Impressionism.”
Finally, the careful, hyper-realistic techniques of French neo-classicism were seen as stiff and lifeless when compared to the remarkable new vision of the world as seen through the new invention of photography by the mid-1850s. It was not merely that the increasing ability of this new invention, particularly by the French inventor Daguerre, made the realism of the painted image redundant as he deliberately competed in the Paris diorama with large-scale historical paintings. The neo-classical subject matter, limited by Academic tradition to Greek and Roman legends, historical battles and Biblical stories, seemed oppressively clichéd and limited to artists eager to explore the actual world in front of their own eyes revealed by the camera - daily life, candid groupings of everyday people doing simple things, Paris itself, rural landscapes and most particularly the play of captured light - not the imaginary lionizing of unseen past events. Early photographs influenced Impressionist style by its use of asymmetry, cropping and most obviously the blurring of motion, as inadvertently captured in the very slow speeds of early photography.
Edgar Degas, Claude Monet, Pierre-Auguste Renoir - in their framing, use of color, light and shadow, subject matter - put these innovations to work to create a new language of visual beauty and meaning.
## Faking revolution: the C.I.A. and Abstract Expressionism.
Their initial break with realism into an exploration of light, color and the nature of paint was brought to an ultimate conclusion by the Abstract Expressionists who broke away from recognizable content of any kind into works of pure shape, color and painterliness which emerged at the end of the second world war. At first thought of as primitive, inept works - as in "my four year old could do that"—these works were misunderstood and neglected until given critical and support by the rise of art journalists and critics who championed their work in the 1940s and 50's, expressing the power of such work in aesthetic terms the artists themselves seldom used, or even understood. Jackson Pollock who pioneered splatter painting, dispensing with a paint brush altogether, soon became lionized as the angry young man in a large spread in Life Magazine.
In fact, in a deliberate, secret and successful effort to separate artistic revolutions from political ones, abstract expressionists like Pollack, Robert Motherwell, Willem de Kooning and Mark Rothko, while seemingly difficult, pathbreaking artists, were in fact secretly supported for twenty years by the C.I.A. in a Cold War policy begun in 1947 to prove that the United States could foster more artistic freedom than the Soviet bloc. "It was recognized that Abstract Expressionism was the kind of art that made Socialist Realism look even more stylized and rigid and confined than it was, " said former C.I.A. case worker Donald Jameson, who finally broke the silence on this program in 1995. Ironically, the covert C.I.A. support for these radical works was required because an attempt to use government funds for a European tour of these works during the Truman administration led to a public uproar in conservative McCarthy-era America, with Truman famously remarking, "If that's art, I'm a Hottentot." Thus the program was hidden under the guise of fabricated foundations and the support of wealthy patrons who were actually using C.I.A. funds, not their own, to sponsor traveling exhibitions of American abstract expressionists all over the world, publish books and articles praising them and to purchase and exhibit Abstract Expressionist works in major American and British museums. Thomas Braden, in charge of these cultural programs for the C.I.A.. in the early years of the Cold War, had formerly been executive secretary of the Museum of Modern Art, America's leading institution for 20th Century art and the charges of collusion between the two echoed for many years after this program was revealed, though most of the artists involved had no idea they were being used in this way and were furious when they found out.

</doc>
<doc id="1193" url="https://en.wikipedia.org/wiki?curid=1193" title="Agrarianism">
Agrarianism

Agrarianism is a political and social philosophy that has promoted subsistence agriculture, smallholdings, egalitarianism, with agrarian political parties normally supporting the rights and sustainability of small farmers and poor peasants against the wealthy in society. In highly developed and industrial nations or regions it can denote use of financial and social incentives for self-sustainability, more community involvement in food production (such as allotment gardens) and smart growth that avoids urban sprawl and, many of its advocates contend, risks of human overpopulation; when overpopulation occurs the available resources become too limited for the entire population to survive comfortably or at all in the long term.
## Philosophy.
Some scholars suggest that agrarianism values rural society as superior to urban society and the independent farmer as superior to the paid worker, and sees farming as a way of life that can shape the ideal social values. It stresses the superiority of a simpler rural life as opposed to the complexity of city life. For example, M. Thomas Inge defines agrarianism by the following basic tenets:
## History.
The philosophical roots of agrarianism include European and Chinese philosophers. The Chinese school of Agriculturalism (农家/農家) was a philosophy that advocated peasant utopian communalism and egalitarianism. In societies influenced by Confucianism, the farmer was considered an esteemed productive member of society, but merchants who made money were looked down upon. That influenced European intellectuals like François Quesnay, an avid Confucianist and advocate of China's agrarian policies, in forming the French agrarian philosophy of physiocracy. The physiocrats, along with the ideas of John Locke and the Romantic Era, formed the basis of modern European and American agrarianism.
## Types of agrarianism.
### Jeffersonian democracy.
The United States president Thomas Jefferson was an agrarian who based his ideas about the budding American democracy around the notion that farmers are “the most valuable citizens” and the truest republicans. Jefferson and his support base were committed to American republicanism, which they saw as being in opposition to aristocracy and corruption, and which prioritized virtue, exemplified by the "yeoman farmer", "planters", and the "plain folk". In praising the rural farmfolk, the Jeffersonians felt that financiers, bankers and industrialists created "cesspools of corruption" in the cities and should thus be avoided.
The Jeffersonians sought to align the American economy more with agriculture than industry. Part of their motive to do so was Jefferson's fear that the over-industrialization of America would create a class of wage laborers who relied on their employers for income and sustenance. In turn, these workers would cease to be independent voters as their vote could be manipulated by said employers. To counter this, Jefferson introduced, as scholar Clay Jenkinson noted, "a graduated income tax that would serve as a disincentive to vast accumulations of wealth and would make funds available for some sort of benign redistribution downward" and tariffs on imported articles, which were mainly purchased by the wealthy. In 1811 Jefferson, writing to a friend, explained: "these revenues will be levied entirely on the rich... . the rich alone use imported articles, and on these alone the whole taxes of the general government are levied. the poor man ... pays not a farthing of tax to the general government, but on his salt."
There is general agreement that the substantial United States' federal policy of offering land grants (such as thousands of gifts of land to veterans) had a positive impact on economic development in the 19th century. 
### Agrarian socialism.
Agrarian socialism is a form of agrarianism that is anti-capitalist in nature and seeks to introduce socialist economic systems in their stead.
#### Zapatismo.
Notable agrarian socialists include Emiliano Zapata who was a leading figure in the Mexican Revolution. As part of the Liberation Army of the South, his group of revolutionaries fought on behalf of the Mexican peasants, whom they saw as exploited by the landowning classes. Zapata published Plan of Ayala, which called for significant land reforms and land redistribution in Mexico as part of the revolution. Zapata was killed and his forces crushed over the course of the Revolution, but his political ideas lived on in the form of Zapatismo.
Zapatismo would form the basis for neozapatismo, the ideology of the Zapatista Army of National Liberation. Known as "Ejército Zapatista de Liberación Nacional" or EZLN in Spanish, EZLN is a far-left libertarian socialist political and militant group that emerged in the state of Chiapas in southmost Mexico in 1994. EZLN and Neozapatismo, as explicit in their name, seek to revive the agrarian socialist movement of Zapata, but fuse it with new elements such as a commitment to indigenous rights and community-level decision making.
Subcommander Marcos, a leading member of the movement, argues that the peoples' collective ownership of the land was and is the basis for all subsequent developments the movement sought to create:
…When the land became property of the peasants … when the land passed into the hands of those who work it … [This was] the starting point for advances in government, health, education, housing, nutrition, women’s participation, trade, culture, communication, and information …[it was] recovering the means of production, in this case, the land, animals, and machines that were in the hands of large property owners.”
#### Maoism.
Maoism, the far-left ideology of Mao Zedong and his followers, places a heavy emphasis on the role of peasants in its goals. In contrast to other Marxist schools of thought which normally seek to acquire the support of urban workers, Maoism sees the peasantry as key. Believing that "political power grows out of the barrel of a gun", Maoism saw the Chinese Peasantry as the prime source for a Marxist vanguard because it possessed two qualities: (i) they were poor, and (ii) they were a political blank slate; in Mao's words, “A clean sheet of paper has no blotches, and so the newest and most beautiful words can be written on it”. During the Chinese Civil War and the Second Sino-Japanese War, Mao and the Chinese Communist Party made extensive use of peasants and rural bases in their military tactics, often eschewing the cities.
Following the eventual victory of the Communist Party in both wars, the countryside and how it should be run remained a focus for Mao. In 1958 Mao launched the Great Leap Forward, a social and economic campaign which, amongst other things, altered many aspects of rural Chinese life. It introduced mandatory collective farming and forced the peasantry to organize itself into communal living units which were known as people's communes. These communes, which consisted of 5,000 people on average, were expected to meet high production quotas while the peasants who lived on them adapted to this radically new way of life. The communes were run as co-operatives where wages and money were replaced by work points. Peasants who criticised this new system were persecuted as "rightists" and "counter-revolutionaries". Leaving the communes was forbidden and escaping from them was difficult or impossible, and those who attempted it were subjected to party-orchestrated "public struggle sessions," which further jeopardized their survival. These public criticism sessions were often used to intimidate the peasants into obeying local officials and they often devolved into little more than public beatings.
On the communes, experiments were conducted in order to find new methods of planting crops, efforts were made to construct new irrigation systems on a massive scale, and the communes were all encouraged to produce steel backyard furnaces as part of an effort to increase steel production. However, following the Anti-Rightist Campaign, Mao had instilled a mass distrust of intellectuals into China, and thus engineers often were not consulted with regard to the new irrigation systems and the wisdom of asking untrained peasants to produce good quality steel from scrap iron was not publicly questioned. Similarly, the experimentation with the crops did not produce results. In addition to this the Four Pests Campaign was launched, in which the peasants were called upon to destroy sparrows and other wild birds that ate crop seeds, in order to protect fields. Pest birds were shot down or scared away from landing until they dropped from exhaustion. This campaign resulted in an ecological disaster that saw an explosion of the vermin population, especially crop-eating insects, which was consequently not in danger of being killed by predators.
None of these new systems were working, but local leaders did not dare to state this, instead, they falsified reports so as not to be punished for failing to meet the quotas. In many cases they stated that they were greatly exceeding their quotas, and in turn, the Chinese state developed a completely false sense of success with regard to the commune system.
All of this culminated in the Great Chinese Famine, which began in 1959, lasted 3 years, and saw an estimated 15 to 30 million Chinese people die. A combination of bad weather and the new, failed farming techniques that were introduced by the state led to massive shortages of food. By 1962, the Great Leap Forward was declared to be at an end.
In the late 1960s and early 1970s, Mao once again radically altered life in rural China with the launching of the Down to the Countryside Movement. As a response to the Great Chinese Famine, the Chinese President Liu Shaoqi began "sending down" urban youths to rural China in order to recover its population losses and alleviate overcrowding in the cities. However, Mao turned the practice into a political crusade, declaring that the sending down would strip the youth of any bourgeois tendencies by forcing them to learn from the unprivileged rural peasants. In reality, it was the Communist Party's attempt to reign in the Red Guards, who had become uncontrollable during the course of the Cultural Revolution. 10% of the 1970 urban population of China was sent out to remote rural villages, often in Inner Mongolia. The villages, which were still poorly recovering from the effects of the Great Chinese Famine, did not have the excess resources that were needed to support the newcomers. Furthermore, the so-called "sent-down youth" had no agricultural experience and as a result, they were unaccustomed to the harsh lifestyle that existed in the countryside, and their unskilled labor in the villages provided little benefit to the agricultural sector. As a result, many of the sent-down youth died in the countryside. The relocation of the youths was originally intended to be permanent, but by the end of the Cultural Revolution, the Communist Party relented and some of those who had the capacity to return to the cities were allowed to do so.
In imitation of Mao's policies, the Khmer Rouge of Cambodia (who were heavily funded and supported by the People's Republic of China) created their own version of the Great Leap Forward which was known as "Maha Lout Ploh". With the Great Leap Forward as its model, it had similarly disastrous effects, contributing to what is now known as the Cambodian genocide. As a part of the Maha Lout Ploh, the Khmer Rouge sought to create an entirely agrarian socialist society by forcibly relocating 100,000 people to move from Cambodia's cities into newly created communes. The Khmer Rouge leader, Pol Pot sought to "purify" the country by setting it back to "Year Zero", freeing it from "corrupting influences". Besides trying to completely de-urbanize Cambodia, ethnic minorities were slaughtered along with anyone else who was suspected of being a "reactionary" or a member of the "bourgeoisie", to the point that wearing glasses was seen as grounds for execution. The killings were only brought to an end when Cambodia was invaded by the neighboring socialist nation of Vietnam, whose army toppled the Khmer Rouge. However, with Cambodia's entire society and economy in disarray, including its agricultural sector, the country still plunged into renewed famine due to vast food shortages. However, as international journalists began to report on the situation and send images of it out to the world, a massive international response was provoked, leading to one of the most concentrated relief efforts of its time.
## Agrarian parties.
Peasant parties first appeared across Eastern Europe between 1860 and 1910, when commercialized agriculture and world market forces disrupted traditional rural society, and the railway and growing literacy facilitated the work of roving organizers. Agrarian parties advocated land reforms to redistribute land on large estates among those who work it. They also wanted village cooperatives to keep the profit from crop sales in local hands and credit institutions to underwrite needed improvements. Many peasant parties were also nationalist parties because peasants often worked their land for the benefit of landlords of different ethnicity.
Peasant parties rarely had any power before World War I but some became influential in the interwar era, especially in Bulgaria and Czechoslovakia. For a while, in the 1920s and the 1930s, there was a Green International (International Agrarian Bureau) based on the peasant parties in Bulgaria, Czechoslovakia, Poland, and Serbia. It functioned primarily as an information center that spread the ideas of agrarianism and combating socialism on the left and landlords on the right and never launched any significant activities.
### Tunisia.
The Farmers' Voice Party won a seat in the district of Jendouba after the parliamentary election of 2014.
### Europe.
#### Bulgaria.
In Bulgaria, the Bulgarian Agrarian National Union (BZNS) was organized in 1899 to resist taxes and build cooperatives. BZNS came to power in 1919 and introduced many economic, social, and legal reforms. However, conservative forces crushed BZNS in a 1923 coup and assassinated its leader, Aleksandar Stamboliyski (1879–1923). BZNS was made into a communist puppet group until 1989, when it reorganized as a genuine party.
#### Czechoslovakia.
In Czechoslovakia, the Republican Party of Agricultural and Smallholder People often shared power in parliament as a partner in the five-party pětka coalition. The party's leader, Antonin Svehla (1873–1933), was prime minister several times. It was consistently the strongest party, forming and dominating coalitions. It moved beyond its original agrarian base to reach middle-class voters. The party was banned by the National Front after the Second World War.
#### France.
In France, the Hunting, Fishing, Nature, Tradition party is a moderate conservative, agrarian party, reaching a peak of 4.23% in the 2002 French presidential election. It would later on become affiliated to France's main conservative party, Union for a Popular Movement. More recently, the Resistons! movement of Jean Lassalle espoused agrarianism.
#### Hungary.
In Hungary, the first major agrarian party, the small-holders party was founded in 1908. The party became part of the government in the 1920s but lost influence in the government. A new party, the Independent Smallholders, Agrarian Workers and Civic Party was established in 1930 with a more radical program representing larger scale land redistribution initiatives. They implemented this program together with the other coalition parties after WWII. However, after 1949 the party was outlawed when a one-party system was introduced. They became part of the government again 1990-1994, and 1998-2002 after which they lost political support. The ruling Fidesz party has an agrarian faction, and promotes agrarian interest since 2010 with the emphasis now placed on supporting larger family farms versus small-holders.
#### Ireland.
In the late 19th century, the Irish National Land League aimed to abolish landlordism in Ireland and enable tenant farmers to own the land they worked on. The "Land War" of 1878–1909 led to the Irish Land Acts, ending absentee landlords and ground rent and redistributing land among peasant farmers.
Post-independence, the Farmers' Party operated in the Irish Free State from 1922, folding into the National Centre Party in 1932. It was mostly supported by wealthy farmers in the east of Ireland.
Clann na Talmhan (Family of the Land; also called the "National Agricultural Party") was founded in 1938. They focused more on the poor smallholders of the west, supporting land reclamation, afforestation, social democracy and rates reform. They formed part of the governing coalition of the Government of the 13th Dáil and Government of the 15th Dáil. Economic improvement in the 1960s saw farmers vote for other parties and Clann na Talmhan disbanded in 1965.
#### Latvia.
In Latvia, the Union of Greens and Farmers is supportive of traditional small farms and perceives them as more environmentally friendly than large-scale farming: Nature is threatened by development, while small farms are threatened by large industrial-scale farms.
#### Lithuania.
In Lithuania, as of 2017, the government is led by the Lithuanian Farmers and Greens Union, under the leadership of industrial farmer Ramūnas Karbauskis.
#### Poland.
In Poland, the Polish People's Party traces its tradition to an agrarian party in Austro-Hungarian-controlled Galician Poland. After the fall of the communist regime, PPP's biggest success came in 1993 elections, where it won 132 out of 460 parliamentary seats. Since then, PPP's support has steadily declined, until 2019, when they formed Polish Coalition with an anti- establishment, direct democracy Kukiz'15 party, and managed to get 8.5% of votes. Moreover, PPP tends to get much better results in local elections. In 2014 elections they have managed to get 23.88% of votes.
The right-wing Law and Justice party has also become supportive of agrarian policies in recent years and polls show that most of their support comes from rural areas. AGROunia resembles the features of agrarianism.
#### Romania.
In Romania, older parties from Transylvania, Moldavia, and Wallachia merged to become the National Peasants' Party in 1926. Iuliu Maniu (1873–1953) was a prime minister with an agrarian cabinet from 1928 to 1930 and briefly in 1932–1933, but the Great Depression made proposed reforms impossible. The communist regime dissolved the party in 1947, but it reformed in 1989 after they fell from power.
The reformed party, which also incorporated elements of Christian democracy in its ideology, governed Romania as part of the Romanian Democratic Convention between 1996 and 2000.
#### Serbia.
In Serbia, Nikola Pašić (1845–1926) and his People's Radical Party dominated Serbian politics after 1903. The party also monopolized power in Yugoslavia from 1918 to 1929. During the dictatorship of the 1930s, the prime minister was from that party.
#### Ukraine.
In Ukraine, the Radical Party of Oleh Lyashko has promised to purify the country of oligarchs "with a pitchfork". The party advocates a number of traditional left-wing positions (a progressive tax structure, a ban on agricultural land sale and eliminating the illegal land market, a tenfold increase in budget spending on health, setting up primary health centres in every village
), and mixes them with strong nationalist sentiments.
#### United Kingdom.
In land law the heyday of English, Irish (and thus Welsh) agrarianism was to 1603, led by the Tudor royal advisors, who sought to maintain a broad pool of agricultural commoners from which to draw military men, against the interests of larger landowners who sought enclosure (meaning complete private control of common land, over which by custom and common law lords of the manor always enjoyed minor rights). The heyday was eroded by hundreds of Acts of Parliament to expressly permit enclosure, chiefly from 1650 to the 1810s. Politicians standing strongly as reactionaries to this included the Levellers, those anti-industrialists (Luddites) going beyond opposing new weaving technology and, later, radicals such as William Cobbett. 
A high level of net national or local self-sufficiency has a strong base in campaigns and movements. In the 19th century such empowered advocates included Peelites and most Conservatives. The 20th century saw the growth or start of influential non-governmental organisations, such as the National Farmers' Union of England and Wales, Campaign for Rural England, Friends of the Earth (EWNI) and of the England Wales, Scottish and Northern Irish political parties prefixed by and focussed on Green politics. The 21st century has seen decarbonisation already in electricity markets. Following protests and charitable lobbying local food has seen growing market share, sometimes backed by wording in public policy papers and manifestos. The UK has many sustainability-prioritising businesses, green charity campaigns, events and lobby groups ranging from espousing allotment gardens (hobby community farming) through to a clear policy of local food and/or self-sustainability models.
### Oceania.
#### Australia.
Historian F.K. Crowley finds that:
The National Party of Australia (formerly called the Country Party), from the 1920s to the 1970s, promulgated its version of agrarianism, which it called "countrymindedness". The goal was to enhance the status of the graziers (operators of big sheep stations) and small farmers and justified subsidies for them.
#### New Zealand.
The New Zealand Liberal Party aggressively promoted agrarianism in its heyday (1891–1912). The landed gentry and aristocracy ruled Britain at this time. New Zealand never had an aristocracy but its wealthy landowners largely controlled politics before 1891. The Liberal Party set out to change that by a policy it called "populism." Richard Seddon had proclaimed the goal as early as 1884: "It is the rich and the poor; it is the wealthy and the landowners against the middle and labouring classes. That, Sir, shows the real political position of New Zealand." The Liberal strategy was to create a large class of small landowning farmers who supported Liberal ideals. The Liberal government also established the basis of the later welfare state such as old age pensions and developed a system for settling industrial disputes, which was accepted by both employers and trade unions. In 1893, it extended voting rights to women, making New Zealand the first country in the world to do so.
To obtain land for farmers, the Liberal government from 1891 to 1911 purchased of Maori land. The government also purchased from large estate holders for subdivision and closer settlement by small farmers. The Advances to Settlers Act (1894) provided low-interest mortgages, and the agriculture department disseminated information on the best farming methods. The Liberals proclaimed success in forging an egalitarian, anti-monopoly land policy. The policy built up support for the Liberal Party in rural North Island electorates. By 1903, the Liberals were so dominant that there was no longer an organized opposition in Parliament.
## Back-to-the-land movement.
Agrarianism is similar to but not identical with the back-to-the-land movement. Agrarianism concentrates on the fundamental goods of the earth, on communities of more limited economic and political scale than in modern society, and on simple living, even when the shift involves questioning the "progressive" character of some recent social and economic developments. Thus, agrarianism is not industrial farming, with its specialization on products and industrial scale.

</doc>
<doc id="1194" url="https://en.wikipedia.org/wiki?curid=1194" title="Atomic">
Atomic

Atomic may refer to:

</doc>
<doc id="1195" url="https://en.wikipedia.org/wiki?curid=1195" title="Allotropes">
Allotropes



</doc>
<doc id="1196" url="https://en.wikipedia.org/wiki?curid=1196" title="Angle">
Angle

In Euclidean geometry, an angle is the figure formed by two rays, called the "sides" of the angle, sharing a common endpoint, called the "vertex" of the angle.
Angles formed by two rays lie in the plane that contains the rays. Angles are also formed by the intersection of two planes. These are called dihedral angles. Two intersecting curves may also define an angle, which is the angle of the rays lying tangent to the respective curves at their point of intersection. 
"Angle" is also used to designate the measure of an angle or of a rotation. This measure is the ratio of the length of a circular arc to its radius. In the case of a geometric angle, the arc is centered at the vertex and delimited by the sides. In the case of a rotation, the arc is centered at the center of the rotation and delimited by any other point and its image by the rotation.
## History and etymology.
The word "angle" comes from the Latin word "angulus", meaning "corner"; cognate words are the Greek "(ankylοs)", meaning "crooked, curved," and the English word "ankle". Both are connected with the Proto-Indo-European root "*ank-", meaning "to bend" or "bow".
Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other. According to Proclus, an angle must be either a quality or a quantity, or a relationship. The first concept was used by Eudemus, who regarded an angle as a deviation from a straight line; the second by Carpus of Antioch, who regarded it as the interval or space between the intersecting lines; Euclid adopted the third concept.
## Identifying angles.
In mathematical expressions, it is common to use Greek letters (α, β, γ, θ, φ, . . . ) as variables denoting the size of some angle (to avoid confusion with its other meaning, the symbol is typically not used for this purpose). Lower case Roman letters ("a", "b", "c", . . . ) are also used, as are upper case Roman letters in the context of polygons. See the figures in this article for examples.
In geometric figures, angles may also be identified by the labels attached to the three points that define them. For example, the angle at vertex A enclosed by the rays AB and AC (i.e. the lines from point A to point B and point A to point C) is denoted ∠BAC (in Unicode ) or formula_1. Where there is no risk of confusion, the angle may sometimes be referred to simply by its vertex (in this case "angle A").
Potentially, an angle denoted as, say, ∠BAC, might refer to any of four angles: the clockwise angle from B to C, the anticlockwise angle from B to C, the clockwise angle from C to B, or the anticlockwise angle from C to B, where the direction in which the angle is measured determines its sign (see Positive and negative angles). However, in many geometrical situations, it is obvious from context that the positive angle less than or equal to 180 degrees is meant, in which case no ambiguity arises. Otherwise, a convention may be adopted so that ∠BAC always refers to the anticlockwise (positive) angle from B to C, and ∠CAB the anticlockwise (positive) angle from C to B.
## Types of angles.
### Individual angles.
There is some common terminology for angles, whose measure is always non-negative (see ""):
The names, intervals, and measuring units are shown in the table below:
### Vertical and adjacent angle pairs.
When two straight lines intersect at a point, four angles are formed. Pairwise these angles are named according to their location relative to each other.
A transversal is a line that intersects a pair of (often parallel) lines, and is associated with "alternate interior angles", "corresponding angles", "interior angles", and "exterior angles".
### Combining angle pairs.
Three special angle pairs involve the summation of angles:
## Measuring angles.
The size of a geometric angle is usually characterized by the magnitude of the smallest rotation that maps one of the rays into the other. Angles that have the same size are said to be "equal" or "congruent" or "equal in measure".
In some contexts, such as identifying a point on a circle or describing the "orientation" of an object in two dimensions relative to a reference orientation, angles that differ by an exact multiple of a full turn are effectively equivalent. In other contexts, such as identifying a point on a spiral curve or describing the "cumulative rotation" of an object in two dimensions relative to a reference orientation, angles that differ by a non-zero multiple of a full turn are not equivalent.
In order to measure an angle θ, a circular arc centered at the vertex of the angle is drawn, e.g. with a pair of compasses. The ratio of the length s of the arc by the radius r of the circle is the number of radians in the angle. Conventionally, in mathematics and in the SI, the radian is treated as being equal to the dimensionless value 1.
The angle expressed another angular unit may then be obtained by multiplying the angle by a suitable conversion constant of the form , where "k" is the measure of a complete turn expressed in the chosen unit (for example, for degrees or 400 grad for gradians):
The value of thus defined is independent of the size of the circle: if the length of the radius is changed then the arc length changes in the same proportion, so the ratio "s"/"r" is unaltered.
In particular, the measure of angle is radian can be also interpreted as the arc length of its corresponding unit circle:
### Angle addition postulate.
The angle addition postulate states that if B is in the interior of angle AOC, then
The measure of the angle AOC is the sum of the measure of angle AOB and the measure of angle BOC.
### Units.
Throughout history, angles have been measured in many different units. These are known as angular units, with the most contemporary units being the degree ( ° ), the radian (rad), and the gradian (grad), though many others have been used throughout history.
Angles expressed in radians are dimensionless for dimensional analysis.
Most units of angular measurement are defined such that one turn (i.e. one full circle) is equal to "n" units, for some whole number "n". The two exceptions are the radian (and its decimal submultiples) and the diameter part.
One "radian" is the angle subtended by an arc of a circle that has the same length as the circle's radius. The radian is the derived quantity of angular measurement in the SI system. By definition, it is dimensionless, though it may be specified as "rad" to avoid ambiguity. Angles measured in degrees, are shown with the symbol °. Subdivisions of the degree are minute (symbol ′, 1′ = 1/60°) and second {symbol ″, 1″ = 1/3600°}. An angle of 360° corresponds to the angle subtended by a full circle, and is equal to radians, or 400 gradians.
Other units used to represent angles are listed in the following table. These units are defined such that the number of turns is equivalent to a full circle.
### Signed angles.
Although the definition of the measurement of an angle does not support the concept of a negative angle, it is frequently useful to impose a convention that allows positive and negative angular values to represent orientations and/or rotations in opposite directions relative to some reference.
In a two-dimensional Cartesian coordinate system, an angle is typically defined by its two sides, with its vertex at the origin. The "initial side" is on the positive x-axis, while the other side or "terminal side" is defined by the measure from the initial side in radians, degrees, or turns. With "positive angles" representing rotations toward the positive y-axis and "negative angles" representing rotations toward the negative "y"-axis. When Cartesian coordinates are represented by "standard position", defined by the "x"-axis rightward and the "y"-axis upward, positive rotations are anticlockwise and negative rotations are clockwise.
In many contexts, an angle of −"θ" is effectively equivalent to an angle of "one full turn minus "θ"". For example, an orientation represented as −45° is effectively equivalent to an orientation represented as 360° − 45° or 315°. Although the final position is the same, a physical rotation (movement) of −45° is not the same as a rotation of 315° (for example, the rotation of a person holding a broom resting on a dusty floor would leave visually different traces of swept regions on the floor).
In three-dimensional geometry, "clockwise" and "anticlockwise" have no absolute meaning, so the direction of positive and negative angles must be defined relative to some reference, which is typically a vector passing through the angle's vertex and perpendicular to the plane in which the rays of the angle lie.
In navigation, bearings or azimuth are measured relative to north. By convention, viewed from above, bearing angles are positive clockwise, so a bearing of 45° corresponds to a north-east orientation. Negative bearings are not used in navigation, so a north-west orientation corresponds to a bearing of 315°.
### Alternative ways of measuring the size of an angle.
There are several alternatives to measuring the size of an angle by the angle of rotation.
The "slope" or "gradient" is equal to the tangent of the angle, or sometimes (rarely) the sine; a gradient is often expressed as a percentage. For very small values (less than 5%), the grade of a slope is approximately the measure of the angle in radians.
In rational geometry the "spread" between two lines is defined as the square of the sine of the angle between the lines. As the sine of an angle and the sine of its supplementary angle are the same, any angle of rotation that maps one of the lines into the other leads to the same value for the spread between the lines.
### Astronomical approximations.
Astronomers measure angular separation of objects in degrees from their point of observation.
These measurements clearly depend on the individual subject, and the above should be treated as rough rule of thumb approximations only.
In astronomy, right ascension and declination are usually measured in angular units, expressed in terms of time, based on a 24-hour day.
### Measurements that are not angular units.
Not all angle measurements are angular units, for an angular measurement, it is definitional that the angle addition postulate holds.
Some angle measurements where the angle addition postulate does not hold include:
## Angles between curves.
The angle between a line and a curve (mixed angle) or between two intersecting curves (curvilinear angle) is defined to be the angle between the tangents at the point of intersection. Various names (now rarely, if ever, used) have been given to particular cases:—"amphicyrtic" (Gr. , on both sides, κυρτός, convex) or "cissoidal" (Gr. κισσός, ivy), biconvex; "xystroidal" or "sistroidal" (Gr. ξυστρίς, a tool for scraping), concavo-convex; "amphicoelic" (Gr. κοίλη, a hollow) or "angulus lunularis", biconcave.
## Bisecting and trisecting angles.
The ancient Greek mathematicians knew how to bisect an angle (divide it into two angles of equal measure) using only a compass and straightedge, but could only trisect certain angles. In 1837 Pierre Wantzel showed that for most angles this construction cannot be performed.
## Dot product and generalisations.
In the Euclidean space, the angle "θ" between two Euclidean vectors u and v is related to their dot product and their lengths by the formula
This formula supplies an easy method to find the angle between two planes (or curved surfaces) from their normal vectors and between skew lines from their vector equations.
### Inner product.
To define angles in an abstract real inner product space, we replace the Euclidean dot product ( · ) by the inner product formula_6, i.e.
In a complex inner product space, the expression for the cosine above may give non-real values, so it is replaced with
or, more commonly, using the absolute value, with
The latter definition ignores the direction of the vectors and thus describes the angle between one-dimensional subspaces formula_10 and formula_11 spanned by the vectors formula_12 and formula_13 correspondingly.
### Angles between subspaces.
The definition of the angle between one-dimensional subspaces formula_10 and formula_11 given by
in a Hilbert space can be extended to subspaces of any finite dimensions. Given two subspaces formula_17, formula_18 with formula_19, this leads to a definition of formula_20 angles called canonical or principal angles between subspaces.
### Angles in Riemannian geometry.
In Riemannian geometry, the metric tensor is used to define the angle between two tangents. Where "U" and "V" are tangent vectors and "g""ij" are the components of the metric tensor "G",
### Hyperbolic angle.
A hyperbolic angle is an argument of a hyperbolic function just as the "circular angle" is the argument of a circular function. The comparison can be visualized as the size of the openings of a hyperbolic sector and a circular sector since the areas of these sectors correspond to the angle magnitudes in each case. Unlike the circular angle, the hyperbolic angle is unbounded. When the circular and hyperbolic functions are viewed as infinite series in their angle argument, the circular ones are just alternating series forms of the hyperbolic functions. This weaving of the two types of angle and function was explained by Leonhard Euler in "Introduction to the Analysis of the Infinite".
## Angles in geography and astronomy.
In geography, the location of any point on the Earth can be identified using a "geographic coordinate system". This system specifies the latitude and longitude of any location in terms of angles subtended at the center of the Earth, using the equator and (usually) the Greenwich meridian as references.
In astronomy, a given point on the celestial sphere (that is, the apparent position of an astronomical object) can be identified using any of several "astronomical coordinate systems", where the references vary according to the particular system. Astronomers measure the "angular separation" of two stars by imagining two lines through the center of the Earth, each intersecting one of the stars. The angle between those lines can be measured and is the angular separation between the two stars.
In both geography and astronomy, a sighting direction can be specified in terms of a vertical angle such as altitude /elevation with respect to the horizon as well as the azimuth with respect to north.
Astronomers also measure the "apparent size" of objects as an angular diameter. For example, the full moon has an angular diameter of approximately 0.5°, when viewed from Earth. One could say, "The Moon's diameter subtends an angle of half a degree." The small-angle formula can be used to convert such an angular measurement into a distance/size ratio.

</doc>
<doc id="1197" url="https://en.wikipedia.org/wiki?curid=1197" title="Asa">
Asa

Asa may refer to:

</doc>
<doc id="1198" url="https://en.wikipedia.org/wiki?curid=1198" title="Acoustics">
Acoustics

Acoustics is a branch of physics that deals with the study of mechanical waves in gases, liquids, and solids including topics such as vibration, sound, ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries.
Hearing is one of the most crucial means of survival in the animal world and speech is one of the most distinctive characteristics of human development and culture. Accordingly, the science of acoustics spreads across many facets of human society—music, medicine, architecture, industrial production, warfare and more. Likewise, animal species such as songbirds and frogs use sound and hearing as a key element of mating rituals or marking territories. Art, craft, science and technology have provoked one another to advance the whole, as in many other fields of knowledge. Robert Bruce Lindsay's "Wheel of Acoustics" is a well accepted overview of the various fields in acoustics.
## History.
### Etymology.
The word "acoustic" is derived from the Greek word ἀκουστικός ("akoustikos"), meaning "of or for hearing, ready to hear" and that from ἀκουστός ("akoustos"), "heard, audible", which in turn derives from the verb ἀκούω("akouo"), "I hear".
The Latin synonym is "sonic", after which the term sonics used to be a synonym for acoustics and later a branch of acoustics. Frequencies above and below the audible range are called "ultrasonic" and "infrasonic", respectively.
### Early research in acoustics.
In the 6th century BC, the ancient Greek philosopher Pythagoras wanted to know why some combinations of musical sounds seemed more beautiful than others, and he found answers in terms of numerical ratios representing the harmonic overtone series on a string. He is reputed to have observed that when the lengths of vibrating strings are expressible as ratios of integers (e.g. 2 to 3, 3 to 4), the tones produced will be harmonious, and the smaller the integers the more harmonious the sounds. For example, a string of a certain length would sound particularly harmonious with a string of twice the length (other factors being equal). In modern parlance, if a string sounds the note C when plucked, a string twice as long will sound a C an octave lower. In one system of musical tuning, the tones in between are then given by 16:9 for D, 8:5 for E, 3:2 for F, 4:3 for G, 6:5 for A, and 16:15 for B, in ascending order.
Aristotle (384–322 BC) understood that sound consisted of compressions and rarefactions of air which "falls upon and strikes the air which is next to it...", a very good expression of the nature of wave motion. "On Things Heard", generally ascribed to Strato of Lampsacus, states that the pitch is related to the frequency of vibrations of the air and to the speed of sound.
In about 20 BC, the Roman architect and engineer Vitruvius wrote a treatise on the acoustic properties of theaters including discussion of interference, echoes, and reverberation—the beginnings of architectural acoustics. In Book V of his "De architectura" ("The Ten Books of Architecture") Vitruvius describes sound as a wave comparable to a water wave extended to three dimensions, which, when interrupted by obstructions, would flow back and break up following waves. He described the ascending seats in ancient theaters as designed to prevent this deterioration of sound and also recommended bronze vessels of appropriate sizes be placed in theaters to resonate with the fourth, fifth and so on, up to the double octave, in order to resonate with the more desirable, harmonious notes.
During the Islamic golden age, Abū Rayhān al-Bīrūnī (973-1048) is believed to postulated that the speed of sound was much slower than the speed of light.
The physical understanding of acoustical processes advanced rapidly during and after the Scientific Revolution. Mainly Galileo Galilei (1564–1642) but also Marin Mersenne (1588–1648), independently, discovered the complete laws of vibrating strings (completing what Pythagoras and Pythagoreans had started 2000 years earlier). Galileo wrote "Waves are produced by the vibrations of a sonorous body, which spread through the air, bringing to the tympanum of the ear a stimulus which the mind interprets as sound", a remarkable statement that points to the beginnings of physiological and psychological acoustics. Experimental measurements of the speed of sound in air were carried out successfully between 1630 and 1680 by a number of investigators, prominently Mersenne. Meanwhile, Newton (1642–1727) derived the relationship for wave velocity in solids, a cornerstone of physical acoustics (Principia, 1687).
### Age of Enlightenment and onward.
Substantial progress in acoustics, resting on firmer mathematical and physical concepts, was made during the eighteenth century by Euler (1707–1783), Lagrange (1736–1813), and d'Alembert (1717–1783). During this era, continuum physics, or field theory, began to receive a definite mathematical structure. The wave equation emerged in a number of contexts, including the propagation of sound in air.
In the nineteenth century the major figures of mathematical acoustics were Helmholtz in Germany, who consolidated the field of physiological acoustics, and Lord Rayleigh in England, who combined the previous knowledge with his own copious contributions to the field in his monumental work "The Theory of Sound" (1877). Also in the 19th century, Wheatstone, Ohm, and Henry developed the analogy between electricity and acoustics.
The twentieth century saw a burgeoning of technological applications of the large body of scientific knowledge that was by then in place. The first such application was Sabine's groundbreaking work in architectural acoustics, and many others followed. Underwater acoustics was used for detecting submarines in the first World War. Sound recording and the telephone played important roles in a global transformation of society. Sound measurement and analysis reached new levels of accuracy and sophistication through the use of electronics and computing. The ultrasonic frequency range enabled wholly new kinds of application in medicine and industry. New kinds of transducers (generators and receivers of acoustic energy) were invented and put to use.
## Fundamental concepts of acoustics.
### Definition.
Acoustics is defined by ANSI/ASA S1.1-2013 as "(a) Science of sound, including its production, transmission, and effects, including biological and psychological effects. (b) Those qualities of a room that, together, determine its character with respect to auditory effects."
The study of acoustics revolves around the generation, propagation and reception of mechanical waves and vibrations.
The steps shown in the above diagram can be found in any acoustical event or process. There are many kinds of cause, both natural and volitional. There are many kinds of transduction process that convert energy from some other form into sonic energy, producing a sound wave. There is one fundamental equation that describes sound wave propagation, the acoustic wave equation, but the phenomena that emerge from it are varied and often complex. The wave carries energy throughout the propagating medium. Eventually this energy is transduced again into other forms, in ways that again may be natural and/or volitionally contrived. The final effect may be purely physical or it may reach far into the biological or volitional domains. The five basic steps are found equally well whether we are talking about an earthquake, a submarine using sonar to locate its foe, or a band playing in a rock concert.
The central stage in the acoustical process is wave propagation. This falls within the domain of physical acoustics. In fluids, sound propagates primarily as a pressure wave. In solids, mechanical waves can take many forms including longitudinal waves, transverse waves and surface waves.
Acoustics looks first at the pressure levels and frequencies in the sound wave and how the wave interacts with the environment. This interaction can be described as either a diffraction, interference or a reflection or a mix of the three. If several media are present, a refraction can also occur. Transduction processes are also of special importance to acoustics.
### Wave propagation: pressure levels.
In fluids such as air and water, sound waves propagate as disturbances in the ambient pressure level. While this disturbance is usually small, it is still noticeable to the human ear. The smallest sound that a person can hear, known as the threshold of hearing, is nine orders of magnitude smaller than the ambient pressure. The loudness of these disturbances is related to the sound pressure level (SPL) which is measured on a logarithmic scale in decibels.
### Wave propagation: frequency.
Physicists and acoustic engineers tend to discuss sound pressure levels in terms of frequencies, partly because this is how our ears interpret sound. What we experience as "higher pitched" or "lower pitched" sounds are pressure vibrations having a higher or lower number of cycles per second. In a common technique of acoustic measurement, acoustic signals are sampled in time, and then presented in more meaningful forms such as octave bands or time frequency plots. Both of these popular methods are used to analyze sound and better understand the acoustic phenomenon.
The entire spectrum can be divided into three sections: audio, ultrasonic, and infrasonic. The audio range falls between 20 Hz and 20,000 Hz. This range is important because its frequencies can be detected by the human ear. This range has a number of applications, including speech communication and music. The ultrasonic range refers to the very high frequencies: 20,000 Hz and higher. This range has shorter wavelengths which allow better resolution in imaging technologies. Medical applications such as ultrasonography and elastography rely on the ultrasonic frequency range. On the other end of the spectrum, the lowest frequencies are known as the infrasonic range. These frequencies can be used to study geological phenomena such as earthquakes.
Analytic instruments such as the spectrum analyzer facilitate visualization and measurement of acoustic signals and their properties. The spectrogram produced by such an instrument is a graphical display of the time varying pressure level and frequency profiles which give a specific acoustic signal its defining character.
### Transduction in acoustics.
A transducer is a device for converting one form of energy into another. In an electroacoustic context, this means converting sound energy into electrical energy (or vice versa). Electroacoustic transducers include loudspeakers, microphones, particle velocity sensors, hydrophones and sonar projectors. These devices convert a sound wave to or from an electric signal. The most widely used transduction principles are electromagnetism, electrostatics and piezoelectricity.
The transducers in most common loudspeakers (e.g. woofers and tweeters), are electromagnetic devices that generate waves using a suspended diaphragm driven by an electromagnetic voice coil, sending off pressure waves. Electret microphones and condenser microphones employ electrostatics—as the sound wave strikes the microphone's diaphragm, it moves and induces a voltage change. The ultrasonic systems used in medical ultrasonography employ piezoelectric transducers. These are made from special ceramics in which mechanical vibrations and electrical fields are interlinked through a property of the material itself.
## Acoustician.
An acoustician is an expert in the science of sound.
### Education.
There are many types of acoustician, but they usually have a Bachelor's degree or higher qualification. Some possess a degree in acoustics, while others enter the discipline via studies in fields such as physics or engineering. Much work in acoustics requires a good grounding in Mathematics and science. Many acoustic scientists work in research and development. Some conduct basic research to advance our knowledge of the perception (e.g. hearing, psychoacoustics or neurophysiology) of speech, music and noise. Other acoustic scientists advance understanding of how sound is affected as it moves through environments, e.g. underwater acoustics, architectural acoustics or structural acoustics. Other areas of work are listed under subdisciplines below. Acoustic scientists work in government, university and private industry laboratories. Many go on to work in Acoustical Engineering. Some positions, such as Faculty (academic staff) require a Doctor of Philosophy.
## Subdisciplines.
### Archaeoacoustics.
Archaeoacoustics, also known as the archaeology of sound, is one of the only ways to experience the past with senses other than our eyes. Archaeoacoustics is studied by testing the acoustic properties of prehistoric sites, including caves. Iegor Rezkinoff, a sound archaeologist, studies the acoustic properties of caves through natural sounds like humming and whistling. Archaeological theories of acoustics are focused around ritualistic purposes as well as a way of echolocation in the caves. In archaeology, acoustic sounds and rituals directly correlate as specific sounds were meant to bring ritual participants closer to a spiritual awakening. Parallels can also be drawn between cave wall paintings and the acoustic properties of the cave; they are both dynamic. Because archaeoacoustics is a fairly new archaeological subject, acoustic sound is still being tested in these prehistoric sites today.
### Aeroacoustics.
Aeroacoustics is the study of noise generated by air movement, for instance via turbulence, and the movement of sound through the fluid air. This knowledge is applied in acoustical engineering to study how to quieten aircraft. Aeroacoustics is important for understanding how wind musical instruments work.
### Acoustic signal processing.
Acoustic signal processing is the electronic manipulation of acoustic signals. Applications include: active noise control; design for hearing aids or cochlear implants; echo cancellation; music information retrieval, and perceptual coding (e.g. MP3 or Opus).
### Architectural acoustics.
Architectural acoustics (also known as building acoustics) involves the scientific understanding of how to achieve good sound within a building. It typically involves the study of speech intelligibility, speech privacy, music quality, and vibration reduction in the built environment. Commonly studied environments are hospitals, classrooms, dwellings, performance venues, recording and broadcasting studios. Focus considerations include room acoustics, airborne and impact transmission in building structures, airborne and structure-borne noise control, noise control of building systems and electroacoustic systems[].
### Bioacoustics.
Bioacoustics is the scientific study of the hearing and calls of animal calls, as well as how animals are affected by the acoustic and sounds of their habitat.
### Electroacoustics.
This subdiscipline is concerned with the recording, manipulation and reproduction of audio using electronics. This might include products such as mobile phones, large scale public address systems or virtual reality systems in research laboratories.
### Environmental noise and soundscapes.
Environmental acoustics is concerned with noise and vibration caused by railways, road traffic, aircraft, industrial equipment and recreational activities. The main aim of these studies is to reduce levels of environmental noise and vibration. Research work now also has a focus on the positive use of sound in urban environments: soundscapes and tranquility.
### Musical acoustics.
Musical acoustics is the study of the physics of acoustic instruments; the audio signal processing used in electronic music; the computer analysis of music and composition, and the perception and cognitive neuroscience of music.
### Noise.
The goal this acoustics sub-discipline is to reduce the impact of unwanted sound. Scope of noise studies includes the generation, propagation, and impact on structures, objects, and people.
Noise research investigates the impact of noise on humans and animals to include work in definitions, abatement, transportation noise, hearing protection, Jet and rocket noise, building system noise and vibration, atmospheric sound propagation, soundscapes, and low-frequency sound.
### Psychoacoustics.
Many studies have been conducted to identify the relationship between acoustics and cognition, or more commonly known as psychoacoustics, in which what one hears is a combination of perception and biological aspects. The information intercepted by the passage of sound waves through the ear is understood and interpreted through the brain, emphasizing the connection between the mind and acoustics. Psychological changes have been seen as brain waves slow down or speed up as a result of varying auditory stimulus which can in turn affect the way one thinks, feels, or even behaves. This correlation can be viewed in normal, everyday situations in which listening to an upbeat or uptempo song can cause one's foot to start tapping or a slower song can leave one feeling calm and serene. In a deeper biological look at the phenomenon of psychoacoustics, it was discovered that the central nervous system is activated by basic acoustical characteristics of music. By observing how the central nervous system, which includes the brain and spine, is influenced by acoustics, the pathway in which acoustic affects the mind, and essentially the body, is evident.
### Speech.
Acousticians study the production, processing and perception of speech. Speech recognition and Speech synthesis are two important areas of speech processing using computers. The subject also overlaps with the disciplines of physics, physiology, psychology, and linguistics.
### Structural Vibration and Dynamics.
Structural acoustics is the study of motions and interactions of mechanical systems with their environments and the methods of their measurement, analysis, and control. There are several sub-disciplines found within this regime:
Applications might include: ground vibrations from railways; vibration isolation to reduce vibration in operating theatres; studying how vibration can damage health (vibration white finger); vibration control to protect a building from earthquakes, or measuring how structure-borne sound moves through buildings.
### Ultrasonics.
Ultrasonics deals with sounds at frequencies too high to be heard by humans. Specialisms include medical ultrasonics (including medical ultrasonography), sonochemistry, ultrasonic testing, material characterisation and underwater acoustics (sonar).
### Underwater acoustics.
Underwater acoustics is the scientific study of natural and man-made sounds underwater. Applications include sonar to locate submarines, underwater communication by whales, climate change monitoring by measuring sea temperatures acoustically, sonic weapons, and marine bioacoustics.

</doc>
<doc id="1199" url="https://en.wikipedia.org/wiki?curid=1199" title="Angle tribe">
Angle tribe



</doc>
<doc id="1200" url="https://en.wikipedia.org/wiki?curid=1200" title="Atomic physics">
Atomic physics

Atomic physics is the field of physics that studies atoms as an isolated system of electrons and an atomic nucleus. It is primarily concerned with the arrangement of electrons around the nucleus and
the processes by which these arrangements change. This comprises ions, neutral atoms and, unless otherwise stated, it can be assumed that the term "atom" includes ions.
The term "atomic physics" can be associated with nuclear power and nuclear weapons, due to the synonymous use of "atomic" and "nuclear" in standard English. Physicists distinguish between atomic physics—which deals with the atom as a system consisting of a nucleus and electrons—and nuclear physics, which studies nuclear reactions and special properties of atomic nuclei.
As with many scientific fields, strict delineation can be highly contrived and atomic physics is often considered in the wider context of "atomic, molecular, and optical physics". Physics research groups are usually so classified.
## Isolated atoms.
Atomic physics primarily considers atoms in isolation. Atomic models will consist of a single nucleus that may be surrounded by one or more bound electrons. It is not concerned with the formation of molecules (although much of the physics is identical), nor does it examine atoms in a solid state as condensed matter. It is concerned with processes such as ionization and excitation by photons or collisions with atomic particles.
While modelling atoms in isolation may not seem realistic, if one considers atoms in a gas or plasma then the time-scales for atom-atom interactions are huge in comparison to the atomic processes that are generally considered. This means that the individual atoms can be treated as if each were in isolation, as the vast majority of the time they are. By this consideration atomic physics provides the underlying theory in plasma physics and atmospheric physics, even though both deal with very large numbers of atoms.
## Electronic configuration.
Electrons form notional shells around the nucleus. These are normally in a ground state but can be excited by the absorption of energy from light (photons), magnetic fields, or interaction with a colliding particle (typically ions or other electrons).
 Electrons that populate a shell are said to be in a bound state. The energy necessary to remove an electron from its shell (taking it to infinity) is called the binding energy. Any quantity of energy absorbed by the electron in excess of this amount is converted to kinetic energy according to the conservation of energy. The atom is said to have undergone the process of ionization.
If the electron absorbs a quantity of energy less than the binding energy, it will be transferred to an excited state. After a certain time, the electron in an excited state will "jump" (undergo a transition) to a lower state. In a neutral atom, the system will emit a photon of the difference in energy, since energy is conserved.
If an inner electron has absorbed more than the binding energy (so that the atom ionizes), then a more outer electron may undergo a transition to fill the inner orbital. In this case, a visible photon or a characteristic x-ray is emitted, or a phenomenon known as the Auger effect may take place, where the released energy is transferred to another bound electron, causing it to go into the continuum. The Auger effect allows one to multiply ionize an atom with a single photon.
There are rather strict selection rules as to the electronic configurations that can be reached by excitation by light — however there are no such rules for excitation by collision processes.
## History and developments.
One of the earliest steps towards atomic physics was the recognition that matter was composed
of "atoms". It forms a part of the texts written in 6th century BC to 2nd century BC such as those of Democritus or Vaisheshika Sutra written by Kanad. This theory was later developed in the modern sense of the basic unit of a chemical element by the British chemist and physicist John Dalton in the 18th century. At this stage, it wasn't clear what atoms were although they could be described and classified by their properties (in bulk). The invention of the periodic system of elements by Mendeleev was another great step forward.
The true beginning of atomic physics is marked by the discovery of spectral lines and attempts to describe the phenomenon, most notably by Joseph von Fraunhofer. The study of these lines led to the Bohr atom model and to the birth of quantum mechanics. In seeking to explain atomic spectra an entirely new mathematical model of matter was revealed. As far as atoms and their electron shells were concerned, not only did this yield a better overall description, i.e. the atomic orbital model, but it also provided a new theoretical basis for chemistry
(quantum chemistry) and spectroscopy.
Since the Second World War, both theoretical and experimental fields have advanced at a rapid pace. This can be attributed to progress in computing technology, which has allowed larger and more sophisticated models of atomic structure and associated collision processes. Similar technological advances in accelerators, detectors, magnetic field generation and lasers have greatly assisted experimental work.

</doc>
<doc id="1201" url="https://en.wikipedia.org/wiki?curid=1201" title="American Sign Language">
American Sign Language

American Sign Language (ASL) is a natural language that serves as the predominant sign language of Deaf communities in the United States and most of Anglophone Canada. ASL is a complete and organized visual language that is expressed by facial expression as well as movements and motions with the hands. Besides North America, dialects of ASL and ASL-based creoles are used in many countries around the world, including much of West Africa and parts of Southeast Asia. ASL is also widely learned as a second language, serving as a lingua franca. ASL is most closely related to French Sign Language (LSF). It has been proposed that ASL is a creole language of LSF, although ASL shows features atypical of creole languages, such as agglutinative morphology.
ASL originated in the early 19th century in the American School for the Deaf (ASD) in West Hartford, Connecticut, from a situation of language contact. Since then, ASL use has propagated widely by schools for the deaf and Deaf community organizations. Despite its wide use, no accurate count of ASL users has been taken. Reliable estimates for American ASL users range from 250,000 to 500,000 persons, including a number of children of deaf adults.
ASL signs have a number of phonemic components, such as movement of the face, the torso, and the hands. ASL is not a form of pantomime although iconicity plays a larger role in ASL than in spoken languages. English loan words are often borrowed through fingerspelling, although ASL grammar is unrelated to that of English. ASL has verbal agreement and aspectual marking and has a productive system of forming agglutinative classifiers. Many linguists believe ASL to be a subject–verb–object (SVO) language. However, there are several alternative proposals to account for ASL word order.
## Classification.
ASL emerged as a language in the American School for the Deaf (ASD), founded by Thomas Gallaudet in 1817, which brought together Old French Sign Language, various village sign languages, and home sign systems; ASL was created in that situation by language contact. ASL was influenced by its forerunners but distinct from all of them.
The influence of French Sign Language (LSF) on ASL is readily apparent; for example, it has been found that about 58% of signs in modern ASL are cognate to Old French Sign Language signs. However, that is far less than the standard 80% measure used to determine whether related languages are actually dialects. That suggests that nascent ASL was highly affected by the other signing systems brought by the ASD students although the school's original director, Laurent Clerc, taught in LSF. In fact, Clerc reported that he often learned the students' signs rather than conveying LSF:
It has been proposed that ASL is a creole in which LSF is the superstrate language and the native village sign languages are substrate languages. However, more recent research has shown that modern ASL does not share many of the structural features that characterize creole languages. ASL may have begun as a creole and then undergone structural change over time, but it is also possible that it was never a creole-type language. There are modality-specific reasons that sign languages tend towards agglutination, such as the ability to simultaneously convey information via the face, head, torso, and other body parts. That might override creole characteristics such as the tendency towards isolating morphology. Additionally, Clerc and Thomas Hopkins Gallaudet may have used an artificially constructed form of manually coded language in instruction rather than true LSF.
Although the United States, the United Kingdom, and Australia share English as a common oral and written language, ASL is not mutually intelligible with either British Sign Language (BSL) or Auslan. All three languages show degrees of borrowing from English, but that alone is not sufficient for cross-language comprehension. It has been found that a relatively high percentage (37–44%) of ASL signs have similar translations in Auslan, which for oral languages would suggest that they belong to the same language family. However, that does not seem justified historically for ASL and Auslan, and it is likely that the resemblance is caused by the higher degree of iconicity in sign languages in general as well as contact with English.
American Sign Language is growing in popularity in many states. Many people in high school and colleges desire to take it as a foreign language, but until recently, it was not a creditable foreign language elective. The issue was that many did not consider it a foreign language. ASL users, however, have a very distinct culture, and they interact very differently when they talk. Their facial expressions and hand movements reflect what they are communicating. They also have their own sentence structure, which sets the language apart.
American Sign Language is now being accepted by many colleges as a foreign language credit; many states are making it mandatory to accept it.
## History.
Prior to the birth of ASL, sign language had been used by various communities in the United States. In the United States, as elsewhere in the world, hearing families with deaf children have historically employed ad hoc home sign, which often reaches much higher levels of sophistication than gestures used by hearing people in spoken conversation. As early as 1541 at first contact by Francisco Vásquez de Coronado, there were reports that the Indigenous peoples of the Great Plains widely spoke a sign language to communicate across vast national and linguistic lines.
In the 19th century, a "triangle" of village sign languages developed in New England: one in Martha's Vineyard, Massachusetts; one in Henniker, New Hampshire, and one in Sandy River Valley, Maine. Martha's Vineyard Sign Language (MVSL), which was particularly important for the history of ASL, was used mainly in Chilmark, Massachusetts. Due to intermarriage in the original community of English settlers of the 1690s, and the recessive nature of genetic deafness, Chilmark had a high 4% rate of genetic deafness. MVSL was used even by hearing residents whenever a deaf person was present, and also in some situations where spoken language would be ineffective or inappropriate, such as during church sermons or between boats at sea.
ASL is thought to have originated in the American School for the Deaf (ASD), founded in Hartford, Connecticut, in 1817. Originally known as "The American Asylum, At Hartford, For The Education And Instruction Of The Deaf And Dumb", the school was founded by the Yale graduate and divinity student Thomas Hopkins Gallaudet. Gallaudet, inspired by his success in demonstrating the learning abilities of a young deaf girl Alice Cogswell, traveled to Europe in order to learn deaf pedagogy from European institutions. Ultimately, Gallaudet chose to adopt the methods of the French Institut National de Jeunes Sourds de Paris, and convinced Laurent Clerc, an assistant to the school's founder Charles-Michel de l'Épée, to accompany him back to the United States. Upon his return, Gallaudet founded the ASD on April 15, 1817.
The largest group of students during the first seven decades of the school were from Martha's Vineyard, and they brought MVSL with them. There were also 44 students from around Henniker, New Hampshire, and 27 from the Sandy River valley in Maine, each of which had their own village sign language. Other students brought knowledge of their own home signs. Laurent Clerc, the first teacher at ASD, taught using French Sign Language (LSF), which itself had developed in the Parisian school for the deaf established in 1755. From that situation of language contact, a new language emerged, now known as ASL.
More schools for the deaf were founded after ASD, and knowledge of ASL spread to those schools. In addition, the rise of Deaf community organizations bolstered the continued use of ASL. Societies such as the National Association of the Deaf and the National Fraternal Society of the Deaf held national conventions that attracted signers from across the country. All of that contributed to ASL's wide use over a large geographical area, atypical of a sign language.
Up to the 1950s, the predominant method in deaf education was oralism, acquiring oral language comprehension and production. Linguists did not consider sign language to be true "language" but as something inferior. Recognition of the legitimacy of ASL was achieved by William Stokoe, a linguist who arrived at Gallaudet University in 1955 when that was still the dominant assumption. Aided by the Civil Rights Movement of the 1960s, Stokoe argued for manualism, the use of sign language in deaf education. Stokoe noted that sign language shares the important features that oral languages have as a means of communication, and even devised a transcription system for ASL. In doing so, Stokoe revolutionized both deaf education and linguistics. In the 1960s, ASL was sometimes referred to as "Ameslan", but that term is now considered obsolete.
## Population.
Counting the number of ASL signers is difficult because ASL users have never been counted by the American census. The ultimate source for current estimates of the number of ASL users in the United States is a report for the National Census of the Deaf Population (NCDP) by Schein and Delk (1974). Based on a 1972 survey of the NCDP, Schein and Delk provided estimates consistent with a signing population between 250,000 and 500,000. The survey did not distinguish between ASL and other forms of signing; in fact, the name "ASL" was not yet in widespread use.
Incorrect figures are sometimes cited for the population of ASL users in the United States based on misunderstandings of known statistics. Demographics of the deaf population have been confused with those of ASL use since adults who become deaf late in life rarely use ASL in the home. That accounts for currently-cited estimations that are greater than 500,000; such mistaken estimations can reach as high as 15,000,000. A 100,000-person lower bound has been cited for ASL users; the source of that figure is unclear, but it may be an estimate of prelingual deafness, which is correlated with but not equivalent to signing.
ASL is sometimes incorrectly cited as the third- or fourth-most-spoken language in the United States. Those figures misquote Schein and Delk (1974), who actually concluded that ASL speakers constituted the third-largest population "requiring an interpreter in court". Although that would make ASL the third-most used language among monolinguals other than English, it does not imply that it is the fourth-most-spoken language in the United States since speakers of other languages may also speak English.
## Geographic distribution.
ASL is used throughout Anglo-America. That contrasts with Europe, where a variety of sign languages are used within the same continent. The unique situation of ASL seems to have been caused by the proliferation of ASL through schools influenced by the American School for the Deaf, wherein ASL originated, and the rise of community organizations for the Deaf.
Throughout West Africa, ASL-based sign languages are signed by educated Deaf adults. Such languages, imported by boarding schools, are often considered by associations to be the official sign languages of their countries and are named accordingly, such as Nigerian Sign Language, Ghanaian Sign Language. Such signing systems are found in Benin, Burkina Faso, Ivory Coast, Ghana, Liberia, Mauritania, Mali, Nigeria, and Togo. Due to lack of data, it is still an open question how similar those sign languages are to the variety of ASL used in America.
In addition to the aforementioned West African countries, ASL is reported to be used as a first language in Barbados, Bolivia, Cambodia (alongside Cambodian Sign Language), the Central African Republic, Chad, China (Hong Kong), the Democratic Republic of the Congo, Gabon, Jamaica, Kenya, Madagascar, the Philippines, Singapore, and Zimbabwe. ASL is also used as a lingua franca throughout the deaf world, widely learned as a second language.
## Regional variation.
### Sign production.
Sign production can often vary according to location. Signers from the South tend to sign with more flow and ease. Native signers from New York have been reported as signing comparatively more quickly and sharply. Sign production of native Californian signers has also been reported as being fast as well. Research on that phenomenon often concludes that the fast-paced production for signers from the coasts could be due to the fast-paced nature of living in large metropolitan areas. That conclusion also supports how the ease with which Southern sign could be caused by the easygoing environment of the South in comparison to that of the coasts.
Sign production can also vary depending on age and native language. For example, sign production of letters may vary in older signers. Slight differences in finger spelling production can be a signal of age. Additionally, signers who learned American Sign Language as a second language vary in production. For Deaf signers who learned a different sign language before learning American Sign Language, qualities of their native language may show in their ASL production. Some examples of that varied production include finger spelling towards the body, instead of away from it, and signing certain movement from bottom to top, instead of top to bottom. Hearing people who learn American Sign Language also have noticeable differences in signing production. The most notable production difference of hearing people learning American Sign Language is their rhythm and arm posture.
### Sign variants.
Most popularly, there are variants of the signs for English words such as "birthday", "pizza", "Halloween", "early", and "soon", just a sample of the most commonly recognized signs with variant based on regional change. The sign for "school" is commonly varied between black and white signers. The variation between sign produced by black and white signers is sometimes referred to as Black American Sign Language.
### History and implications.
The prevalence of residential Deaf schools can account for much of the regional variance of signs and sign productions across the United States. Deaf schools often serve students of the state in which the school resides. That limited access to signers from other regions, combined with the residential quality of Deaf Schools promoted specific use of certain sign variants. Native signers did not have much access to signers from other regions during the beginning years of their education. It is hypothesized that because of that seclusion, certain variants of a sign prevailed over others due to the choice of variant used by the student of the school/signers in the community.
However, American Sign Language does not appear to be vastly varied in comparison to other signed languages. That is because when Deaf education was beginning in the United States, many educators flocked to the American School for the Deaf in Hartford, Connecticut, whose central location for the first generation of educators in Deaf education to learn American Sign Language allows ASL to be more standardized than its variant.
## Varieties.
Varieties of ASL are found throughout the world. There is little difficulty in comprehension among the varieties of the United States and Canada.
Mutual intelligibility among those ASL varieties is high, and the variation is primarily lexical. For example, there are three different words for English "about" in Canadian ASL; the standard way, and two regional variations (Atlantic and Ontario). Variation may also be phonological, meaning that the same sign may be signed in a different way depending on the region. For example, an extremely common type of variation is between the handshapes /1/, /L/, and /5/ in signs with one handshape.
There is also a distinct variety of ASL used by the Black Deaf community. Black ASL evolved as a result of racially segregated schools in some states, which included the residential schools for the deaf. Black ASL differs from standard ASL in vocabulary, phonology, and some grammatical structure. While African American English (AAE) is generally viewed as more innovating than standard English, Black ASL is more conservative than standard ASL, preserving older forms of many signs. Black sign language speakers use more two-handed signs than in mainstream ASL, are less likely to show assimilatory lowering of signs produced on the forehead (e.g. KNOW) and use a wider signing space. Modern Black ASL borrows a number of idioms from AAE; for instance, the AAE idiom "I feel you" is calqued into Black ASL.
ASL is used internationally as a lingua franca, and a number of closely related sign languages derived from ASL are used in many different countries. Even so, there have been varying degrees of divergence from standard ASL in those imported ASL varieties. Bolivian Sign Language is reported to be a dialect of ASL, no more divergent than other acknowledged dialects. On the other hand, it is also known that some imported ASL varieties have diverged to the extent of being separate languages. For example, Malaysian Sign Language, which has ASL origins, is no longer mutually comprehensible with ASL and must be considered its own language. For some imported ASL varieties, such as those used in West Africa, it is still an open question how similar they are to American ASL.
When communicating with hearing English speakers, ASL-speakers often use what is commonly called Pidgin Signed English (PSE) or 'contact signing', a blend of English structure with ASL vocabulary. Various types of PSE exist, ranging from highly English-influenced PSE (practically relexified English) to PSE which is quite close to ASL lexically and grammatically, but may alter some subtle features of ASL grammar. Fingerspelling may be used more often in PSE than it is normally used in ASL. There have been some constructed sign languages, known as Manually Coded English (MCE), which match English grammar exactly and simply replace spoken words with signs; those systems are not considered to be varieties of ASL.
Tactile ASL (TASL) is a variety of ASL used throughout the United States by and with the deaf-blind. It is particularly common among those with Usher's syndrome. It results in deafness from birth followed by loss of vision later in life; consequently, those with Usher's syndrome often grow up in the Deaf community using ASL, and later transition to TASL. TASL differs from ASL in that signs are produced by touching the palms, and there are some grammatical differences from standard ASL in order to compensate for the lack of non-manual signing.
## Stigma.
In 2013 the White House published a response to a petition that gained over 37,000 signatures to "officially recognize American Sign Language as a community language and a language of instruction in schools". The response is titled "there shouldn't be any stigma about American Sign Language" and addressed that ASL is a vital language for the Deaf and hard of hearing. Stigmas associated with sign languages and the use of sign for educating children often lead to the absence of sign during periods in children's lives when they can access languages most effectively. Scholars such as Beth S. Benedict advocate not only for bilingualism (using ASL and English training) but also for early childhood intervention for children who are deaf. York University psychologist Ellen Bialystok has also campaigned for bilingualism, arguing that those who are bilingual acquire cognitive skills that may help to prevent dementia later in life.
Most children born to deaf parents are hearing. Known as CODAs ("Children Of Deaf Adults"), they are often more culturally Deaf than deaf children, most of whom are born to hearing parents. Unlike many deaf children, CODAs acquire ASL as well as Deaf cultural values and behaviors from birth. Such bilingual hearing children may be mistakenly labeled as being "slow learners" or as having "language difficulties" because of preferential attitudes towards spoken language.
## Writing systems.
Although there is no well-established writing system for ASL, written sign language dates back almost two centuries. The first systematic writing system for a sign language seems to be that of Roch-Ambroise Auguste Bébian, developed in 1825. However, written sign language remained marginal among the public. In the 1960s, linguist William Stokoe created Stokoe notation specifically for ASL. It is alphabetic, with a letter or diacritic for every phonemic (distinctive) hand shape, orientation, motion, and position, though it lacks any representation of facial expression, and is better suited for individual words than for extended passages of text. Stokoe used that system for his 1965 "A Dictionary of American Sign Language on Linguistic Principles".
SignWriting, proposed in 1974 by Valerie Sutton, is the first writing system to gain use among the public and the first writing system for sign languages to be included in the Unicode Standard. SignWriting consists of more than 5000 distinct iconic graphs/glyphs. Currently, it is in use in many schools for the Deaf, particularly in Brazil, and has been used in International Sign forums with speakers and researchers in more than 40 countries, including Brazil, Ethiopia, France, Germany, Italy, Portugal, Saudi Arabia, Slovenia, Tunisia, and the United States. Sutton SignWriting has both a printed and an electronically produced form so that persons can use the system anywhere that oral languages are written (personal letters, newspapers, and media, academic research). The systematic examination of the International Sign Writing Alphabet (ISWA) as an equivalent usage structure to the International Phonetic Alphabet for spoken languages has been proposed. According to some researchers, SignWriting is not a phonemic orthography and does not have a one-to-one map from phonological forms to written forms. That assertion has been disputed, and the process for each country to look at the ISWA and create a phonemic/morphemic assignment of features of each sign language was proposed by researchers Msc. Roberto Cesar Reis da Costa and Madson Barreto in a thesis forum on June 23, 2014. The SignWriting community has an open project on Wikimedia Labs to support the various Wikimedia projects on Wikimedia Incubator and elsewhere involving SignWriting. The ASL Wikipedia request was marked as eligible in 2008 and the test ASL Wikipedia has 50 articles written in ASL using SignWriting.
The most widely used transcription system among academics is HamNoSys, developed at the University of Hamburg. Based on Stokoe Notation, HamNoSys was expanded to about 200 graphs in order to allow transcription of any sign language. Phonological features are usually indicated with single symbols, though the group of features that make up a handshape is indicated collectively with a symbol.
Several additional candidates for written ASL have appeared over the years, including SignFont, ASL-phabet, and Si5s.
For English-speaking audiences, ASL is often glossed using English words. Such glosses are typically all-capitalized and are arranged in ASL order. For example, the ASL sentence DOG NOW CHASE&gt;IX=3 CAT, meaning "the dog is chasing the cat", uses NOW to mark ASL progressive aspect and shows ASL verbal inflection for the third person (written with &gt;IX=3). However, glossing is not used to write the language for speakers of ASL.
## Phonology.
Each sign in ASL is composed of a number of distinctive components, generally referred to as parameters. A sign may use one hand or both. All signs can be described using the five parameters involved in signed languages, which are handshape, movement, palm orientation, location and non-manual markers. Just as phonemes of sound distinguish meaning in spoken languages, those parameters are the phonemes that distinguish meaning in signed languages like ASL. Changing any one of them may change the meaning of a sign, as illustrated by the ASL signs THINK and DISAPPOINTED:
There are also meaningful non-manual signals in ASL, which may include movement of the eyebrows, the cheeks, the nose, the head, the torso, and the eyes.
William Stokoe proposed that such components are analogous to the phonemes of spoken languages. There has also been a proposal that they are analogous to classes like place and manner of articulation. As in spoken languages, those phonological units can be split into distinctive features. For instance, the handshapes /2/ and /3/ are distinguished by the presence or absence of the feature [± closed thumb], as illustrated to the right. ASL has processes of allophony and phonotactic restrictions. There is ongoing research into whether ASL has an analog of syllables in spoken language.
## Grammar.
### Morphology.
ASL has a rich system of verbal inflection, which involves both grammatical aspect: how the action of verbs flows in time—and agreement marking. Aspect can be marked by changing the manner of movement of the verb; for example, continuous aspect is marked by incorporating rhythmic, circular movement, while punctual aspect is achieved by modifying the sign so that it has a stationary hand position. Verbs may agree with both the subject and the object, and are marked for number and reciprocity. Reciprocity is indicated by using two one-handed signs; for example, the sign SHOOT, made with an L-shaped handshape with inward movement of the thumb, inflects to SHOOT[reciprocal], articulated by having two L-shaped hands "shooting" at each other.
ASL has a productive system of classifiers, which are used to classify objects and their movement in space. For example, a rabbit running downhill would use a classifier consisting of a bent V classifier handshape with a downhill-directed path; if the rabbit is hopping, the path is executed with a bouncy manner. In general, classifiers are composed of a "classifier handshape" bound to a "movement root". The classifier handshape represents the object as a whole, incorporating such attributes as surface, depth, and shape, and is usually very iconic. The movement root consists of a path, a direction and a manner.
#### Fingerspelling.
ASL possesses a set of 26 signs known as the American manual alphabet, which can be used to spell out words from the English language. Such signs make use of the 19 handshapes of ASL. For example, the signs for 'p' and 'k' use the same handshape but different orientations. A common misconception is that ASL consists only of fingerspelling; although such a method (Rochester Method) has been used, it is not ASL.
Fingerspelling is a form of borrowing, a linguistic process wherein words from one language are incorporated into another. In ASL, fingerspelling is used for proper nouns and for technical terms with no native ASL equivalent. There are also some other loan words which are fingerspelled, either very short English words or abbreviations of longer English words, e.g. "O-N" from English 'on', and "A-P-T" from English 'apartment'. Fingerspelling may also be used to emphasize a word that would normally be signed otherwise.
### Syntax.
ASL is a subject–verb–object (SVO) language, but various phenomena affect that basic word order. Basic SVO sentences are signed without any pauses:
However, other word orders may also occur since ASL allows the topic of a sentence to be moved to sentence-initial position, a phenomenon known as topicalization. In object–subject–verb (OSV) sentences, the object is topicalized, marked by a forward head-tilt and a pause:
Besides, word orders can be obtained through the phenomenon of subject copy in which the subject is repeated at the end of the sentence, accompanied by head nodding for clarification or emphasis:
ASL also allows null subject sentences whose subject is implied, rather than stated explicitly. Subjects can be copied even in a null subject sentence, and the subject is then omitted from its original position, yielding a verb–object–subject (VOS) construction:
Topicalization, accompanied with a null subject and a subject copy, can produce yet another word order, object–verb–subject (OVS).
Those properties of ASL allow it a variety of word orders, leading many to question which is the true, underlying, "basic" order. There are several other proposals that attempt to account for the flexibility of word order in ASL. One proposal is that languages like ASL are best described with a topic–comment structure whose words are ordered by their importance in the sentence, rather than by their syntactic properties. Another hypothesis is that ASL exhibits free word order, in which syntax is not encoded in word order but can be encoded by other means such as head nods, eyebrow movement, and body position.
## Iconicity.
Common misconceptions are that signs are iconically self-explanatory, that they are a transparent imitation of what they mean, or even that they are pantomime. In fact, many signs bear no resemblance to their referent because they were originally arbitrary symbols, or their iconicity has been obscured over time. Even so, in ASL iconicity plays a significant role; a high percentage of signs resemble their referents in some way. That may be because the medium of sign, three-dimensional space, naturally allows more iconicity than oral language.
In the era of the influential linguist Ferdinand de Saussure, it was assumed that the mapping between form and meaning in language must be completely arbitrary. Although onomatopoeia is a clear exception, since words like 'choo-choo' bear clear resemblance to the sounds that they mimic, the Saussurean approach was to treat them as marginal exceptions. ASL, with its significant inventory of iconic signs, directly challenges that theory.
Research on acquisition of pronouns in ASL has shown that children do not always take advantage of the iconic properties of signs when they interpret their meaning. It has been found that when children acquire the pronoun "you", the iconicity of the point (at the child) is often confused, being treated more like a name. That is a similar finding to research in oral languages on pronoun acquisition. It has also been found that iconicity of signs does not affect immediate memory and recall; less iconic signs are remembered just as well as highly-iconic signs.

</doc>
<doc id="1202" url="https://en.wikipedia.org/wiki?curid=1202" title="Applet">
Applet

In computing, an applet is any small application that performs one specific task that runs within the scope of a dedicated widget engine or a larger program, often as a plug-in. The term is frequently used to refer to a Java applet, a program written in the Java programming language that is designed to be placed on a web page. Applets are typical examples of transient and auxiliary applications that don't monopolize the user's attention. Applets are not full-featured application programs, and are intended to be easily accessible.
## History.
The word "applet" was first used in 1990 in PC Magazine. However, the concept of an applet, or more broadly a small interpreted program downloaded and executed by the user, dates at least to RFC 5 (1969) by Jeff Rulifson, which described the Decode-Encode Language (DEL), which was designed to allow remote use of the oN-Line System (NLS) over ARPANET, by downloading small programs to enhance the interaction. This has been specifically credited as a forerunner of Java's downloadable programs in RFC 2555.
## Applet as an extension of other software.
In some cases, an applet does not run independently. These applets must run either in a container provided by a host program, through a plugin, or a variety of other applications including mobile devices that support the applet programming model.
### Web-based Applets.
Applets were used to provide interactive features to web applications that historically could not be provided by HTML alone. They could capture mouse input and also had controls like buttons or check boxes. In response to the user action an applet could change the provided graphic content. This made applets well suitable for demonstration, visualization, and teaching. There were online applet collections for studying various subjects, from physics to heart physiology. Applets were also used to create online game collections that allowed players to compete against live opponents in real-time.
An applet could also be a text area only, providing, for instance, a cross platform command-line interface to some remote system. If needed, an applet could leave the dedicated area and run as a separate window. However, applets had very little control over web page content outside the applet dedicated area, so they were less useful for improving the site appearance in general (while applets like news tickers or WYSIWYG editors are also known). Applets could also play media in formats that are not natively supported by the browser.
HTML pages could embed parameters that were passed to the applet. Hence the same applet could appear differently depending on the parameters that were passed.
Examples of Web-based Applets include:
### Applet Vs. Subroutine.
A larger application distinguishes its applets through several features:
## Java applets.
A Java applet is a Java program that is launched from HTML and run in a web browser.It takes code from server and run in a web browser.It can provide web applications with interactive features that cannot be provided by HTML. Since Java's bytecode is platform-independent, Java applets can be executed by browsers running under many platforms, including Windows, Unix, macOS, and Linux. When a Java technology-enabled web browser processes a page that contains an applet, the applet's code is transferred to the client's system and executed by the browser's Java Virtual Machine (JVM). An HTML page references an applet either via the deprecated tag or via its replacement, the tag.
## Security.
Recent developments in the coding of applications including mobile and embedded systems have led to the awareness of the security of applets.
### Open platform applets.
Applets in an open platform environment should provide secure interactions between different applications. A compositional approach can be used to provide security for open platform applets. Advanced compositional verification methods have been developed for secure applet interactions.
### Java applets.
A Java applet contains different security models: unsigned Java applet security, signed Java applet security, and self signed Java applet security.
### Web-based applets.
In an applet-enabled web browser, many methods can be used to provide applet security for malicious applets. A malicious applet can infect a computer system in many ways, including denial of service, invasion of privacy, and annoyance. A typical solution for malicious applets is to make the web browser to monitor applets' activities. This will result in a web browser that will enable the manual or automatic stopping of malicious applets.

</doc>
<doc id="1203" url="https://en.wikipedia.org/wiki?curid=1203" title="Alternate history">
Alternate history

Alternate history (also alternative history, althist, AH) is a genre of speculative fiction of stories in which one or more historical events occur and are resolved differently than in real life. As conjecture based upon historical fact, alternative history stories propose "What if?" scenarios about crucial events in human history, and present outcomes very different from the historical record. Alternate history also is a subgenre of literary fiction, science fiction, and historical fiction; as literature, alternate history uses the tropes of the genre to answer the "What if?" speculations of the story.
Since the 1950s, as a subgenre of science fiction, alternative history stories feature the tropes of time travel between histories, and the psychic awareness of the existence of an alternative universe, by the inhabitants of a given universe; and time travel that divides history into various timestreams. In the Spanish, French, German, and Portuguese, Italian, Catalan, and Galician languages, the terms "Uchronie", "ucronia", "ucronía", and "Uchronie" identify the alternate history genre, from which derives the English term "Uchronia", composed of the Greek prefix ("not", "not any", and "no") and the Greek word () "time", to describe a story that occurs "[in] no time"; analogous to a story that occurs in "utopia", "[in] no place". The term "Uchronia" also is the name of the list of alternate-history books, "". Moreover, "Allohistory" (other history) is another term for the genre of alternative history.
## Definition.
Alternative history is a genre of fiction wherein the author speculates upon how the course of history might have been altered if a particular historical event had an outcome different from the real life outcome. An alternate history requires three conditions: (i) A point of divergence from the historical record, before the time in which the author is writing; (ii) A change that would alter known history; and (iii) An examination of the ramifications of that alteration to history. Occasionally, some types of genre fiction are misidentified as "alternative history", specifically science fiction stories set in a time that was the future for the writer, but now is the past for the reader, such as the novels "" (1968), by Arthur C. Clarke and "Nineteen Eighty-Four" (1949), by George Orwell, because the authors did not alter the history of the past when they wrote the stories.
Moreover, the genre of the Secret History of an event, which can be either fictional or non-fictional, documents events that might have occurred in history, but which had no effect upon the recorded historical outcome. Alternative history also is thematically related to, but distinct from, Counterfactual History, which is a form of historiography that attempts to answer the "What if?" speculations that arise from counterfactual conditions in order to understand what did happen. As a method of historical research, counterfactual history explores historical events with an extrapolated timeline in which key historical events either did not occur or had an outcome different from the historical record.
## History of literature.
### Antiquity and medieval.
The earliest example of alternate (or counterfactual) history is found in Livy's "Ab Urbe Condita Libri" (book IX, sections 17–19). Livy contemplated an alternative 4th century BC in which Alexander the Great had survived to attack Europe as he had planned; asking, "What would have been the results for Rome if she had been engaged in a war with Alexander?" Livy concluded that the Romans would likely have defeated Alexander. An even earlier possibility is Herodotus's "Histories", which contains speculative material.
Another example of counterfactual history was posited by cardinal and Doctor of the Church Peter Damian in the 11th century. In his famous work "De Divina Omnipotentia", a long letter in which he discusses God's omnipotence, he treats questions related to the limits of divine power, including the question of whether God can change the past, for example, bringing about that Rome was never founded:I see I must respond finally to what many people, on the basis of your holiness’s [own] judgment, raise as an objection on the topic of this dispute. For they say: If, as you assert, God is omnipotent in all things, can he manage this, that things that have been made were not made? He can certainly destroy all things that have been made, so that they do not exist now. But it cannot be seen how he can bring it about that things that have been made were not made. To be sure, it can come about that from now on and hereafter Rome does not exist; for it can be destroyed. But no opinion can grasp how it can come about that it was not founded long ago...One early work of fiction detailing an alternate history is Joanot Martorell's 1490 epic romance "Tirant lo Blanch", which was written when the loss of Constantinople to the Turks was still a recent and traumatic memory for Christian Europe. It tells the story of the knight Tirant the White from Brittany who travels to the embattled remnants of the Byzantine Empire. He becomes a Megaduke and commander of its armies and manages to fight off the invading Ottoman armies of . He saves the city from Islamic conquest, and even chases the Turks deeper into lands they had previously conquered.
### 19th century.
One of the earliest works of alternate history published in large quantities for the reception of a large audience may be Louis Geoffroy's "Histoire de la Monarchie universelle: Napoléon et la conquête du monde (1812–1832)" (History of the Universal Monarchy: Napoleon and the Conquest of the World) (1836), which imagines Napoleon's First French Empire emerging victorious in the French invasion of Russia in 1811 and in an invasion of England in 1814, later unifying the world under Bonaparte's rule.
In the English language, the first known complete alternate history is Nathaniel Hawthorne's short story "P.'s Correspondence", published in 1845. It recounts the tale of a man who is considered "a madman" due to his perceptions of a different 1845, a reality in which long-dead famous people, such as the poets Robert Burns, Lord Byron, Percy Bysshe Shelley and John Keats, the actor Edmund Kean, the British politician George Canning, and Napoleon Bonaparte, are still alive.
The first novel-length alternate history in English would seem to be Castello Holford's "Aristopia" (1895). While not as nationalistic as Louis Geoffroy's "Napoléon et la conquête du monde, 1812–1823", "Aristopia" is another attempt to portray a Utopian society. In "Aristopia", the earliest settlers in Virginia discover a reef made of solid gold and are able to build a Utopian society in North America.
### Early 20th century and the era of the pulps.
In 1905, H. G. Wells published "A Modern Utopia". As explicitly noted in the book itself, Wells's main aim in writing it was to set out his social and political ideas, the plot serving mainly as a vehicle to expound them. This book introduced the idea of a person being transported from a point in our familiar world to the precise geographical equivalent point in an alternate world in which history had gone differently. The protagonists undergo various adventures in the alternate world, and then are finally transported back to our world, again to the precise geographical equivalent point. Since then, that has become a staple of the alternate history genre. 
A number of alternate history stories and novels appeared in the late 19th and early 20th centuries (see, for example, Joseph Edgar Chamberlin's "The Ifs of History" [1907] and Charles Petrie's "If: A Jacobite Fantasy" [1926]). In 1931, British historian Sir John Squire collected a series of essays from some of the leading historians of the period for his anthology "If It Had Happened Otherwise". In that work, scholars from major universities, as well as important non-academic authors, turned their attention to such questions as "If the Moors in Spain Had Won" and "If Louis XVI Had Had an Atom of Firmness". The essays range from serious scholarly efforts to Hendrik Willem van Loon's fanciful and satiric portrayal of an independent 20th-century New Amsterdam, a Dutch city-state on the island of Manhattan. Among the authors included were Hilaire Belloc, André Maurois, and Winston Churchill.
One of the entries in Squire's volume was Churchill's "If Lee Had Not Won the Battle of Gettysburg", written from the viewpoint of a historian in a world in which the Confederacy had won the American Civil War. The entry considers what would have happened if the North had been victorious (in other words, a character from an alternate world imagines a world more like the real one we live in, although it is not identical in every detail). Speculative work that narrates from the point of view of an alternate history is variously known as "recursive alternate history", a "double-blind what-if", or an "alternate-alternate history". Churchill's essay was one of the influences behind Ward Moore's alternate history novel "Bring the Jubilee" in which General Robert E. Lee won the Battle of Gettysburg and paved the way for the eventual victory of the Confederacy in the American Civil War (named the "War of Southron Independence" in this timeline). The protagonist, the autodidact Hodgins Backmaker, travels back to the aforementioned battle and inadvertently changes history, which results in the emergence of our own timeline and the consequent victory of the Union instead.
The American humorist author James Thurber parodied alternate history stories about the American Civil War in his 1930 story "If Grant Had Been Drinking at Appomattox", which he accompanied with this very brief introduction: ""Scribner's" magazine is publishing a series of three articles: 'If Booth Had Missed Lincoln', 'If Lee Had Won the Battle of Gettysburg', and 'If Napoleon Had Escaped to America'. This is the fourth".
Another example of alternate history from this period (and arguably the first that explicitly posited cross-time travel from one universe to another as anything more than a visionary experience) is H.G. Wells' "Men Like Gods" (1923) in which the London-based journalist Mr. Barnstable, along with two cars and their passengers, is mysteriously teleported into "another world", which the "Earthlings" call Utopia. Being far more advanced than Earth, Utopia is some 3000 years ahead of humanity in its development. Wells describes a multiverse of alternative worlds, complete with the paratime travel machines that would later become popular with American pulp writers. However, since his hero experiences only a single alternate world, the story is not very different from conventional alternate history.
In the 1930s, alternate history moved into a new arena. The December 1933 issue of "Astounding" published Nat Schachner's "Ancestral Voices", which was quickly followed by Murray Leinster's "Sidewise in Time". While earlier alternate histories examined reasonably-straightforward divergences, Leinster attempted something completely different. In his "World gone mad", pieces of Earth traded places with their analogs from different timelines. The story follows Professor Minott and his students from a fictitious Robinson College as they wander through analogues of worlds that followed a different history.
A somewhat similar approach was taken by Robert A. Heinlein in his 1941 novelette "Elsewhen" in which a professor trains his mind to move his body across timelines. He then hypnotizes his students so that they can explore more of them. Eventually, each settles into the reality that is most suitable for him or her. Some of the worlds they visit are mundane, some are very odd, and others follow science fiction or fantasy conventions.
World War II produced alternate history for propaganda: both British and American authors wrote works depicting Nazi invasions of their respective countries as cautionary tales.
#### Time travel to create historical divergences.
The period around World War II also saw the publication of the time travel novel "Lest Darkness Fall" by L. Sprague de Camp in which an American academic travels to Italy at the time of the Byzantine invasion of the Ostrogoths. De Camp's time traveler, Martin Padway, is depicted as making permanent historical changes and implicitly forming a new time branch, thereby making the work an alternate history.
In William Tenn's short story "Brooklyn Project" (1948), a tyrannical US Government brushes aside the warnings of scientists about the dangers of time travel and goes on with a planned experiment - with the result that minor changes to the prehistoric past cause Humanity to never have existed, its place taken by tentacled underwater intelligent creatures - who also have a tyrannical government which also insists on experimenting with time-travel.
Time travel as the cause of a point of divergence (POD), which can denote either the bifurcation of a historical timeline or a simple replacement of the future that existed before the time-travelling event, has continued to be a popular theme. In Ward Moore's "Bring the Jubilee", the protagonist lives in an alternate history in which the Confederacy has won the American Civil War. He travels backward through time and brings about a Union victory at the Battle of Gettysburg.
When a story's assumptions about the nature of time travel lead to the complete replacement of the visited time's future, rather than just the creation of an additional time line, the device of a "time patrol" is often used where guardians move through time to preserve the "correct" history.
A more recent example is "Making History" by Stephen Fry in which a time machine is used to alter history so that Adolf Hitler was never born. That ironically results in a more competent leader of Nazi Germany and results in the country's ascendancy and longevity in the altered timeline.
### Cross-time stories.
H.G. Wells' "cross-time" or "many universes" variant (see above) was fully developed by Murray Leinster in his 1934 short story "Sidewise in Time", in which sections of the Earth's surface begin changing places with their counterparts in alternate timelines.
Fredric Brown employed this subgenre to satirize the science fiction pulps and their adolescent readers—and fears of foreign invasion—in the classic "What Mad Universe" (1949). In Clifford D. Simak's "Ring Around the Sun" (1953), the hero ends up in an alternate earth of thick forests in which humanity never developed but a band of mutants is establishing a colony; the story line appears to frame the author's anxieties regarding McCarthyism and the Cold War.
#### Quantum theory of many worlds.
While many justifications for alternate histories involve a multiverse, the "many world" theory would naturally involve many worlds, in fact a continually exploding array of universes. In quantum theory, new worlds would proliferate with every quantum event, and even if the writer uses human decisions, every decision that could be made differently would result in a different timeline. A writer's fictional multiverse may, in fact, preclude some decisions as humanly impossible, as when, in "Night Watch", Terry Pratchett depicts a character informing Vimes that while anything that can happen, has happened, nevertheless there is no history whatsoever in which Vimes has ever murdered his wife. When the writer explicitly maintains that "all" possible decisions are made in all possible ways, one possible conclusion is that the characters were neither brave, nor clever, nor skilled, but simply lucky enough to happen on the universe in which they did not choose the cowardly route, take the stupid action, fumble the crucial activity, etc.; few writers focus on this idea, although it has been explored in stories such as Larry Niven's story "All the Myriad Ways", where the reality of all possible universes leads to an epidemic of suicide and crime because people conclude their choices have no moral import.
In any case, even if it is true that every possible outcome occurs in some world, it can still be argued that traits such as bravery and intelligence might still affect the relative frequency of worlds in which better or worse outcomes occurred (even if the total number of worlds with each type of outcome is infinite, it is still possible to assign a different measure to different infinite sets). The physicist David Deutsch, a strong advocate of the many-worlds interpretation of quantum mechanics, has argued along these lines, saying that "By making good choices, doing the right thing, we thicken the stack of universes in which versions of us live reasonable lives. When you succeed, all the copies of you who made the same decision succeed too. What you do for the better increases the portion of the multiverse where good things happen." This view is perhaps somewhat too abstract to be explored directly in science fiction stories, but a few writers have tried, such as Greg Egan in his short story "The Infinite Assassin", where an agent is trying to contain reality-scrambling "whirlpools" that form around users of a certain drug, and the agent is constantly trying to maximize the consistency of behavior among his alternate selves, attempting to compensate for events and thoughts he experiences, he guesses are of low measure relative to those experienced by most of his other selves.
Many writers—perhaps the majority—avoid the discussion entirely. In one novel of this type, H. Beam Piper's "Lord Kalvan of Otherwhen", a Pennsylvania State Police officer, who knows how to make gunpowder, is transported from our world to an alternate universe where the recipe for gunpowder is a tightly held secret and saves a country that is about to be conquered by its neighbors. The paratime patrol members are warned against going into the timelines immediately surrounding it, where the country "will" be overrun, but the book never depicts the slaughter of the innocent thus entailed, remaining solely in the timeline where the country is saved.
The cross-time theme was further developed in the 1960s by Keith Laumer in the first three volumes of his "Imperium" sequence, which would be completed in "Zone Yellow" (1990). Piper's politically more sophisticated variant was adopted and adapted by Michael Kurland and Jack Chalker in the 1980s; Chalker's "G.O.D. Inc" trilogy (1987–89), featuring paratime detectives Sam and Brandy Horowitz, marks the first attempt at merging the paratime thriller with the police procedural. Kurland's "Perchance" (1988), the first volume of the never-completed "Chronicles of Elsewhen", presents a multiverse of secretive cross-time societies that utilize a variety of means for cross-time travel, ranging from high-tech capsules to mutant powers. Harry Turtledove has launched the Crosstime Traffic series for teenagers featuring a variant of H. Beam Piper's paratime trading empire.
#### Rival paratime worlds.
The concept of a cross-time version of a world war, involving rival paratime empires, was developed in Fritz Leiber's Change War series, starting with the Hugo Award winning "The Big Time" (1958); followed by Richard C. Meredith's "Timeliner" trilogy in the 1970s, Michael McCollum's "A Greater Infinity" (1982) and John Barnes' "Timeline Wars" trilogy in the 1990s.
Such "paratime" stories may include speculation that the laws of nature can vary from one universe to the next, providing a science fictional explanation—or veneer—for what is normally fantasy. Aaron Allston's "Doc Sidhe" and "Sidhe Devil" take place between our world, the "grim world" and an alternate "fair world" where the Sidhe retreated to. Although technology is clearly present in both worlds, and the "fair world" parallels our history, about fifty years out of step, there is functional magic in the fair world. Even with such explanation, the more explicitly the alternate world resembles a normal fantasy world, the more likely the story is to be labelled fantasy, as in Poul Anderson's "House Rule" and "Loser's Night". In both science fiction and fantasy, whether a given parallel universe is an alternate history may not be clear. The writer might allude to a POD only to explain the existence and make no use of the concept, or may present the universe without explanation of its existence.
### Major writers explore alternate histories.
Isaac Asimov's short story "What If—" (1952) is about a couple who can explore alternate realities by means of a television-like device. This idea can also be found in Asimov's novel "The End of Eternity" (1955), in which the "Eternals" can change the realities of the world, without people being aware of it. Poul Anderson's "Time Patrol" stories feature conflicts between forces intent on changing history and the Patrol who work to preserve it. One story, Delenda Est, describes a world in which Carthage triumphed over the Roman Republic. "The Big Time", by Fritz Leiber, describes a Change War ranging across all of history.
Keith Laumer's "Worlds of the Imperium" is one of the earliest alternate history novels; it was published by "Fantastic Stories of the Imagination" in 1961, in magazine form, and reprinted by Ace Books in 1962 as one half of an Ace Double. Besides our world, Laumer describes a world ruled by an Imperial aristocracy formed by the merger of European empires, in which the American Revolution never happened, and a third world in post-war chaos ruled by the protagonist's doppelganger.
Philip K. Dick's novel, "The Man in the High Castle" (1962), is an alternate history in which Nazi Germany and Imperial Japan won World War II. This book contains an example of "alternate-alternate" history, in that one of its characters authored a book depicting a reality in which the Allies won the war, itself divergent from real-world history in several aspects. The several characters live within a divided United States, in which the Empire of Japan takes the Pacific states, governing them as a puppet, Nazi Germany takes the East Coast of the United States and parts of the Midwest, with the remnants of the old United States' government as the Neutral Zone, a buffer state between the two superpowers. The book has inspired an Amazon series of the same name.
Vladimir Nabokov's novel, "" (1969), is a story of incest that takes place within an alternate North America settled in part by Czarist Russia and that borrows from Dick's idea of "alternate-alternate" history (the world of Nabokov's hero is wracked by rumors of a "counter-earth" that apparently is ours). Some critics believe that the references to a counter-earth suggest that the world portrayed in "Ada" is a delusion in the mind of the hero (another favorite theme of Dick's novels). Strikingly, the characters in "Ada" seem to acknowledge their own world as the copy or negative version, calling it "Anti-Terra", while its mythical twin is the real "Terra". Like history, science has followed a divergent path on Anti-Terra: it boasts all the same technology as our world, but all based on water instead of electricity; e.g., when a character in "Ada" makes a long-distance call, all the toilets in the house flush at once to provide hydraulic power.
Guido Morselli described the defeat of Italy (and subsequently France) in World War I in his novel, "Past Conditional" (1975; ), wherein the static Alpine front line which divided Italy from Austria during that war collapses when the Germans and the Austrians forsake trench warfare and adopt blitzkrieg twenty years in advance.
Kingsley Amis set his novel, "The Alteration" (1976), in the 20th century, but major events in the Reformation did not take place, and Protestantism is limited to the breakaway Republic of New England. Martin Luther was reconciled to the Roman Catholic Church and later became Pope Germanian I.
In Nick Hancock and Chris England's 1997 book "What Didn't Happen Next: An Alternative History of Football" it is suggested that, had Gordon Banks been fit to play in the 1970 FIFA World Cup quarter-final, there would have been no Thatcherism and the post-war consensus would have continued indefinitely.
Kim Stanley Robinson's novel, "The Years of Rice and Salt" (2002), starts at the point of divergence with Timur turning his army away from Europe, and the Black Death has killed 99% of Europe's population, instead of only a third. Robinson explores world history from that point in AD 1405 (807 AH) to about AD 2045 (1467 AH). Rather than following the great man theory of history, focusing on leaders, wars, and major events, Robinson writes more about social history, similar to the Annales School of history theory and Marxist historiography, focusing on the lives of ordinary people living in their time and place.
Philip Roth's novel, "The Plot Against America" (2004), looks at an America where Franklin D. Roosevelt is defeated in 1940 in his bid for a third term as President of the United States, and Charles Lindbergh is elected, leading to a US that features increasing fascism and anti-Semitism.
Michael Chabon, occasionally an author of speculative fiction, contributed to the genre with his novel "The Yiddish Policemen's Union" (2007), which explores a world in which the State of Israel was destroyed in its infancy and many of the world's Jews instead live in a small strip of Alaska set aside by the US government for Jewish settlement. The story follows a Jewish detective solving a murder case in the Yiddish-speaking semi-autonomous city state of Sitka. Stylistically, Chabon borrows heavily from the noir and detective fiction genres, while exploring social issues related to Jewish history and culture. Apart from the alternate history of the Jews and Israel, Chabon also plays with other common tropes of alternate history fiction; in the book, Germany actually loses the war even "harder" than they did in reality, getting hit with a nuclear bomb instead of just simply losing a ground war (subverting the common "what if Germany won WWII?" trope).
### Contemporary alternate history in popular literature.
The late 1980s and the 1990s saw a boom in popular-fiction versions of alternate history, fueled by the emergence of the prolific alternate history author Harry Turtledove, as well as the development of the steampunk genre and two series of anthologies—the "What Might Have Been" series edited by Gregory Benford and the "Alternate ..." series edited by Mike Resnick. This period also saw alternate history works by S. M. Stirling, Kim Stanley Robinson, Harry Harrison, Howard Waldrop, Peter Tieryas, and others.
In 1986, a sixteen-part epic comic book series called "Captain Confederacy" began examining a world where the Confederate States of America won the American Civil War. In the series, the Captain and others heroes are staged government propaganda events featuring the feats of these superheroes.
Since the late 1990s, Harry Turtledove has been the most prolific practitioner of alternate history and has been given the title "Master of Alternate History" by some. His books include those of Timeline 191 (a.k.a. Southern Victory, also known as TL-191), in which, while the Confederate States of America won the American Civil War, the Union and Imperial Germany defeat the Entente Powers in the two "Great War"s of the 1910s and 1940s (with a Nazi-esque Confederate government attempting to exterminate its Black population), and the Worldwar series, in which aliens invaded Earth during World War II. Other stories by Turtledove include "A Different Flesh", in which America was not colonized from Asia during the last ice age; "In the Presence of Mine Enemies", in which the Nazis won World War II; and "Ruled Britannia", in which the Spanish Armada succeeded in conquering England in the Elizabethan era, with William Shakespeare being given the task of writing the play that will motivate the Britons to rise up against their Spanish conquerors. He also co-authored a book with actor Richard Dreyfuss, "The Two Georges", in which the United Kingdom retained the American colonies, with George Washington and King George III making peace. He did a two-volume series in which the Japanese not only bombed Pearl Harbor but also invaded and occupied the Hawaiian Islands.
Perhaps the most incessantly explored theme in popular alternate history focuses on worlds in which the Nazis won World War Two. In some versions, the Nazis and/or Axis Powers conquer the entire world; in others, they conquer most of the world but a "Fortress America" exists under siege; while in others, there is a Nazi/Japanese Cold War comparable to the US/Soviet equivalent in 'our' timeline. "Fatherland" (1992), by Robert Harris, is set in Europe following the Nazi victory. The novel "Dominion" by C.J. Sansom (2012) is similar in concept but is set in England, with Churchill the leader of an anti-German Resistance and other historic persons in various fictional roles. In the Mecha Samurai Empire series (2016), Peter Tieryas focuses on the Asian-American side of the alternate history, exploring an America ruled by the Japanese Empire while integrating elements of Asian pop culture like mechas and videogames.
Several writers have posited points of departure for such a world but then have injected time splitters from the future or paratime travel, for instance James P. Hogan's "The Proteus Operation". Norman Spinrad wrote "The Iron Dream" in 1972, which is intended to be a science fiction novel written by Adolf Hitler after fleeing from Europe to North America in the 1920s.
In Jo Walton's "Small Change" series, the United Kingdom made peace with Hitler before the involvement of the United States in World War II, and slowly collapses due to severe economic depression. Former House Speaker Newt Gingrich and William R. Forstchen have written a novel, "1945", in which the US defeated Japan but not Germany in World War II, resulting in a Cold War with Germany rather than the Soviet Union. Gingrich and Forstchen neglected to write the promised sequel; instead, they wrote a trilogy about the American Civil War, starting with "", in which the Confederates win a victory at the Battle of Gettysburg - however, after Lincoln responds by bringing Grant and his forces to the eastern theater, the Army of Northern Virginia is soon trapped and destroyed in Maryland, and the war ends within weeks. Also from that general era, Martin Cruz Smith, in his first novel, posited an independent American Indian nation following the defeat of Custer in "The Indians Won" (1970).
Beginning with "The Probability Broach" in 1980, L. Neil Smith wrote several novels that postulated the disintegration of the US Federal Government after Albert Gallatin joins the Whiskey Rebellion in 1794 and eventually leads to the creation of a libertarian utopia.
A recent time traveling splitter variant involves entire communities being shifted elsewhere to become the unwitting creators of new time branches. These communities are transported from the present (or the near-future) to the past or to another time-line via a natural disaster, the action of technologically advanced aliens, or a human experiment gone wrong. S. M. Stirling wrote the "Island in the Sea of Time" trilogy, in which Nantucket Island and all its modern inhabitants are transported to Bronze Age times to become the world's first superpower. In Eric Flint's 1632 series, a small town in West Virginia is transported to 17th century central Europe and drastically changes the course of the Thirty Years' War, which was then underway. John Birmingham's "Axis of Time" trilogy deals with the culture shock when a United Nations naval task force from 2021 finds itself back in 1942 helping the Allies against the Empire of Japan and the Germans (and doing almost as much harm as good in spite of its advanced weapons). Similarly, Robert Charles Wilson's "Mysterium" depicts a failed US government experiment which transports a small American town into an alternative version of the US run by believers in a form of Christianity known as Gnosticism, who are engaged in a bitter war with the "Spanish" in Mexico (the chief scientist at the laboratory where the experiment occurred is described as a Gnostic, and references to Christian Gnosticism appear repeatedly in the book). In "Time for Patriots" by retired astronomer Thomas Wm. Hamilton (4897 Tomhamilton) a town and military academy on Long Island are transported back to 1770, where they shorten the American Revolution, rewrite the Constitution, prolong Mozart's life, battle Barbary pirates, and have other adventures.
Although not dealing in physical time travel, in his alt-history novel "Marx Returns", Jason Barker introduces anachronisms into the life and times of Karl Marx, such as when his wife Jenny sings a verse from the Sex Pistols's song "Anarchy in the U.K.", or in the games of chess she plays with the Marxes' housekeeper Helene Demuth, which on one occasion involves a Caro–Kann Defence. In her review of the novel, Nina Power writes of "Jenny’s 'utopian' desire for an end to time", an attitude which, according to Power, is inspired by her husband's co-authored book "The German Ideology". However, in keeping with the novel's anachronisms, the latter was not published until 1932. By contrast, the novel's timeline ends in 1871.
### In fantasy genre.
Many works of straight fantasy and science fantasy take place in historical settings, though with the addition of, for example, magic or mythological beasts. Some present a secret history in which the modern day world no longer believes that these elements ever existed. Many ambiguous alternate/secret histories are set in Renaissance or pre-Renaissance times, and may explicitly include a "retreat" from the world, which would explain the current absence of such phenomena. Other stories make plan a divergence of some kind. 
In Poul Anderson's "Three Hearts and Three Lions" in which the Matter of France is history and the fairy folk are real and powerful. The same author's "A Midsummer Tempest", occurs in a world in which the plays of William Shakespeare (called here "the Great Historian"), presented the literal truth in every instance. The novel itself takes place in the era of Oliver Cromwell and Charles I. Here, the English Civil War had a different outcome, and the Industrial Revolution has occurred early. 
Randall Garrett's "Lord Darcy" series presents a point of divergence: a monk systemizes magic rather than science, so the use of foxglove to treat heart disease is regarded as superstition. Another point of divergence occurs in 1199, when Richard the Lionheart survives the Siege of Chaluz and returns to England and makes the Angevin Empire so strong that it survives into the 20th century.
"Jonathan Strange &amp; Mr Norrell" by Susanna Clarke takes place in an England where a separate Kingdom ruled by the Raven King and founded on magic existed in Northumbria for over 300 years. In Patricia Wrede's Regency fantasies, Great Britain has a Royal Society of Wizards.
"The Tales of Alvin Maker" series by Orson Scott Card (a parallel to the life of Joseph Smith, founder of the Latter Day Saint movement) takes place in an alternate America, beginning in the early 19th century. Prior to that time, a POD occurred: England, under the rule of Oliver Cromwell, had banished "makers", or anyone else demonstrating "knacks" (an ability to perform seemingly supernatural feats) to the North American continent. Thus the early American colonists embraced as perfectly ordinary these gifts, and counted on them as a part of their daily lives. The political division of the continent is considerably altered, with two large English colonies bookending a smaller "American" nation, one aligned with England, and the other governed by exiled Cavaliers. Actual historical figures are seen in a much different light: Ben Franklin is revered as the continent's finest "maker", George Washington was executed after being captured, and "Tom" Jefferson is the first president of "Appalachia", the result of a compromise between the Continentals and the British Crown.
On the other hand, when the "Old Ones" (fairies) still manifest themselves in England in Keith Roberts's "Pavane", which takes place in a technologically backward world after a Spanish assassination of Elizabeth I allowed the Spanish Armada to conquer England, the possibility that the fairies were real but retreated from modern advances makes the POD possible: the fairies really were present all along, in a secret history. 
Again, in the English Renaissance fantasy "Armor of Light" by Melissa Scott and Lisa A. Barnett, the magic used in the book, by Dr. John Dee and others, actually was practiced in the Renaissance; positing a secret history of effective magic makes this an alternate history with a point of departure. Sir Philip Sidney survives the Battle of Zutphen in 1586, and shortly thereafter saving the life of Christopher Marlowe.
When the magical version of our world's history is set in contemporary times, the distinction becomes clear between alternate history on the one hand and contemporary fantasy, using in effect a form of secret history (as when Josepha Sherman's "Son of Darkness" has an elf living in New York City, in disguise) on the other. In works such as Robert A. Heinlein's "Magic, Incorporated" where a construction company can use magic to rig up stands at a sporting event and Poul Anderson's "Operation Chaos" and its sequel "Operation Luna", where djinns are serious weapons of war—with atomic bombs—the use of magic throughout the United States and other modern countries makes it clear that this is not secret history—although references in "Operation Chaos" to degaussing the effects of cold iron make it possible that it is the result of a POD. The sequel clarifies this as the result of a collaboration of Einstein and Planck in 1901, resulting in the theory of "rhea tics". Henry Moseley applies this theory to "degauss the effects of cold iron and release the goetic forces." This results in the suppression of ferromagnetism and the re-emergence of magic and magical creatures.
Alternate history shades off into other fantasy subgenres when the use of actual, though altered, history and geography decreases, although a culture may still be clearly the original source; Barry Hughart's "Bridge of Birds" and its sequels take place in a fantasy world, albeit one clearly based on China, and with allusions to actual Chinese history, such as the Empress Wu. Richard Garfinkle's "Celestial Matters" incorporates ancient Chinese physics and Greek Aristotelian physics, using them as if factual.
Alternate history has long been a staple of Japanese speculative fiction with such authors as Futaro Yamada and Ryō Hanmura writing novels set in recognizable historical settings withaddded supernatural or science fiction elements. Ryō Hanmura's 1973 "Musubi no Yama Hiroku" which recreated 400 years of Japan's history from the perspective of a secret magical family with psychic abilities. The novel has since come to be recognized as a masterpiece of Japanese speculative fiction. Twelve years later, author Hiroshi Aramata wrote the groundbreaking "Teito Monogatari" which reimagined the history of Tokyo across the 20th century in a world heavily influenced by the supernatural.
### Television.
The TV show "Sliders" explores different possible alternate realities by having the protagonist "slide" into different parallel dimensions of the same planet Earth. Another TV show " explores a female-dominated world in which witchcraft is real. Its world diverged from our timeline when the Salem witch trials are resolved by an agreement between witches and ungifted humans.
The anime " featured an alternate 18th century.
The TV show "The Man in the High Castle" is an adaptation of the novel with the same name that ran for four seasons.
### Video games.
For the same reasons that this genre is explored by role-playing games, alternate history is also an intriguing backdrop for the storylines of many video games. A famous example of an alternate history game is "". Released in 1996, the game presents a point of divergence in 1946 in which Albert Einstein goes back in time to prevent World War II from ever taking place by erasing Adolf Hitler from time after he is released from Landsberg Prison in 1924. Einstein is successful in his mission, but in the process, he allows Joseph Stalin and the Soviet Union to become powerful enough to launch a massive campaign to conquer Europe.
In the "Civilization" series, the player guides a civilization from prehistory to the present and creates radically altered versions of history on a long time scale. Several scenarios recreate a particular period, which becomes the "point of divergence" in an alternate history shaped by the player's actions. Popular examples in "Sid Meier's Civilization IV" include "Desert War", set in the Mediterranean theatre of World War II and featuring scripted events tied to possible outcomes of battles; "Broken Star", set in a hypothetical Russian civil war in 2010; and "Rhye's and Fall of Civilization", an 'Earth simulator' designed to mirror a history as closely as possible but incorporating unpredictable elements to provide realistic alternate settings.
In some games such as the "Metal Gear" and "Resident Evil" series, events that were originally intended to represent the near future when the games were originally released later ended up becoming alternate histories in later entries in those franchises. For example, "" (1990), set in 1999, depicted a near future that ended up becoming an alternate history in "Metal Gear Solid" (1998). Likewise, "Resident Evil" (1996) and "Resident Evil 2" (1998), both set in 1998, depicted near-future events that had later become an alternative history by the time "Resident Evil 4" (2005) was released.
In the 2009 steampunk shooter, "Damnation" is set on an alternate version of planet Earth, in the early 20th century after the American Civil War, which had spanned over several decades, and steam engines replace combustion engines. The game sees the protagonists fighting off a rich industrialist who wants to do away with both the Union and the Confederacy in one swift movement and turn the United States of America into a country called the "American Empire" with a totalitarian dictatorship.
"Crimson Skies" is one example of an alternate history spawning multiple interpretations in multiple genres. The stories and games in "Crimson Skies" take place in an alternate 1930s United States in which the nation crumbled into many hostile states following the effects of the Great Depression, the Great War, and Prohibition. With the road and railway system destroyed, commerce took to the skies, which led to the emergence of air pirate gangs who plunder the aerial commerce.
The game "Freedom Fighters" portrays a situation similar to that of the movie "Red Dawn" and "Red Alert 2" but less comically than the latter. The point of divergence is during World War II in which the Soviet Union develops an atomic bomb first and uses it on Berlin. With the balance of power and influence tipped in Russia's favor, history diverges. Brief summaries at the beginning of the game inform the player of the Communist bloc's complete takeover of Europe by 1953, a different ending to the Cuban Missile Crisis, and the spread of Soviet influence into South America and Mexico.
Similarly, the 2007 video game "World in Conflict" is set in 1989, with the Soviet Union on the verge of collapse. The point of divergence is several months before the opening of the game, when Warsaw Pact forces staged a desperate invasion of Western Europe. As the game begins, a Soviet invasion force lands in Seattle and takes advantage of the fact that most of the US military is in Europe.
The game ", released in 2008, offered in alternate history campaign for the Imperial Japanese Navy in which Japan destroys all three carriers in the Battle of Midway, which is followed by a successful invasion of the island. That causes the United States to lack any sort of aerial power to fight the Japanese and to be continuously forced onto the defense.
", released in February 2008, is an alternate history first-person shooter in which Winston Churchill died in 1931 from being struck by a taxi cab. Therefore, Great Britain lacks the charismatic leader needed to keep the country together and allows it to be successfully conquered by Nazi Germany during Operation Sea Lion in 1940. Germany later conquers the rest of Europe, as well as North Africa and the Middle East, and produces a massive number of "Wunderwaffe". The Axis powers launch a surprise invasion of the isolationist United States in the Eastern Seaboard in 1953, which forces the country to surrender and submit to a puppet government.
Another alternate history game involving Nazis is "" in which Hitler died during the early days of World War II and so a much more effective leadership rose to power. Under the command of a new Führer (who is referred to as "Chancellor", with his real name never being revealed), Operation Sealion succeeds, and the Nazis successfully conquer Britain and spark a cold war between them and the Allied Powers.
The "Fallout" series of computer role-playing games is set in a divergent US, whose history after World War II diverges from the real world to follow a retro-futuristic timeline. For example, fusion power was invented quite soon after the end of the war, but the transistor was never developed. The result was a future that has a 1950s "World of Tomorrow" feel to it, with extremely high technology like artificial intelligence implemented with thermionic valves and other technologies that are now considered obsolete.
Many game series by the Swedish developer Paradox Interactive start at a concise point in history and allow the player to immerse in the role of a contemporary leader and alter the course of in-game history. The most prominent game with that setting is "Crusader Kings II".
"S.T.A.L.K.E.R." games have an alternate history at the Chernobyl Exclusion Zone in which a special area called "The Zone" is formed.
' is set in an alternate 1960 in which the Nazis won World War II and do so also by acquiring high technology. The sequel ' continues that but is set in the conquered United States of America.
## Online.
Fans of alternate history have made use of the internet from a very early point to showcase their own works and provide useful tools for those fans searching for anything alternate history, first in mailing lists and usenet groups, later in web databases and forums. The "Usenet Alternate History List" was first posted on April 11, 1991, to the Usenet newsgroup rec.arts.sf-lovers. In May 1995, the dedicated newsgroup "soc.history.what-if" was created for showcasing and discussing alternate histories. Its prominence declined with the general migration from unmoderated usenet to moderated web forums, most prominently AlternateHistory.com, the self-described "largest gathering of alternate history fans on the internet" with over 10,000 active members.
In addition to these discussion forums, in 1997 was created as an online repository, now containing over 2,900 alternate history novels, stories, essays, and other printed materials in several different languages. Uchronia was selected as the Sci Fi Channel's "Sci Fi Site of the Week" twice.

</doc>
<doc id="1205" url="https://en.wikipedia.org/wiki?curid=1205" title="Atomic orbitals">
Atomic orbitals



</doc>
<doc id="1206" url="https://en.wikipedia.org/wiki?curid=1206" title="Atomic orbital">
Atomic orbital

In atomic theory and quantum mechanics, an atomic orbital is a mathematical function describing the location and wave-like behavior of an electron in an atom. This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atom's nucleus. The term "atomic orbital" may also refer to the physical region or space where the electron can be calculated to be present, as predicted by the particular mathematical form of the orbital.
Each orbital in an atom is characterized by a set of values of the three quantum numbers , , and , which respectively correspond to the electron's energy, angular momentum, and an angular momentum vector component (the magnetic quantum number). Alternative to the magnetic quantum number, the orbitals are often labeled by the associated harmonic polynomials (e.g."xy","x"2−"y"2). Each such orbital can be occupied by a maximum of two electrons, each with its own projection of spin formula_1. The simple names s orbital, p orbital, d orbital, and f orbital refer to orbitals with angular momentum quantum number and respectively. These names, together with the value of , are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp, principal, diffuse, and fundamental. Orbitals for &gt; 3 continue alphabetically (g, h, i, k, ...), omitting j because some languages do not distinguish between the letters "i" and "j".
Atomic orbitals are the basic building blocks of the atomic orbital model (alternatively known as the electron cloud or wave mechanics model), a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multi-electron atom may be seen as being built up (in approximation) in an electron configuration that is a product of simpler hydrogen-like atomic orbitals. The repeating "periodicity" of the blocks of 2, 6, 10, and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s, p, d, and f atomic orbitals, respectively, although for higher values of the quantum number , particularly when the atom in question bears a positive charge, the energies of certain sub-shells become very similar and so the order in which they are said to be populated by electrons (e.g. Cr = [Ar]4s13d5 and Cr2+ = [Ar]3d4) can only be rationalized somewhat arbitrarily.
## Electron properties.
With the development of quantum mechanics and experimental findings (such as the two slit diffraction of electrons), it was found that the orbiting electrons around a nucleus could not be fully described as particles, but needed to be explained by the wave-particle duality. In this sense, the electrons have the following properties:
Wave-like properties:
Particle-like properties:
Thus, electrons cannot be described simply as solid particles. An analogy might be that of a large and often oddly shaped "atmosphere" (the electron), distributed around a relatively tiny planet (the atomic nucleus). Atomic orbitals exactly describe the shape of this "atmosphere" only when a single electron is present in an atom. When more electrons are added to a single atom, the additional electrons tend to more evenly fill in a volume of space around the nucleus so that the resulting collection (sometimes termed the atom's "electron cloud") tends toward a generally spherical zone of probability describing the electron's location, because of the uncertainty principle.
### Formal quantum mechanical definition.
Atomic orbitals may be defined more precisely in formal quantum mechanical language. They are an approximate solution to the Schrodinger equation for the electrons bound to the atom by the electric field of the atom's nucleus. Specifically, in quantum mechanics, the state of an atom, i.e., an eigenstate of the atomic Hamiltonian, is approximated by an expansion (see configuration interaction expansion and basis set) into linear combinations of anti-symmetrized products (Slater determinants) of one-electron functions. The spatial components of these one-electron functions are called atomic orbitals. (When one considers also their spin component, one speaks of atomic spin orbitals.) A state is actually a function of the coordinates of all the electrons, so that their motion is correlated, but this is often approximated by this independent-particle model of products of single electron wave functions. (The London dispersion force, for example, depends on the correlations of the motion of the electrons.)
In atomic physics, the atomic spectral lines correspond to transitions (quantum leaps) between quantum states of an atom. These states are labeled by a set of quantum numbers summarized in the term symbol and usually associated with particular electron configurations, i.e., by occupation schemes of atomic orbitals (for example, 1s2 2s2 2p6 for the ground state of neon-term symbol: 1S0).
This notation means that the corresponding Slater determinants have a clear higher weight in the configuration interaction expansion. The atomic orbital concept is therefore a key concept for visualizing the excitation process associated with a given transition. For example, one can say for a given transition that it corresponds to the excitation of an electron from an occupied orbital to a given unoccupied orbital. Nevertheless, one has to keep in mind that electrons are fermions ruled by the Pauli exclusion principle and cannot be distinguished from each other. Moreover, it sometimes happens that the configuration interaction expansion converges very slowly and that one cannot speak about simple one-determinant wave function at all. This is the case when electron correlation is large.
Fundamentally, an atomic orbital is a one-electron wave function, even though most electrons do not exist in one-electron atoms, and so the one-electron view is an approximation. When thinking about orbitals, we are often given an orbital visualization heavily influenced by the Hartree–Fock approximation, which is one way to reduce the complexities of molecular orbital theory.
### Types of orbitals.
Atomic orbitals can be the hydrogen-like "orbitals" which are exact solutions to the Schrödinger equation for a hydrogen-like "atom" (i.e., an atom with one electron). Alternatively, atomic orbitals refer to functions that depend on the coordinates of one electron (i.e., orbitals) but are used as starting points for approximating wave functions that depend on the simultaneous coordinates of all the electrons in an atom or molecule. The coordinate systems chosen for atomic orbitals are usually spherical coordinates in atoms and Cartesian in polyatomic molecules. The advantage of spherical coordinates (for atoms) is that an orbital wave function is a product of three factors each dependent on a single coordinate: . The angular factors of atomic orbitals generate s, p, d, etc. functions as real combinations of spherical harmonics (where and are quantum numbers). There are typically three mathematical forms for the radial functions  which can be chosen as a starting point for the calculation of the properties of atoms and molecules with many electrons:
Although hydrogen-like orbitals are still used as pedagogical tools, the advent of computers has made STOs preferable for atoms and diatomic molecules since combinations of STOs can replace the nodes in hydrogen-like atomic orbital. Gaussians are typically used in molecules with three or more atoms. Although not as accurate by themselves as STOs, combinations of many Gaussians can attain the accuracy of hydrogen-like orbitals.
## History.
The term "orbital" was coined by Robert Mulliken in 1932 as an abbreviation for "one-electron orbital wave function". However, the idea that electrons might revolve around a compact nucleus with definite angular momentum was convincingly argued at least 19 years earlier by Niels Bohr, and the Japanese physicist Hantaro Nagaoka published an orbit-based hypothesis for electronic behavior as early as 1904. Explaining the behavior of these electron "orbits" was one of the driving forces behind the development of quantum mechanics.
### Early models.
With J. J. Thomson's discovery of the electron in 1897, it became clear that atoms were not the smallest building blocks of nature, but were rather composite particles. The newly discovered structure within atoms tempted many to imagine how the atom's constituent parts might interact with each other. Thomson theorized that multiple electrons revolved in orbit-like rings within a positively charged jelly-like substance, and between the electron's discovery and 1909, this "plum pudding model" was the most widely accepted explanation of atomic structure.
Shortly after Thomson's discovery, Hantaro Nagaoka predicted a different model for electronic structure. Unlike the plum pudding model, the positive charge in Nagaoka's "Saturnian Model" was concentrated into a central core, pulling the electrons into circular orbits reminiscent of Saturn's rings. Few people took notice of Nagaoka's work at the time, and Nagaoka himself recognized a fundamental defect in the theory even at its conception, namely that a classical charged object cannot sustain orbital motion because it is accelerating and therefore loses energy due to electromagnetic radiation. Nevertheless, the Saturnian model turned out to have more in common with modern theory than any of its contemporaries.
### Bohr atom.
In 1909, Ernest Rutherford discovered that the bulk of the atomic mass was tightly condensed into a nucleus, which was also found to be positively charged. It became clear from his analysis in 1911 that the plum pudding model could not explain atomic structure. In 1913, Rutherford's post-doctoral student, Niels Bohr, proposed a new model of the atom, wherein electrons orbited the nucleus with classical periods, but were only permitted to have discrete values of angular momentum, quantized in units "h"/2π. This constraint automatically permitted only certain values of electron energies. The Bohr model of the atom fixed the problem of energy loss from radiation from a ground state (by declaring that there was no state below this), and more importantly explained the origin of spectral lines.
After Bohr's use of Einstein's explanation of the photoelectric effect to relate energy levels in atoms with the wavelength of emitted light, the connection between the structure of electrons in atoms and the emission and absorption spectra of atoms became an increasingly useful tool in the understanding of electrons in atoms. The most prominent feature of emission and absorption spectra (known experimentally since the middle of the 19th century), was that these atomic spectra contained discrete lines. The significance of the Bohr model was that it related the lines in emission and absorption spectra to the energy differences between the orbits that electrons could take around an atom. This was, however, "not" achieved by Bohr through giving the electrons some kind of wave-like properties, since the idea that electrons could behave as matter waves was not suggested until eleven years later. Still, the Bohr model's use of quantized angular momenta and therefore quantized energy levels was a significant step towards the understanding of electrons in atoms, and also a significant step towards the development of quantum mechanics in suggesting that quantized restraints must account for all discontinuous energy levels and spectra in atoms.
With de Broglie's suggestion of the existence of electron matter waves in 1924, and for a short time before the full 1926 Schrödinger equation treatment of hydrogen-like atoms, a Bohr electron "wavelength" could be seen to be a function of its momentum, and thus a Bohr orbiting electron was seen to orbit in a circle at a multiple of its half-wavelength. The Bohr model for a short time could be seen as a classical model with an additional constraint provided by the 'wavelength' argument. However, this period was immediately superseded by the full three-dimensional wave mechanics of 1926. In our current understanding of physics, the Bohr model is called a semi-classical model because of its quantization of angular momentum, not primarily because of its relationship with electron wavelength, which appeared in hindsight a dozen years after the Bohr model was proposed.
The Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the "n" = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the "n" = 1 state can hold one or two electrons, while the "n" = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all "n" = 1 states are fully occupied; the same is true for "n" = 1 and "n" = 2 in neon. In argon, the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon (contrary to the situation in the hydrogen atom) and remains empty.
### Modern conceptions and connections to the Heisenberg uncertainty principle.
Immediately after Heisenberg discovered his uncertainty principle, Bohr noted that the existence of any sort of wave packet implies uncertainty in the wave frequency and wavelength, since a spread of frequencies is needed to create the packet itself. In quantum mechanics, where all particle momenta are associated with waves, it is the formation of such a wave packet which localizes the wave, and thus the particle, in space. In states where a quantum mechanical particle is bound, it must be localized as a wave packet, and the existence of the packet and its minimum size implies a spread and minimal value in particle wavelength, and thus also momentum and energy. In quantum mechanics, as a particle is localized to a smaller region in space, the associated compressed wave packet requires a larger and larger range of momenta, and thus larger kinetic energy. Thus the binding energy to contain or trap a particle in a smaller region of space increases without bound as the region of space grows smaller. Particles cannot be restricted to a geometric point in space, since this would require an infinite particle momentum.
In chemistry, Schrödinger, Pauling, Mulliken and others noted that the consequence of Heisenberg's relation was that the electron, as a wave packet, could not be considered to have an exact location in its orbital. Max Born suggested that the electron's position needed to be described by a probability distribution which was connected with finding the electron at some point in the wave-function which described its associated wave packet. The new quantum mechanics did not give exact results, but only the probabilities for the occurrence of a variety of possible such results. Heisenberg held that the path of a moving particle has no meaning if we cannot observe it, as we cannot with electrons in an atom.
In the quantum picture of Heisenberg, Schrödinger and others, the Bohr atom number "n" for each orbital became known as an "n-sphere" in a three-dimensional atom and was pictured as the most probable energy of the probability cloud of the electron's wave packet which surrounded the atom.
## Orbital names.
### Orbital notation and subshells.
Orbitals have been given names, which are usually given in the form:
where "X" is the energy level corresponding to the principal quantum number ; type is a lower-case letter denoting the shape or subshell of the orbital, corresponding to the angular momentum quantum number . 
For example, the orbital 1s (pronounced as the individual numbers and letters: "'one' 'ess'") is the lowest energy level () and has an angular quantum number of , denoted as s. Orbitals with are denoted as p, d and f respectively.
The set of orbitals for a given n and is called a "subshell", denoted 
The exponent y shows the number of electrons in the subshell. For example, the notation 2p4 indicates that the 2p subshell of an atom contains 4 electrons. This subshell has 3 orbitals, each with n = 2 and = 1.
### X-ray notation.
There is also another, less common system still used in X-ray science known as X-ray notation, which is a continuation of the notations used before orbital theory was well understood. In this system, the principal quantum number is given a letter associated with it. For , the letters associated with those numbers are K, L, M, N, O, ... respectively.
## Hydrogen-like orbitals.
The simplest atomic orbitals are those that are calculated for systems with a single electron, such as the hydrogen atom. An atom of any other element ionized down to a single electron is very similar to hydrogen, and the orbitals take the same form. In the Schrödinger equation for this system of one negative and one positive particle, the atomic orbitals are the eigenstates of the Hamiltonian operator for the energy. They can be obtained analytically, meaning that the resulting orbitals are products of a polynomial series, and exponential and trigonometric functions. (see hydrogen atom).
For atoms with two or more electrons, the governing equations can only be solved with the use of methods of iterative approximation. Orbitals of multi-electron atoms are "qualitatively" similar to those of hydrogen, and in the simplest models, they are taken to have the same form. For more rigorous and precise analysis, numerical approximations must be used.
A given (hydrogen-like) atomic orbital is identified by unique values of three quantum numbers: , , and . The rules restricting the values of the quantum numbers, and their energies (see below), explain the electron configuration of the atoms and the periodic table.
The stationary states (quantum states) of the hydrogen-like atoms are its atomic orbitals. However, in general, an electron's behavior is not fully described by a single orbital. Electron states are best represented by time-depending "mixtures" (linear combinations) of multiple orbitals. See Linear combination of atomic orbitals molecular orbital method.
The quantum number first appeared in the Bohr model where it determines the radius of each circular electron orbit. In modern quantum mechanics however, determines the mean distance of the electron from the nucleus; all electrons with the same value of "n" lie at the same average distance. For this reason, orbitals with the same value of "n" are said to comprise a "shell". Orbitals with the same value of "n" and also the same value of  are even more closely related, and are said to comprise a "subshell".
## Quantum numbers.
Because of the quantum mechanical nature of the electrons around a nucleus, atomic orbitals can be uniquely defined by a set of integers known as quantum numbers. These quantum numbers only occur in certain combinations of values, and their physical interpretation changes depending on whether real or complex versions of the atomic orbitals are employed.
### Complex orbitals.
In physics, the most common orbital descriptions are based on the solutions to the hydrogen atom, where orbitals are given by the product between a radial function and a pure spherical harmonic. The quantum numbers, together with the rules governing their possible values, are as follows:
The principal quantum number describes the energy of the electron and is always a positive integer. In fact, it can be any positive integer, but for reasons discussed below, large numbers are seldom encountered. Each atom has, in general, many orbitals associated with each value of "n"; these orbitals together are sometimes called "electron shells".
The azimuthal quantum number describes the orbital angular momentum of each electron and is a non-negative integer. Within a shell where is some integer , ranges across all (integer) values satisfying the relation formula_5. For instance, the  shell has only orbitals with formula_6, and the  shell has only orbitals with formula_6, and formula_8. The set of orbitals associated with a particular value of  are sometimes collectively called a "subshell".
The magnetic quantum number, formula_9, describes the magnetic moment of an electron in an arbitrary direction, and is also always an integer. Within a subshell where formula_10 is some integer formula_11, formula_9 ranges thus: formula_13.
The above results may be summarized in the following table. Each cell represents a subshell, and lists the values of formula_9 available in that subshell. Empty cells represent subshells that do not exist.
Subshells are usually identified by their formula_15- and formula_10-values. formula_15 is represented by its numerical value, but formula_10 is represented by a letter as follows: 0 is represented by 's', 1 by 'p', 2 by 'd', 3 by 'f', and 4 by 'g'. For instance, one may speak of the subshell with formula_19 and formula_6 as a '2s subshell'.
Each electron also has a spin quantum number, "s", which describes the spin of each electron (spin up or spin down). The number "s" can be + or −.
The Pauli exclusion principle states that no two electrons in an atom can have the same values of all four quantum numbers. If there are two electrons in an orbital with given values for three quantum numbers, (, , ), these two electrons must differ in their spin.
The above conventions imply a preferred axis (for example, the "z" direction in Cartesian coordinates), and they also imply a preferred direction along this preferred axis. Otherwise there would be no sense in distinguishing from . As such, the model is most useful when applied to physical systems that share these symmetries. The Stern–Gerlach experiment — where an atom is exposed to a magnetic field — provides one such example.
### Real orbitals.
An atom that is embedded in a crystalline solid feels multiple preferred axes, but often no preferred direction. Instead of building atomic orbitals out of the product of radial functions and a single spherical harmonic, linear combinations of spherical harmonics are typically used, designed so that the imaginary part of the spherical harmonics cancel out. These real orbitals are the building blocks most commonly shown in orbital visualizations.
In the real hydrogen-like orbitals, for example, and have the same interpretation and significance as their complex counterparts, but is no longer a good quantum number (though its absolute value is). The orbitals are given new names based on their shape with respect to a standardized Cartesian basis. The real hydrogen-like p orbitals are given by the following
where , , and , are the complex orbitals corresponding to .
The equations for the p"x" and p"y" orbitals depend on the phase convention used for the spherical harmonics. The above equations suppose that the spherical harmonics are defined by formula_24. However some quantum physicists include a phase factor in these definitions, which has the effect of relating the p"x" orbital to a "difference" of spherical harmonics and the p"y" orbital to the corresponding "sum". (For more detail, see Spherical harmonics#Conventions).
## Shapes of orbitals.
Simple pictures showing orbital shapes are intended to describe the angular forms of regions in space where the electrons occupying the orbital are likely to be found. The diagrams cannot show the entire region where an electron can be found, since according to quantum mechanics there is a non-zero probability of finding the electron (almost) anywhere in space. Instead the diagrams are approximate representations of boundary or contour surfaces where the probability density has a constant value, chosen so that there is a certain probability (for example 90%) of finding the electron within the contour. Although as the square of an absolute value is everywhere non-negative, the sign of the wave function is often indicated in each subregion of the orbital picture.
Sometimes the function will be graphed to show its phases, rather than the which shows probability density but has no phases (which have been lost in the process of taking the absolute value, since is a complex number). orbital graphs tend to have less spherical, thinner lobes than graphs, but have the same number of lobes in the same places, and otherwise are recognizable. This article, in order to show wave function phases, shows mostly graphs.
The lobes can be viewed as standing wave interference patterns between the two counter rotating, ring resonant travelling wave " and " modes, with the projection of the orbital onto the xy plane having a resonant "" wavelengths around the circumference. Though rarely depicted, the travelling wave solutions can be viewed as rotating banded tori, with the bands representing phase information. For each there are two standing wave solutions and . For the case where the orbital is vertical, counter rotating information is unknown, and the orbital is "z"-axis symmetric. For the case where there are no counter rotating modes. There are only radial modes and the shape is spherically symmetric. For any given , the smaller is, the more radial nodes there are. For any given , the smaller is, the fewer radial nodes there are (zero for whichever first has that orbital). Loosely speaking is energy, is analogous to eccentricity, and is orientation. In the classical case, a ring resonant travelling wave, for example in a circular transmission line, unless actively forced, will spontaneously decay into a ring resonant standing wave because reflections will build up over time at even the smallest imperfection or discontinuity.
Generally speaking, the number determines the size and energy of the orbital for a given nucleus: as increases, the size of the orbital increases. When comparing different elements, the higher nuclear charge of heavier elements causes their orbitals to contract by comparison to lighter ones, so that the overall size of the whole atom remains very roughly constant, even as the number of electrons in heavier elements (higher ) increases.
Also in general terms, determines an orbital's shape, and its orientation. However, since some orbitals are described by equations in complex numbers, the shape sometimes depends on also. Together, the whole set of orbitals for a given and fill space as symmetrically as possible, though with increasingly complex sets of lobes and nodes.
The single s-orbitals (formula_6) are shaped like spheres. For it is roughly a solid ball (it is most dense at the center and fades exponentially outwardly), but for or more, each single s-orbital is composed of spherically symmetric surfaces which are nested shells (i.e., the "wave-structure" is radial, following a sinusoidal radial component as well). See illustration of a cross-section of these nested shells, at right. The s-orbitals for all numbers are the only orbitals with an anti-node (a region of high wave function density) at the center of the nucleus. All other orbitals (p, d, f, etc.) have angular momentum, and thus avoid the nucleus (having a wave node "at" the nucleus). Recently, there has been an effort to experimentally image the 1s and 2p orbitals in a SrTiO3 crystal using scanning transmission electron microscopy with energy dispersive x-ray spectroscopy. Because the imaging was conducted using an electron beam, Coulombic beam-orbital interaction that is often termed as the impact parameter effect is included in the final outcome (see the figure at right).
The shapes of p, d and f-orbitals are described verbally here and shown graphically in the "Orbitals table" below. The three p-orbitals for have the form of two ellipsoids with a point of tangency at the nucleus (the two-lobed shape is sometimes referred to as a "dumbbell"—there are two lobes pointing in opposite directions from each other). The three p-orbitals in each shell are oriented at right angles to each other, as determined by their respective linear combination of values of . The overall result is a lobe pointing along each direction of the primary axes.
Four of the five d-orbitals for look similar, each with four pear-shaped lobes, each lobe tangent at right angles to two others, and the centers of all four lying in one plane. Three of these planes are the xy-, xz-, and yz-planes—the lobes are between the pairs of primary axes—and the fourth has the centre along the x and y axes themselves. The fifth and final d-orbital consists of three regions of high probability density: a torus in between two pear-shaped regions placed symmetrically on its z axis. The overall total of 18 directional lobes point in every primary axis direction and between every pair.
There are seven f-orbitals, each with shapes more complex than those of the d-orbitals.
Additionally, as is the case with the s orbitals, individual p, d, f and g orbitals with values higher than the lowest possible value, exhibit an additional radial node structure which is reminiscent of harmonic waves of the same type, as compared with the lowest (or fundamental) mode of the wave. As with s orbitals, this phenomenon provides p, d, f, and g orbitals at the next higher possible value of (for example, 3p orbitals vs. the fundamental 2p), an additional node in each lobe. Still higher values of further increase the number of radial nodes, for each type of orbital.
The shapes of atomic orbitals in one-electron atom are related to 3-dimensional spherical harmonics. These shapes are not unique, and any linear combination is valid, like a transformation to cubic harmonics, in fact it is possible to generate sets where all the d's are the same shape, just like the and are the same shape.
Although individual orbitals are most often shown independent of each other, the orbitals coexist around the nucleus at the same time. Also, in 1927, Albrecht Unsöld proved that if one sums the electron density of all orbitals of a particular azimuthal quantum number of the same shell (e.g. all three 2p orbitals, or all five 3d orbitals) where each orbital is occupied by an electron or each is occupied by an electron pair, then all angular dependence disappears; that is, the resulting total density of all the atomic orbitals in that subshell (those with the same ) is spherical. This is known as Unsöld's theorem.
### Orbitals table.
This table shows all orbital configurations for the real hydrogen-like wave functions up to 7s, and therefore covers the simple electronic configuration for all elements in the periodic table up to radium. "ψ" graphs are shown with − and + wave function phases shown in two different colors (arbitrarily red and blue). The orbital is the same as the orbital, but the and are formed by taking linear
combinations of the and orbitals (which is why they are listed under the label). Also, the and are not
the same shape as the , since they are pure spherical harmonics.
† "The elements with this magnetic quantum number have been discovered, but their electronic configuration is only a prediction."
‡ "The electronic configuration of the elements with this magnetic quantum number has only been confirmed for a spin quantum number of "+1/2"."
These are the real-valued orbitals commonly used in chemistry. Only the formula_26 orbitals where are eigenstates of the orbital angular momentum operator, formula_27. The columns with formula_28 are contain combinations of two eigenstates. See : 
### Qualitative understanding of shapes.
The shapes of atomic orbitals can be qualitatively understood by considering the analogous case of standing waves on a circular drum. To see the analogy, the mean vibrational displacement of each bit of drum membrane from the equilibrium point over many cycles (a measure of average drum membrane velocity and momentum at that point) must be considered relative to that point's distance from the center of the drum head. If this displacement is taken as being analogous to the probability of finding an electron at a given distance from the nucleus, then it will be seen that the many modes of the vibrating disk form patterns that trace the various shapes of atomic orbitals. The basic reason for this correspondence lies in the fact that the distribution of kinetic energy and momentum in a matter-wave is predictive of where the particle associated with the wave will be. That is, the probability of finding an electron at a given place is also a function of the electron's average momentum at that point, since high electron momentum at a given position tends to "localize" the electron in that position, via the properties of electron wave-packets (see the Heisenberg uncertainty principle for details of the mechanism).
This relationship means that certain key features can be observed in both drum membrane modes and atomic orbitals. For example, in all of the modes analogous to s orbitals (the top row in the animated illustration below), it can be seen that the very center of the drum membrane vibrates most strongly, corresponding to the antinode in all s orbitals in an atom. This antinode means the electron is most likely to be at the physical position of the nucleus (which it passes straight through without scattering or striking it), since it is moving (on average) most rapidly at that point, giving it maximal momentum.
A mental "planetary orbit" picture closest to the behavior of electrons in s orbitals, all of which have no angular momentum, might perhaps be that of a Keplerian orbit with the orbital eccentricity of 1 but a finite major axis, not physically possible (because particles were to collide), but can be imagined as a limit of orbits with equal major axes but increasing eccentricity.
Below, a number of drum membrane vibration modes and the respective wave functions of the hydrogen atom are shown. A correspondence can be considered where the wave functions of a vibrating drum head are for a two-coordinate system and the wave functions for a vibrating sphere are three-coordinate .
None of the other sets of modes in a drum membrane have a central antinode, and in all of them the center of the drum does not move. These correspond to a node at the nucleus for all non-s orbitals in an atom. These orbitals all have some angular momentum, and in the planetary model, they correspond to particles in orbit with eccentricity less than 1.0, so that they do not pass straight through the center of the primary body, but keep somewhat away from it.
In addition, the drum modes analogous to p and d modes in an atom show spatial irregularity along the different radial directions from the center of the drum, whereas all of the modes analogous to s modes are perfectly symmetrical in radial direction. The non radial-symmetry properties of non-s orbitals are necessary to localize a particle with angular momentum and a wave nature in an orbital where it must tend to stay away from the central attraction force, since any particle localized at the point of central attraction could have no angular momentum. For these modes, waves in the drum head tend to avoid the central point. Such features again emphasize that the shapes of atomic orbitals are a direct consequence of the wave nature of electrons.
## Orbital energy.
In atoms with a single electron (hydrogen-like atoms), the energy of an orbital (and, consequently, of any electrons in the orbital) is determined mainly by formula_15. The formula_30 orbital has the lowest possible energy in the atom. Each successively higher value of formula_15 has a higher level of energy, but the difference decreases as formula_15 increases. For high formula_15, the level of energy becomes so high that the electron can easily escape from the atom. In single electron atoms, all levels with different formula_10 within a given formula_15 are degenerate in the Schrödinger approximation, and have the same energy. This approximation is broken to a slight extent in the solution to the Dirac equation (where the energy depends on and another quantum number ), and by the effect of the magnetic field of the nucleus and quantum electrodynamics effects. The latter induce tiny binding energy differences especially for s electrons that go nearer the nucleus, since these feel a very slightly different nuclear charge, even in one-electron atoms; see Lamb shift.
In atoms with multiple electrons, the energy of an electron depends not only on the intrinsic properties of its orbital, but also on its interactions with the other electrons. These interactions depend on the detail of its spatial probability distribution, and so the energy levels of orbitals depend not only on formula_15 but also on formula_10. Higher values of formula_10 are associated with higher values of energy; for instance, the 2p state is higher than the 2s state. When formula_39, the increase in energy of the orbital becomes so large as to push the energy of orbital above the energy of the s-orbital in the next higher shell; when formula_40 the energy is pushed into the shell two steps higher. The filling of the 3d orbitals does not occur until the 4s orbitals have been filled.
The increase in energy for subshells of increasing angular momentum in larger atoms is due to electron–electron interaction effects, and it is specifically related to the ability of low angular momentum electrons to penetrate more effectively toward the nucleus, where they are subject to less screening from the charge of intervening electrons. Thus, in atoms of higher atomic number, the formula_10 of electrons becomes more and more of a determining factor in their energy, and the principal quantum numbers formula_15 of electrons becomes less and less important in their energy placement.
The energy sequence of the first 35 subshells (e.g., 1s, 2p, 3d, etc.) is given in the following table. Each cell represents a subshell with formula_15 and formula_10 given by its row and column indices, respectively. The number in the cell is the subshell's position in the sequence. For a linear listing of the subshells in terms of increasing energies in multielectron atoms, see the section below.
"Note: empty cells indicate non-existent sublevels, while numbers in italics indicate sublevels that could (potentially) exist, but which do not hold electrons in any element currently known."
## Electron placement and the periodic table.
Several rules govern the placement of electrons in orbitals ("electron configuration"). The first dictates that no two electrons in an atom may have the same set of values of quantum numbers (this is the Pauli exclusion principle). These quantum numbers include the three that define orbitals, as well as , or spin quantum number. Thus, two electrons may occupy a single orbital, so long as they have different values of . However, "only" two electrons, because of their spin, can be associated with each orbital.
Additionally, an electron always tends to fall to the lowest possible energy state. It is possible for it to occupy any orbital so long as it does not violate the Pauli exclusion principle, but if lower-energy orbitals are available, this condition is unstable. The electron will eventually lose energy (by releasing a photon) and drop into the lower orbital. Thus, electrons fill orbitals in the order specified by the energy sequence given above.
This behavior is responsible for the structure of the periodic table. The table may be divided into several rows (called 'periods'), numbered starting with 1 at the top. The presently known elements occupy seven periods. If a certain period has number "i", it consists of elements whose outermost electrons fall in the "i"th shell. Niels Bohr was the first to propose (1923) that the periodicity in the properties of the elements might be explained by the periodic filling of the electron energy levels, resulting in the electronic structure of the atom.
The periodic table may also be divided into several numbered rectangular 'blocks'. The elements belonging to a given block have this common feature: their highest-energy electrons all belong to the same -state (but the associated with that -state depends upon the period). For instance, the leftmost two columns constitute the 's-block'. The outermost electrons of Li and Be respectively belong to the 2s subshell, and those of Na and Mg to the 3s subshell.
The following is the order for filling the "subshell" orbitals, which also gives the order of the "blocks" in the periodic table:
The "periodic" nature of the filling of orbitals, as well as emergence of the s, p, d, and f "blocks", is more obvious if this order of filling is given in matrix form, with increasing principal quantum numbers starting the new rows ("periods") in the matrix. Then, each subshell (composed of the first two quantum numbers) is repeated as many times as required for each pair of electrons it may contain. The result is a compressed periodic table, with each entry representing two successive elements:
Although this is the general order of orbital filling according to the Madelung rule, there are exceptions, and the actual electronic energies of each element are also dependent upon additional details of the atoms (see ).
The number of electrons in an electrically neutral atom increases with the atomic number. The electrons in the outermost shell, or "valence electrons", tend to be responsible for an element's chemical behavior. Elements that contain the same number of valence electrons can be grouped together and display similar chemical properties.
### Relativistic effects.
For elements with high atomic number , the effects of relativity become more pronounced, and especially so for s electrons, which move at relativistic velocities as they penetrate the screening electrons near the core of high- atoms. This relativistic increase in momentum for high speed electrons causes a corresponding decrease in wavelength and contraction of 6s orbitals relative to 5d orbitals (by comparison to corresponding s and d electrons in lighter elements in the same column of the periodic table); this results in 6s valence electrons becoming lowered in energy.
Examples of significant physical outcomes of this effect include the lowered melting temperature of mercury (which results from 6s electrons not being available for metal bonding) and the golden color of gold and caesium.
In the Bohr Model, an  electron has a velocity given by formula_45, where is the atomic number, formula_46 is the fine-structure constant, and is the speed of light. In non-relativistic quantum mechanics, therefore, any atom with an atomic number greater than 137 would require its 1s electrons to be traveling faster than the speed of light. Even in the Dirac equation, which accounts for relativistic effects, the wave function of the electron for atoms with formula_47 is oscillatory and unbounded. The significance of element 137, also known as untriseptium, was first pointed out by the physicist Richard Feynman. Element 137 is sometimes informally called feynmanium (symbol Fy). However, Feynman's approximation fails to predict the exact critical value of  due to the non-point-charge nature of the nucleus and very small orbital radius of inner electrons, resulting in a potential seen by inner electrons which is effectively less than . The critical  value, which makes the atom unstable with regard to high-field breakdown of the vacuum and production of electron-positron pairs, does not occur until is about 173. These conditions are not seen except transiently in collisions of very heavy nuclei such as lead or uranium in accelerators, where such electron-positron production from these effects has been claimed to be observed.
There are no nodes in relativistic orbital densities, although individual components of the wave function will have nodes.
### pp hybridisation (conjectured).
In late period-8 elements a hybrid of 8p3/2 and 9p1/2 is expected to exist, where "3/2" and "1/2" refer to the total angular momentum quantum number. This "pp" hybrid may be responsible for the p-block of the period due to properties similar to p subshells in ordinary valence shells. Energy levels of 8p3/2 and 9p1/2 come close due to relativistic spin–orbit effects; the 9s subshell should also participate, as these elements are expected to be analogous to the respective 5p elements indium through xenon.
## Transitions between orbitals.
Bound quantum states have discrete energy levels. When applied to atomic orbitals, this means that the energy differences between states are also discrete. A transition between these states (i.e., an electron absorbing or emitting a photon) can thus only happen if the photon has an energy corresponding with the exact energy difference between said states.
Consider two states of the hydrogen atom:
By quantum theory, state 1 has a fixed energy of , and state 2 has a fixed energy of . Now, what would happen if an electron in state 1 were to move to state 2? For this to happen, the electron would need to gain an energy of exactly . If the electron receives energy that is less than or greater than this value, it cannot jump from state 1 to state 2. Now, suppose we irradiate the atom with a broad-spectrum of light. Photons that reach the atom that have an energy of exactly will be absorbed by the electron in state 1, and that electron will jump to state 2. However, photons that are greater or lower in energy cannot be absorbed by the electron, because the electron can only jump to one of the orbitals, it cannot jump to a state between orbitals. The result is that only photons of a specific frequency will be absorbed by the atom. This creates a line in the spectrum, known as an absorption line, which corresponds to the energy difference between states 1 and 2.
The atomic orbital model thus predicts line spectra, which are observed experimentally. This is one of the main validations of the atomic orbital model.
The atomic orbital model is nevertheless an approximation to the full quantum theory, which only recognizes many electron states. The predictions of line spectra are qualitatively useful but are not quantitatively accurate for atoms and ions other than those containing only one electron.

</doc>
<doc id="1207" url="https://en.wikipedia.org/wiki?curid=1207" title="Amino acid">
Amino acid

 
Amino acids are organic compounds that contain amino () and carboxylate functional groups, along with a side chain (R group) specific to each amino acid. The elements present in every amino acid are carbon (C), hydrogen (H), oxygen (O), and nitrogen (N); in addition sulfur (S) is present in the side chains of cysteine and methionine, and selenium (Se) in the less common amino acid selenocysteine. More than 500 naturally occurring amino acids are known to constitute monomer units of peptides, including proteins, (though only 20 appear in the genetic code, plus selenocysteine, which is encoded in a special way.)
Amino acids are formally named by the IUPAC-IUBMB Joint Commission on Biochemical Nomenclature in terms of the fictitious "neutral" structure shown in the illustration. For example, the systematic name of alanine is 2-aminopropanoic acid, based on the formula . The Commission justified this approach as follows:
The systematic names and formulas given refer to hypothetical forms in which amino groups are unprotonated and carboxyl groups are undissociated. This convention is useful to avoid various nomenclatural problems but should not be taken to imply that these structures represent an appreciable fraction of the amino-acid molecules.
The last part of the second sentence ("[This] should not be taken to imply that these structures represent an appreciable fraction of the amino-acid molecules") is vital.
They can be classified according to the locations of the core structural functional groups, as alpha- (α-), beta- (β-), gamma- (γ-) or delta- (δ-) amino acids; other categories relate to polarity, ionization, and side chain group type (aliphatic, acyclic, aromatic, containing hydroxyl or sulfur, etc.). In the form of proteins, amino acid residues form the second-largest component (water is the largest) of human muscles and other tissues. Beyond their role as residues in proteins, amino acids participate in a number of processes such as neurotransmitter transport and biosynthesis.
## History.
The first few amino acids were discovered in the early 19th century. In 1806, French chemists Louis-Nicolas Vauquelin and Pierre Jean Robiquet isolated a compound from asparagus that was subsequently named asparagine, the first amino acid to be discovered. Cystine was discovered in 1810, although its monomer, cysteine, remained undiscovered until 1884. Glycine and leucine were discovered in 1820. The last of the 20 common amino acids to be discovered was threonine in 1935 by William Cumming Rose, who also determined the essential amino acids and established the minimum daily requirements of all amino acids for optimal growth.
The unity of the chemical category was recognized by Wurtz in 1865, but he gave no particular name to it. The first use of the term "amino acid" in the English language dates from 1898, while the German term, , was used earlier. Proteins were found to yield amino acids after enzymatic digestion or acid hydrolysis. In 1902, Emil Fischer and Franz Hofmeister independently proposed that proteins are formed from many amino acids, whereby bonds are formed between the amino group of one amino acid with the carboxyl group of another, resulting in a linear structure that Fischer termed "peptide".
## General structure.
In the structure shown at the top of the page R represents a side chain specific to each amino acid. The carbon atom next to the carboxyl group is called the α–carbon. Amino acids containing an amino group bonded directly to the α-carbon are referred to as "α-amino acids". These include proline and hydroxyproline, which are secondary amines. In the past they were often called "imino acids", a misnomer because they do not contain an imine grouping . The obsolete term remains frequent.
### Isomerism.
The common natural forms of amino acids have the structure ( in the case of proline) and functional groups attached to the same C atom, and are thus α-amino acids. With the exception of achiral glycine, natural amino acids have the configuration, and are the only ones found in proteins during translation in the ribosome.
The and convention for amino acid configuration refers not to the optical activity of the amino acid itself but rather to the optical activity of the isomer of glyceraldehyde from which that amino acid can, in theory, be synthesized (-glyceraldehyde is dextrorotatory; -glyceraldehyde is levorotatory).
An alternative convention is to use the ("S") and ("R") designators to specify the "absolute configuration". Almost all of the amino acids in proteins are ("S") at the α carbon, with cysteine being ("R") and glycine non-chiral. Cysteine has its side chain in the same geometric location as the other amino acids, but the "R"/"S" terminology is reversed because sulfur has higher atomic number compared to the carboxyl oxygen which gives the side chain a higher priority by the , whereas the atoms in most other side chains give them lower priority compared to the carboxyl group.
-amino acid residues are found in some proteins, but they are rare.
### Side chains.
Amino acids are designated as α- when the amino nitrogen atom is attached to the α-carbon, the carbon atom adjacent to the carboxylate group.
In all cases below in this section the formula_1 values (if any) refer to the ionization of the groups as amino acid residues in proteins. They are not formula_1 values for the free amino acids (which are of little biochemical importance).
#### Aliphatic side-chains.
Several side-chains contain only H and C, and do not ionize. These are as follows (with three- and one-letter symbols in parenthesis):
#### Polar neutral side-chains.
Two aminoacids contains alcohol side-chains. These do not ionize in normal conditions, though one, serine, becomes deprotonated during the catalysis by serine proteases: this is an example of severe perturbation, and is not characteristic of serine residues in general.
Threonine has two chiral centers, not only the (2"S") chiral center at the α-carbon shared by all amino acids apart from achiral glycine, but also (3"R") at the β-carbon. The full stereochemical specification is -threonine (2"S",3"R").
#### Amide side-chains.
Two amino acids have amide side-chains, as follows:
These side-chains do not ionize in the normal range of pH.
#### Sulfur-containing side-chains.
Two side-chains contain sulfur atoms, of which one ionizes in the normal range (with formula_1 indicated) and the other does not:
#### Aromatic side-chains.
Three amino acids have aromatic ring structures as side-chains, as illustrated. Of these, tyrosine ionizes in the normal range; the other two do not).
#### Anionic side-chains.
Two amino acids have side-chains that are anions at ordinary pH. Although the misnomer is so widespread as to be ineradicable, they should not be called "acidic amino acids", because they act as Brønsted bases in all circumstances except for enzymes like pepsin that act in environments of very low pH like the mammalian stomach.
#### Cationic side-chains.
There are three amino acids with side-chains that are cations at neutral pH (though in one, histidine, cationic and neutral forms both exist). They are commonly called "basic amino acids", but this term is misleading: histidine can act both as a Brønsted acid and as a Brønsted base at neutral pH, lysine acts as a Brønsted acid, and arginine has a fixed positive charge and does not ionize in neutral conditions. The names "histidinium, lysinium" and "argininium" would be more accurate names for the structures, but have essentially no currency.
#### β- and γ-amino acids.
Amino acids with the structure , such as β-alanine, a component of carnosine and a few other peptides, are β-amino acids. Ones with the structure are γ-amino acids, and so on, where X and Y are two substituents (one of which is normally H).
### Zwitterions.
In aqueous solution amino acids at moderate pH exist as zwitterions, i.e. as dipolar ions with both and in charged states, so the overall structure is . At physiological pH the so-called "neutral forms" are not present to any measurable degree. Although the two charges in the real structure add up to zero it is misleading and wrong to call a species with a net charge of zero "uncharged".
At very low pH (below 3), the caboxylate group becomes protonated and the structure becomes an ammonio carboxylic acid, . This is relevant for enzymes like pepsin that are active in acidic environments such as the mammalian stomach and lysosomes, but does not significantly apply to intracellular enzymes. At very high pH (greater than 10, not normally seen in physiological conditions), the ammonio group is deprotonated to give .
Although various definitions of acids and bases are used in chemistry, the only one that is useful for chemistry in aqueous solution is that of Brønsted: an acid is a species that can donate a proton to another species, and a base is one that can accept a proton. This criterion is used to label the groups in the above illustration. Notice that aspartate and glutamate are the principal groups that act as Brønsted bases, and the common references to these as "acidic amino acids" (together with the C terminal) is completely wrong and misleading. Likewise the so-called "basic amino acids" include one (histidine) that acts as both a Brønsted acid and a base, one (lysine) that acts primarily as a Brønsted acid, and one (arginine) that is normally irrelevant to acid-base behavior as it has a fixed positive charge. In addition, tyrosine and cysteine, which act primarily as acids at neutral pH, are usually forgotten in the usual classification.
### Isoelectric point.
For amino acids with uncharged side-chains the zwitterion predominates at pH values between the two p"K"a values, but coexists in equilibrium with small amounts of net negative and net positive ions. At the midpoint between the two p"K"a values, the trace amount of net negative and trace of net positive ions balance, so that average net charge of all forms present is zero. This pH is known as the isoelectric point p"I", so p"I" = (p"K"a1 + p"K"a2).
For amino acids with charged side chains, the p"K"a of the side chain is involved. Thus for aspartate or glutamate with negative side chains, the terminal amino group is essentially entirely in the charged form , but this positive charge needs to be balanced by the state with just one C-terminal carboxylate group is negatively charged. This occurs halfway between the two carboxylate p"K"a values: p"I" = (p"K"a1 + p"K"a(R)), where p"K"a(R) is the side chain p"K"a.
Similar considerations apply to other amino acids with ionizable side-chains, including not only glutamate (similar to aspartate), but also
cysteine, histidine, lysine, tyrosine and arginine with positive side chains
Amino acids have zero mobility in electrophoresis at their isoelectric point, although this behaviour is more usually exploited for peptides and proteins than single amino acids. Zwitterions have minimum solubility at their isoelectric point, and some amino acids (in particular, with nonpolar side chains) can be isolated by precipitation from water by adjusting the pH to the required isoelectric point.
## Physicochemical properties of amino acids.
The ca. 20 canonical amino acids can be classified according to their properties. Important factors are charge, hydrophilicity or hydrophobicity, size, and functional groups. These properties influence protein structure and protein–protein interactions. The water-soluble proteins tend to have their hydrophobic residues (Leu, Ile, Val, Phe, and Trp) buried in the middle of the protein, whereas hydrophilic side chains are exposed to the aqueous solvent. (Note that in biochemistry, a residue refers to a specific monomer within the polymeric chain of a polysaccharide, protein or nucleic acid.) The integral membrane proteins tend to have outer rings of exposed hydrophobic amino acids that anchor them into the lipid bilayer. Some peripheral membrane proteins have a patch of hydrophobic amino acids on their surface that locks onto the membrane. In similar fashion, proteins that have to bind to positively charged molecules have surfaces rich with negatively charged amino acids like glutamate and aspartate, while proteins binding to negatively charged molecules have surfaces rich with positively charged chains like lysine and arginine. For example, lysine and arginine are highly enriched in low complexity regions of nucleic-acid binding proteins. There are various hydrophobicity scales of amino acid residues.
Some amino acids have special properties such as cysteine, that can form covalent disulfide bonds to other cysteine residues, proline that forms a cycle to the polypeptide backbone, and glycine that is more flexible than other amino acids.
Furthermore, glycine and proline are highly enriched within low complexity regions of eukaryotic and prokaryotic proteins, whereas the opposite (under-represented) has been observed for highly reactive, or complex, or hydrophobic amino acids, such as cysteine, phenylalanine, tryptophan, methionine, valine, leucine, isoleucine.
Many proteins undergo a range of posttranslational modifications, whereby additional chemical groups are attached to the amino acid side chains. Some modifications can produce hydrophobic lipoproteins, or hydrophilic glycoproteins. These type of modification allow the reversible targeting of a protein to a membrane. For example, the addition and removal of the fatty acid palmitic acid to cysteine residues in some signaling proteins causes the proteins to attach and then detach from cell membranes.
### Table of standard amino acid abbreviations and properties.
Although 1-letter symbols are included in the table IUPAC–IUBMB recommendations refer to them as follows:
 Use of the one-letter symbols should be restricted to the comparison of long sequences
Two additional amino acids are in some species coded for by codons that are usually interpreted as stop codons:
In addition to the specific amino acid codes, placeholders are used in cases where chemical or crystallographic analysis of a peptide or protein cannot conclusively determine the identity of a residue. They are also used to summarise conserved protein sequence motifs. The use of single letters to indicate sets of similar residues is similar to the use of abbreviation codes for degenerate bases.
Unk is sometimes used instead of Xaa, but is less standard.
Ter or * (from termination) is used in notation for mutations in proteins when a stop codon occurs. It correspond to no amino acid at all.
In addition, many nonstandard amino acids have a specific code. For example, several peptide drugs, such as Bortezomib and MG132, are artificially synthesized and retain their protecting groups, which have specific codes. Bortezomib is Pyz–Phe–boroLeu, and MG132 is Z–Leu–Leu–Leu–al. To aid in the analysis of protein structure, photo-reactive amino acid analogs are available. These include photoleucine (pLeu) and photomethionine (pMet).
## Occurrence and functions in biochemistry.
Amino acids which have the amine group attached to the (alpha-) carbon atom next to the carboxyl group have particular importance. They are known as 2-, alpha-, or α-amino acids (generic formula in most cases, where R is an organic substituent known as a "side chain"); often the term "amino acid" is used to refer specifically to these. They include the 22 proteinogenic ("protein-building") amino acids, which combine into peptide chains ("polypeptides") to form the building blocks of a vast array of proteins. These are all -stereoisomers ("left-handed" enantiomers), although a few -amino acids ("right-handed") occur in bacterial envelopes, as a neuromodulator (-serine), and in some antibiotics.
Many proteinogenic and non-proteinogenic amino acids have biological functions. For example, in the human brain, glutamate (standard glutamic acid) and gamma-aminobutyric acid ("GABA", nonstandard gamma-amino acid) are, respectively, the main excitatory and inhibitory neurotransmitters. Hydroxyproline, a major component of the connective tissue collagen, is synthesised from proline. Glycine is a biosynthetic precursor to porphyrins used in red blood cells. Carnitine is used in lipid transport. Nine proteinogenic amino acids are called "essential" for humans because they cannot be produced from other compounds by the human body and so must be taken in as food. Others may be conditionally essential for certain ages or medical conditions. Essential amino acids may also vary from species to species. Because of their biological significance, amino acids are important in nutrition and are commonly used in nutritional supplements, fertilizers, feed, and food technology. Industrial uses include the production of drugs, biodegradable plastics, and chiral catalysts.
### Proteinogenic amino acids.
Amino acids are the precursors to proteins. They join by condensation reactions to form short polymer chains called peptides or longer chains called either polypeptides or proteins. These chains are linear and unbranched, with each amino acid residue within the chain attached to two neighboring amino acids. In Nature, the process of making proteins encoded by DNA/RNA genetic material is called "translation" and involves the step-by-step addition of amino acids to a growing protein chain by a ribozyme that is called a ribosome. The order in which the amino acids are added is read through the genetic code from an mRNA template, which is an RNA copy of one of the organism's genes.
Twenty-two amino acids are naturally incorporated into polypeptides and are called proteinogenic or natural amino acids. Of these, 20 are encoded by the universal genetic code. The remaining 2, selenocysteine and pyrrolysine, are incorporated into proteins by unique synthetic mechanisms. Selenocysteine is incorporated when the mRNA being translated includes a SECIS element, which causes the UGA codon to encode selenocysteine instead of a stop codon. Pyrrolysine is used by some methanogenic archaea in enzymes that they use to produce methane. It is coded for with the codon UAG, which is normally a stop codon in other organisms. This UAG codon is followed by a PYLIS downstream sequence.
Several independent evolutionary studies have suggested that Gly, Ala, Asp, Val, Ser, Pro, Glu, Leu, Thr may belong to a group of amino acids that constituted the early genetic code, whereas Cys, Met, Tyr, Trp, His, Phe may belong to a group of amino acids that constituted later additions of the genetic code.
### Standard vs nonstandard amino acids.
The 20 amino acids that are encoded directly by the codons of the universal genetic code are called "standard" or "canonical" amino acids. A modified form of methionine ("N"-formylmethionine) is often incorporated in place of methionine as the initial amino acid of proteins in bacteria, mitochondria and chloroplasts. Other amino acids are called "nonstandard" or "non-canonical". Most of the nonstandard amino acids are also non-proteinogenic (i.e. they cannot be incorporated into proteins during translation), but two of them are proteinogenic, as they can be incorporated translationally into proteins by exploiting information not encoded in the universal genetic code.
The two nonstandard proteinogenic amino acids are selenocysteine (present in many non-eukaryotes as well as most eukaryotes, but not coded directly by DNA) and pyrrolysine (found only in some archaea and at least one bacterium). The incorporation of these nonstandard amino acids is rare. For example, 25 human proteins include selenocysteine in their primary structure, and the structurally characterized enzymes (selenoenzymes) employ selenocysteine as the catalytic moiety in their active sites. Pyrrolysine and selenocysteine are encoded via variant codons. For example, selenocysteine is encoded by stop codon and SECIS element.
"N"-formylmethionine (which is often the initial amino acid of proteins in bacteria, mitochondria, and chloroplasts) is generally considered as a form of methionine rather than as a separate proteinogenic amino acid. Codon–tRNA combinations not found in nature can also be used to "expand" the genetic code and form novel proteins known as alloproteins incorporating non-proteinogenic amino acids.
### Non-proteinogenic amino acids.
Aside from the 22 proteinogenic amino acids, many "non-proteinogenic" amino acids are known. Those either are not found in proteins (for example carnitine, GABA, levothyroxine) or are not produced directly and in isolation by standard cellular machinery (for example, hydroxyproline and selenomethionine).
Non-proteinogenic amino acids that are found in proteins are formed by post-translational modification, which is modification after translation during protein synthesis. These modifications are often essential for the function or regulation of a protein. For example, the carboxylation of glutamate allows for better binding of calcium cations, and collagen contains hydroxyproline, generated by hydroxylation of proline. Another example is the formation of hypusine in the translation initiation factor EIF5A, through modification of a lysine residue. Such modifications can also determine the localization of the protein, e.g., the addition of long hydrophobic groups can cause a protein to bind to a phospholipid membrane.
Some non-proteinogenic amino acids are not found in proteins. Examples include 2-aminoisobutyric acid and the neurotransmitter gamma-aminobutyric acid. Non-proteinogenic amino acids often occur as intermediates in the metabolic pathways for standard amino acids – for example, ornithine and citrulline occur in the urea cycle, part of amino acid catabolism (see below). A rare exception to the dominance of α-amino acids in biology is the β-amino acid beta alanine (3-aminopropanoic acid), which is used in plants and microorganisms in the synthesis of pantothenic acid (vitamin B5), a component of coenzyme A.
### In human nutrition.
When taken up into the human body from the diet, the 20 standard amino acids either are used to synthesize proteins, other biomolecules, or are oxidized to urea and carbon dioxide as a source of energy. The oxidation pathway starts with the removal of the amino group by a transaminase; the amino group is then fed into the urea cycle. The other product of transamidation is a keto acid that enters the citric acid cycle. Glucogenic amino acids can also be converted into glucose, through gluconeogenesis. Of the 20 standard amino acids, nine (His, Ile, Leu, Lys, Met, Phe, Thr, Trp and Val) are called essential amino acids because the human body cannot synthesize them from other compounds at the level needed for normal growth, so they must be obtained from food. In addition, cysteine, tyrosine, and arginine are considered semiessential amino acids, and taurine a semiessential aminosulfonic acid in children. The metabolic pathways that synthesize these monomers are not fully developed. The amounts required also depend on the age and health of the individual, so it is hard to make general statements about the dietary requirement for some amino acids. Dietary exposure to the nonstandard amino acid BMAA has been linked to human neurodegenerative diseases, including ALS.
### Non-protein functions.
In humans, non-protein amino acids also have important roles as metabolic intermediates, such as in the biosynthesis of the neurotransmitter gamma-aminobutyric acid (GABA). Many amino acids are used to synthesize other molecules, for example:
Some nonstandard amino acids are used as defenses against herbivores in plants. For example, canavanine is an analogue of arginine that is found in many legumes, and in particularly large amounts in "Canavalia gladiata" (sword bean). This amino acid protects the plants from predators such as insects and can cause illness in people if some types of legumes are eaten without processing. The non-protein amino acid mimosine is found in other species of legume, in particular "Leucaena leucocephala". This compound is an analogue of tyrosine and can poison animals that graze on these plants.
## Uses in industry.
Amino acids are used for a variety of applications in industry, but their main use is as additives to animal feed. This is necessary, since many of the bulk components of these feeds, such as soybeans, either have low levels or lack some of the essential amino acids: lysine, methionine, threonine, and tryptophan are most important in the production of these feeds. In this industry, amino acids are also used to chelate metal cations in order to improve the absorption of minerals from supplements, which may be required to improve the health or productivity of these animals.
The food industry is also a major consumer of amino acids, in particular, glutamic acid, which is used as a flavor enhancer, and aspartame (aspartylphenylalanine 1-methyl ester) as a low-calorie artificial sweetener. Similar technology to that used for animal nutrition is employed in the human nutrition industry to alleviate symptoms of mineral deficiencies, such as anemia, by improving mineral absorption and reducing negative side effects from inorganic mineral supplementation.
The chelating ability of amino acids has been used in fertilizers for agriculture to facilitate the delivery of minerals to plants in order to correct mineral deficiencies, such as iron chlorosis. These fertilizers are also used to prevent deficiencies from occurring and to improve the overall health of the plants. The remaining production of amino acids is used in the synthesis of drugs and cosmetics.
Similarly, some amino acids derivatives are used in pharmaceutical industry. They include 5-HTP (5-hydroxytryptophan) used for experimental treatment of depression, -DOPA (-dihydroxyphenylalanine) for Parkinson's treatment, and eflornithine drug that inhibits ornithine decarboxylase and used in the treatment of sleeping sickness.
### Expanded genetic code.
Since 2001, 40 non-natural amino acids have been added into protein by creating a unique codon (recoding) and a corresponding transfer-RNA:aminoacyl – tRNA-synthetase pair to encode it with diverse physicochemical and biological properties in order to be used as a tool to exploring protein structure and function or to create novel or enhanced proteins.
### Nullomers.
Nullomers are codons that in theory code for an amino acid, however, in nature there is a selective bias against using this codon in favor of another, for example bacteria prefer to use CGA instead of AGA to code for arginine. This creates some sequences that do not appear in the genome. This characteristic can be taken advantage of and used to create new selective cancer-fighting drugs and to prevent cross-contamination of DNA samples from crime-scene investigations.
### Chemical building blocks.
Amino acids are important as low-cost feedstocks. These compounds are used in chiral pool synthesis as enantiomerically pure building blocks.
Amino acids have been investigated as precursors chiral catalysts, such as for asymmetric hydrogenation reactions, although no commercial applications exist.
### Biodegradable plastics.
Amino acids have been considered as components of biodegradable polymers, which have applications as environmentally friendly packaging and in medicine in drug delivery and the construction of prosthetic implants. An interesting example of such materials is polyaspartate, a water-soluble biodegradable polymer that may have applications in disposable diapers and agriculture. Due to its solubility and ability to chelate metal ions, polyaspartate is also being used as a biodegradeable antiscaling agent and a corrosion inhibitor. In addition, the aromatic amino acid tyrosine has been considered as a possible replacement for phenols such as bisphenol A in the manufacture of polycarbonates.
## Synthesis.
### Chemical synthesis.
The commercial production of amino acids usually relies on mutant bacteria that overproduce individual amino acids using glucose as a carbon source. Some amino acids are produced by enzymatic conversions of synthetic intermediates. 2-Aminothiazoline-4-carboxylic acid is an intermediate in one industrial synthesis of -cysteine for example. Aspartic acid is produced by the addition of ammonia to fumarate using a lyase.
### Biosynthesis.
In plants, nitrogen is first assimilated into organic compounds in the form of glutamate, formed from alpha-ketoglutarate and ammonia in the mitochondrion. For other amino acids, plants use transaminases to move the amino group from glutamate to another alpha-keto acid. For example, aspartate aminotransferase converts glutamate and oxaloacetate to alpha-ketoglutarate and aspartate. Other organisms use transaminases for amino acid synthesis, too.
Nonstandard amino acids are usually formed through modifications to standard amino acids. For example, homocysteine is formed through the transsulfuration pathway or by the demethylation of methionine via the intermediate metabolite "S"-adenosylmethionine, while hydroxyproline is made by a post translational modification of proline.
Microorganisms and plants synthesize many uncommon amino acids. For example, some microbes make 2-aminoisobutyric acid and lanthionine, which is a sulfide-bridged derivative of alanine. Both of these amino acids are found in peptidic lantibiotics such as alamethicin. However, in plants, 1-aminocyclopropane-1-carboxylic acid is a small disubstituted cyclic amino acid that is an intermediate in the production of the plant hormone ethylene.
## Reactions.
Amino acids undergo the reactions expected of the constituent functional groups.
### Peptide bond formation.
As both the amine and carboxylic acid groups of amino acids can react to form amide bonds, one amino acid molecule can react with another and become joined through an amide linkage. This polymerization of amino acids is what creates proteins. This condensation reaction yields the newly formed peptide bond and a molecule of water. In cells, this reaction does not occur directly; instead, the amino acid is first activated by attachment to a transfer RNA molecule through an ester bond. This aminoacyl-tRNA is produced in an ATP-dependent reaction carried out by an aminoacyl tRNA synthetase. This aminoacyl-tRNA is then a substrate for the ribosome, which catalyzes the attack of the amino group of the elongating protein chain on the ester bond. As a result of this mechanism, all proteins made by ribosomes are synthesized starting at their "N"-terminus and moving toward their "C"-terminus.
However, not all peptide bonds are formed in this way. In a few cases, peptides are synthesized by specific enzymes. For example, the tripeptide glutathione is an essential part of the defenses of cells against oxidative stress. This peptide is synthesized in two steps from free amino acids. In the first step, gamma-glutamylcysteine synthetase condenses cysteine and glutamic acid through a peptide bond formed between the side chain carboxyl of the glutamate (the gamma carbon of this side chain) and the amino group of the cysteine. This dipeptide is then condensed with glycine by glutathione synthetase to form glutathione.
In chemistry, peptides are synthesized by a variety of reactions. One of the most-used in solid-phase peptide synthesis uses the aromatic oxime derivatives of amino acids as activated units. These are added in sequence onto the growing peptide chain, which is attached to a solid resin support. Libraries of peptides are used in drug discovery through high-throughput screening.
The combination of functional groups allow amino acids to be effective polydentate ligands for metal–amino acid chelates.
The multiple side chains of amino acids can also undergo chemical reactions.
### Catabolism.
Degradation of an amino acid often involves deamination by moving its amino group to alpha-ketoglutarate, forming glutamate. This process involves transaminases, often the same as those used in amination during synthesis. In many vertebrates, the amino group is then removed through the urea cycle and is excreted in the form of urea. However, amino acid degradation can produce uric acid or ammonia instead. For example, serine dehydratase converts serine to pyruvate and ammonia. After removal of one or more amino groups, the remainder of the molecule can sometimes be used to synthesize new amino acids, or it can be used for energy by entering glycolysis or the citric acid cycle, as detailed in image at right.
### Complexation.
Amino acids are bidentate ligands, forming transition metal amino acid complexes. 
## Chemical analysis.
The total nitrogen content of organic matter is mainly formed by the amino groups in proteins. The Total Kjeldahl Nitrogen (TKN) is a measure of nitrogen widely used in the analysis of (waste) water, soil, food, feed and organic matter in general. As the name suggests, the Kjeldahl method is applied. More sensitive methods are available.

</doc>
<doc id="1208" url="https://en.wikipedia.org/wiki?curid=1208" title="Alan Turing">
Alan Turing

Alan Mathison Turing (; 23 June 1912 – 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence.
Born in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes–no questions can never be answered by computation and defined a Turing machine, and went on to prove the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School (GC&amp;CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.
After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine (ACE), one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory, at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, he was never fully recognised in his home country during his lifetime because much of his work was covered by the Official Secrets Act.
Turing was prosecuted in 1952 for homosexual acts. He accepted chemical castration treatment, with DES, as an alternative to prison. Turing died in 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning.
In 2009, following an Internet campaign, British Prime Minister Gordon Brown made an official public apology on behalf of the British government for "the appalling way he was treated". Queen Elizabeth II granted Turing a posthumous pardon in 2013. The "Alan Turing law" is now an informal term for a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts. Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations. He appears on the current Bank of England £50 note, which was released to coincide with his birthday. A , as voted by the audience, named him the greatest person of the 20th century.
## Early life and education.
### Family.
Turing was born in Maida Vale, London, while his father, Julius Mathison Turing (1873–1947), was on leave from his position with the Indian Civil Service (ICS) at Chatrapur, then in the Madras Presidency and presently in Odisha state, in India. Turing's father was the son of a clergyman, the Rev. John Robert Turing, from a Scottish family of merchants that had been based in the Netherlands and included a baronet. Turing's mother, Julius's wife, was Ethel Sara Turing (; 1881–1976), daughter of Edward Waller Stoney, chief engineer of the Madras Railways. The Stoneys were a Protestant Anglo-Irish gentry family from both County Tipperary and County Longford, while Ethel herself had spent much of her childhood in County Clare.
Julius's work with the ICS brought the family to British India, where his grandfather had been a general in the Bengal Army. However, both Julius and Ethel wanted their children to be brought up in Britain, so they moved to Maida Vale, London, where Alan Turing was born on 23 June 1912, as recorded by a blue plaque on the outside of the house of his birth, later the Colonnade Hotel. Turing had an elder brother, John (the father of Sir John Dermot Turing, 12th Baronet of the Turing baronets).
Turing's father's civil service commission was still active and during Turing's childhood years, his parents travelled between Hastings in the United Kingdom and India, leaving their two sons to stay with a retired Army couple. At Hastings, Turing stayed at Baston Lodge, Upper Maze Hill, St Leonards-on-Sea, now marked with a blue plaque. The plaque was unveiled on 23 June 2012, the centenary of Turing's birth.
Very early in life, Turing showed signs of the genius that he was later to display prominently. His parents purchased a house in Guildford in 1927, and Turing lived there during school holidays. The location is also marked with a blue plaque.
### School.
Turing's parents enrolled him at St Michael's, a primary school at 20 Charles Road, St Leonards-on-Sea, from the age of six to nine. The headmistress recognised his talent, noting that she has "...had clever boys and hardworking boys, but Alan is a genius."
Between January 1922 and 1926, Turing was educated at Hazelhurst Preparatory School, an independent school in the village of Frant in Sussex (now East Sussex). In 1926, at the age of 13, he went on to Sherborne School, a boarding independent school in the market town of Sherborne in Dorset, where he boarded at Westcott House. The first day of term coincided with the 1926 General Strike, in Britain, but Turing was so determined to attend, that he rode his bicycle unaccompanied from Southampton to Sherborne, stopping overnight at an inn.
Turing's natural inclination towards mathematics and science did not earn him respect from some of the teachers at Sherborne, whose definition of education placed more emphasis on the classics. His headmaster wrote to his parents: "I hope he will not fall between two stools. If he is to stay at public school, he must aim at becoming "educated". If he is to be solely a "Scientific Specialist", he is wasting his time at a public school". Despite this, Turing continued to show remarkable ability in the studies he loved, solving advanced problems in 1927 without having studied even elementary calculus. In 1928, aged 16, Turing encountered Albert Einstein's work; not only did he grasp it, but it is possible that he managed to deduce Einstein's questioning of Newton's laws of motion from a text in which this was never made explicit.
### Christopher Morcom.
At Sherborne, Turing formed a significant friendship with fellow pupil Christopher Collan Morcom (13 July 1911 – 13 February 1930), who has been described as Turing's "first love". Their relationship provided inspiration in Turing's future endeavours, but it was cut short by Morcom's death, in February 1930, from complications of bovine tuberculosis, contracted after drinking infected cow's milk some years previously.
The event caused Turing great sorrow. He coped with his grief by working that much harder on the topics of science and mathematics that he had shared with Morcom. In a letter to Morcom's mother, Frances Isobel Morcom (née Swan), Turing wrote:
Turing's relationship with Morcom's mother continued long after Morcom's death, with her sending gifts to Turing, and him sending letters, typically on Morcom's birthday. A day before the third anniversary of Morcom's death (13 February 1933), he wrote to Mrs. Morcom: 
Some have speculated that Morcom's death was the cause of Turing's atheism and materialism. Apparently, at this point in his life he still believed in such concepts as a spirit, independent of the body and surviving death. In a later letter, also written to Morcom's mother, Turing wrote: 
### University and work on computability.
After Sherborne, Turing studied as an undergraduate from 1931 to 1934 at King's College, Cambridge, where he was awarded first-class honours in mathematics. In 1935, at the age of 22, he was elected a Fellow of King's College on the strength of a dissertation in which he proved the central limit theorem. Unknown to the committee, the theorem had already been proven, in 1922, by Jarl Waldemar Lindeberg.
In 1936, Turing published his paper "On Computable Numbers, with an Application to the Entscheidungsproblem". It was published in the "Proceedings of the London Mathematical Society" journal in two parts, the first on 30 November and the second on 23 December. In this paper, Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. The "Entscheidungsproblem" (decision problem) was originally posed by German mathematician David Hilbert in 1928. Turing proved that his "universal computing machine" would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the "decision problem" by first showing that the halting problem for Turing machines is undecidable: it is not possible to decide algorithmically whether a Turing machine will ever halt. This paper has been called "easily the most influential math paper in history".
Although Turing's proof was published shortly after Alonzo Church's equivalent proof using his lambda calculus, Turing's approach is considerably more accessible and intuitive than Church's. It also included a notion of a 'Universal Machine' (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other computation machine (as indeed could Church's lambda calculus). According to the Church–Turing thesis, Turing machines and the lambda calculus are capable of computing anything that is computable. John von Neumann acknowledged that the central concept of the modern computer was due to Turing's paper. To this day, Turing machines are a central object of study in theory of computation.
From September 1936 to July 1938, Turing spent most of his time studying under Church at Princeton University, in the second year as a Jane Eliza Procter Visiting Fellow. In addition to his purely mathematical work, he studied cryptology and also built three of four stages of an electro-mechanical binary multiplier. In June 1938, he obtained his PhD from the Department of Mathematics at Princeton; his dissertation, "Systems of Logic Based on Ordinals", introduced the concept of ordinal logic and the notion of relative computing, in which Turing machines are augmented with so-called oracles, allowing the study of problems that cannot be solved by Turing machines. John von Neumann wanted to hire him as his postdoctoral assistant, but he went back to the United Kingdom.
## Career and research.
When Turing returned to Cambridge, he attended lectures given in 1939 by Ludwig Wittgenstein about the foundations of mathematics. The lectures have been reconstructed verbatim, including interjections from Turing and other students, from students' notes. Turing and Wittgenstein argued and disagreed, with Turing defending formalism and Wittgenstein propounding his view that mathematics does not discover any absolute truths, but rather invents them.
### Cryptanalysis.
During the Second World War, Turing was a leading participant in the breaking of German ciphers at Bletchley Park. The historian and wartime codebreaker Asa Briggs has said, "You needed exceptional talent, you needed genius at Bletchley and Turing's was that genius."
From September 1938, Turing worked part-time with the Government Code and Cypher School (GC&amp;CS), the British codebreaking organisation. He concentrated on cryptanalysis of the Enigma cipher machine used by Nazi Germany, together with Dilly Knox, a senior GC&amp;CS codebreaker. Soon after the July 1939 meeting near Warsaw at which the Polish Cipher Bureau gave the British and French details of the wiring of Enigma machine's rotors and their method of decrypting Enigma machine's messages, Turing and Knox developed a broader solution. The Polish method relied on an insecure indicator procedure that the Germans were likely to change, which they in fact did in May 1940. Turing's approach was more general, using crib-based decryption for which he produced the functional specification of the bombe (an improvement on the Polish Bomba).
On 4 September 1939, the day after the UK declared war on Germany, Turing reported to Bletchley Park, the wartime station of GC&amp;CS. Like all others who came to Bletchley, he was required to sign the Official Secrets Act, in which he agreed not to disclose anything about his work at Bletchley, with severe legal penalties for violating the Act.
Specifying the bombe was the first of five major cryptanalytical advances that Turing made during the war. The others were: deducing the indicator procedure used by the German navy; developing a statistical procedure dubbed "Banburismus" for making much more efficient use of the bombes; developing a procedure dubbed "Turingery" for working out the cam settings of the wheels of the Lorenz SZ 40/42 ("Tunny") cipher machine and, towards the end of the war, the development of a portable secure voice scrambler at Hanslope Park that was codenamed "Delilah".
By using statistical techniques to optimise the trial of different possibilities in the code breaking process, Turing made an innovative contribution to the subject. He wrote two papers discussing mathematical approaches, titled "The Applications of Probability to Cryptography" and "Paper on Statistics of Repetitions", which were of such value to GC&amp;CS and its successor GCHQ that they were not released to the UK National Archives until April 2012, shortly before the centenary of his birth. A GCHQ mathematician, "who identified himself only as Richard," said at the time that the fact that the contents had been restricted under the Official Secrets Act for some 70 years demonstrated their importance, and their relevance to post-war cryptanalysis: 
Turing had a reputation for eccentricity at Bletchley Park. He was known to his colleagues as "Prof" and his treatise on Enigma was known as the "Prof's Book". According to historian Ronald Lewin, Jack Good, a cryptanalyst who worked with Turing, said of his colleague:
Peter Hilton recounted his experience working with Turing in Hut 8 in his "Reminiscences of Bletchley Park" from "A Century of Mathematics in America:"
Hilton echoed similar thoughts in the Nova PBS documentary "Decoding Nazi Secrets".
While working at Bletchley, Turing, who was a talented long-distance runner, occasionally ran the to London when he was needed for meetings, and he was capable of world-class marathon standards. Turing tried out for the 1948 British Olympic team, but he was hampered by an injury. His tryout time for the marathon was only 11 minutes slower than British silver medallist Thomas Richards' Olympic race time of 2 hours 35 minutes. He was Walton Athletic Club's best runner, a fact discovered when he passed the group while running alone. When asked why he ran so hard in training he replied:
Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war. However, official war historian Harry Hinsley estimated that this work shortened the war in Europe by more than two years and saved over 14 million lives.
At the end of the war, a memo was sent to all those who had worked at Bletchley Park, reminding them that the code of silence dictated by the Official Secrets Act did not end with the war but would continue indefinitely. Thus, even though Turing was appointed an Officer of the Order of the British Empire (OBE) in 1946 by King George VI for his wartime services, his work remained secret for many years.
### Bombe.
Within weeks of arriving at Bletchley Park, Turing had specified an electromechanical machine called the bombe, which could break Enigma more effectively than the Polish "bomba kryptologiczna", from which its name was derived. The bombe, with an enhancement suggested by mathematician Gordon Welchman, became one of the primary tools, and the major automated one, used to attack Enigma-enciphered messages.
The bombe searched for possible correct settings used for an Enigma message (i.e., rotor order, rotor settings and plugboard settings) using a suitable "crib": a fragment of probable plaintext. For each possible setting of the rotors (which had on the order of 1019 states, or 1022 states for the four-rotor U-boat variant), the bombe performed a chain of logical deductions based on the crib, implemented electromechanically.
The bombe detected when a contradiction had occurred and ruled out that setting, moving on to the next. Most of the possible settings would cause contradictions and be discarded, leaving only a few to be investigated in detail. A contradiction would occur when an enciphered letter would be turned back into the same plaintext letter, which was impossible with the Enigma. The first bombe was installed on 18 March 1940.
By late 1941, Turing and his fellow cryptanalysts Gordon Welchman, Hugh Alexander and Stuart Milner-Barry were frustrated. Building on the work of the Poles, they had set up a good working system for decrypting Enigma signals, but their limited staff and bombes meant they could not translate all the signals. In the summer, they had considerable success, and shipping losses had fallen to under 100,000 tons a month; however, they badly needed more resources to keep abreast of German adjustments. They had tried to get more people and fund more bombes through the proper channels, but had failed.
On 28 October they wrote directly to Winston Churchill explaining their difficulties, with Turing as the first named. They emphasised how small their need was compared with the vast expenditure of men and money by the forces and compared with the level of assistance they could offer to the forces. As Andrew Hodges, biographer of Turing, later wrote, "This letter had an electric effect." Churchill wrote a memo to General Ismay, which read: "ACTION THIS DAY. Make sure they have all they want on extreme priority and report to me that this has been done." On 18 November, the chief of the secret service reported that every possible measure was being taken. The cryptographers at Bletchley Park did not know of the Prime Minister's response, but as Milner-Barry recalled, "All that we did notice was that almost from that day the rough ways began miraculously to be made smooth." More than two hundred bombes were in operation by the end of the war.
### Hut 8 and the naval Enigma.
Turing decided to tackle the particularly difficult problem of German naval Enigma "because no one else was doing anything about it and I could have it to myself". In December 1939, Turing solved the essential part of the naval indicator system, which was more complex than the indicator systems used by the other services.
That same night, he also conceived of the idea of "Banburismus", a sequential statistical technique (what Abraham Wald later called sequential analysis) to assist in breaking the naval Enigma, "though I was not sure that it would work in practice, and was not, in fact, sure until some days had actually broken." For this, he invented a measure of weight of evidence that he called the "ban". "Banburismus" could rule out certain sequences of the Enigma rotors, substantially reducing the time needed to test settings on the bombes. Later this sequential process of accumulating sufficient weight of evidence using decibans (one tenth of a ban) was used in Cryptanalysis of the Lorenz cipher.
Turing travelled to the United States in November 1942 and worked with US Navy cryptanalysts on the naval Enigma and bombe construction in Washington; he also visited their Computing Machine Laboratory in Dayton, Ohio.
Turing's reaction to the American bombe design was far from enthusiastic:
During this trip, he also assisted at Bell Labs with the development of secure speech devices. He returned to Bletchley Park in March 1943. During his absence, Hugh Alexander had officially assumed the position of head of Hut 8, although Alexander had been "de facto" head for some time (Turing having little interest in the day-to-day running of the section). Turing became a general consultant for cryptanalysis at Bletchley Park.
Alexander wrote of Turing's contribution:
### Turingery.
In July 1942, Turing devised a technique termed "Turingery" (or jokingly "Turingismus") for use against the Lorenz cipher messages produced by the Germans' new "Geheimschreiber" (secret writer) machine. This was a teleprinter rotor cipher attachment codenamed "Tunny" at Bletchley Park. Turingery was a method of "wheel-breaking", i.e., a procedure for working out the cam settings of Tunny's wheels. He also introduced the Tunny team to Tommy Flowers who, under the guidance of Max Newman, went on to build the Colossus computer, the world's first programmable digital electronic computer, which replaced a simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages. Some have mistakenly said that Turing was a key figure in the design of the Colossus computer. Turingery and the statistical approach of Banburismus undoubtedly fed into the thinking about cryptanalysis of the Lorenz cipher, but he was not directly involved in the Colossus development.
### Delilah.
Following his work at Bell Labs in the US, Turing pursued the idea of electronic enciphering of speech in the telephone system. In the latter part of the war, he moved to work for the Secret Service's Radio Security Service (later HMGCC) at Hanslope Park. At the park, he further developed his knowledge of electronics with the assistance of engineer Donald Bayley. Together they undertook the design and construction of a portable secure voice communications machine codenamed "Delilah". The machine was intended for different applications, but it lacked the capability for use with long-distance radio transmissions. In any case, Delilah was completed too late to be used during the war. Though the system worked fully, with Turing demonstrating it to officials by encrypting and decrypting a recording of a Winston Churchill speech, Delilah was not adopted for use. Turing also consulted with Bell Labs on the development of SIGSALY, a secure voice system that was used in the later years of the war.
### Early computers and the Turing test.
Between 1945 and 1947, Turing lived in Hampton, London, while he worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory (NPL). He presented a paper on 19 February 1946, which was the first detailed design of a stored-program computer. Von Neumann's incomplete "First Draft of a Report on the EDVAC" had predated Turing's paper, but it was much less detailed and, according to John R. Womersley, Superintendent of the NPL Mathematics Division, it "contains a number of ideas which are Dr. Turing's own".
Although ACE was a feasible design, the effect of the Official Secrets Act surrounding the wartime work at Bletchley Park made it impossible for Turing to explain the basis of his analysis of how a computer installation involving human operators would work. This led to delays in starting the project and he became disillusioned. In late 1947 he returned to Cambridge for a sabbatical year during which he produced a seminal work on "Intelligent Machinery" that was not published in his lifetime. While he was at Cambridge, the Pilot ACE was being built in his absence. It executed its first program on 10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until after his death.
According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Düsseldorf, there was a meeting between Turing and Konrad Zuse. It took place in Göttingen in 1947. The interrogation had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing (for more details see Herbert Bruderer, "Konrad Zuse und die Schweiz").
In 1948, Turing was appointed reader in the Mathematics Department at the Victoria University of Manchester. A year later, he became deputy director of the Computing Machine Laboratory, where he worked on software for one of the earliest stored-program computers—the Manchester Mark 1. Turing wrote the first version of the Programmer's Manual for this machine, and was recruited by Ferranti as a consultant in the development of their commercialised machine, the Ferranti Mark 1. He continued to be paid consultancy fees by Ferranti until his death. During this time, he continued to do more abstract work in mathematics,&lt;ref name="doi10.1093/qjmam/1.1.287"&gt;&lt;/ref&gt; and in "Computing Machinery and Intelligence" ("Mind", October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the Turing test, an attempt to define a standard for a machine to be called "intelligent". The idea was that a computer could be said to "think" if a human interrogator could not tell it apart, through conversation, from a human being. In the paper, Turing suggested that rather than building a program to simulate the adult mind, it would be better to produce a simpler one to simulate a child's mind and then to subject it to a course of education. A reversed form of the Turing test is widely used on the Internet; the CAPTCHA test is intended to determine whether the user is a human or a computer.
In 1948 Turing, working with his former undergraduate colleague, D.G. Champernowne, began writing a chess program for a computer that did not yet exist. By 1950, the program was completed and dubbed the Turochamp. In 1952, he tried to implement it on a Ferranti Mark 1, but lacking enough power, the computer was unable to execute the program. Instead, Turing "ran" the program by flipping through the pages of the algorithm and carrying out its instructions on a chessboard, taking about half an hour per move. The game was recorded. According to Garry Kasparov, Turing's program "played a recognizable game of chess." The program lost to Turing's colleague Alick Glennie, although it is said that it won a game against Champernowne's wife,
Isabel.
His Turing test was a significant, characteristically provocative, and lasting contribution to the debate regarding artificial intelligence, which continues after more than half a century.
### Pattern formation and mathematical biology.
When Turing was 39 years old in 1951, he turned to mathematical biology, finally publishing his masterpiece "The Chemical Basis of Morphogenesis" in January 1952. He was interested in morphogenesis, the development of patterns and shapes in biological organisms. He suggested that a system of chemicals reacting with each other and diffusing across space, termed a reaction–diffusion system, could account for "the main phenomena of morphogenesis". He used systems of partial differential equations to model catalytic chemical reactions. For example, if a catalyst A is required for a certain chemical reaction to take place, and if the reaction produced more of the catalyst A, then we say that the reaction is autocatalytic, and there is positive feedback that can be modelled by nonlinear differential equations. Turing discovered that patterns could be created if the chemical reaction not only produced catalyst A, but also produced an inhibitor B that slowed down the production of A. If A and B then diffused through the container at different rates, then you could have some regions where A dominated and some where B did. To calculate the extent of this, Turing would have needed a powerful computer, but these were not so freely available in 1951, so he had to use linear approximations to solve the equations by hand. These calculations gave the right qualitative results, and produced, for example, a uniform mixture that oddly enough had regularly spaced fixed red spots. The Russian biochemist Boris Belousov had performed experiments with similar results, but could not get his papers published because of the contemporary prejudice that any such thing violated the second law of thermodynamics. Belousov was not aware of Turing's paper in the "Philosophical Transactions of the Royal Society".
Although published before the structure and role of DNA was understood, Turing's work on morphogenesis remains relevant today and is considered a seminal piece of work in mathematical biology. One of the early applications of Turing's paper was the work by James Murray explaining spots and stripes on the fur of cats, large and small. Further research in the area suggests that Turing's work can partially explain the growth of "feathers, hair follicles, the branching pattern of lungs, and even the left-right asymmetry that puts the heart on the left side of the chest." In 2012, Sheth, et al. found that in mice, removal of Hox genes causes an increase in the number of digits without an increase in the overall size of the limb, suggesting that Hox genes control digit formation by tuning the wavelength of a Turing-type mechanism. Later papers were not available until "Collected Works of A. M. Turing" was published in 1992.
## Personal life.
### Engagement.
In 1941, Turing proposed marriage to Hut 8 colleague Joan Clarke, a fellow mathematician and cryptanalyst, but their engagement was short-lived. After admitting his homosexuality to his fiancée, who was reportedly "unfazed" by the revelation, Turing decided that he could not go through with the marriage.
### Conviction for indecency.
In January 1952, Turing was 39 when he started a relationship with Arnold Murray, a 19-year-old unemployed man. Just before Christmas, Turing was walking along Manchester's Oxford Road when he met Murray just outside the Regal Cinema and invited him to lunch. On 23 January, Turing's house was burgled. Murray told Turing that he and the burglar were acquainted, and Turing reported the crime to the police. During the investigation, he acknowledged a sexual relationship with Murray. Homosexual acts were criminal offences in the United Kingdom at that time, and both men were charged with "gross indecency" under Section 11 of the Criminal Law Amendment Act 1885. Initial committal proceedings for the trial were held on 27 February during which Turing's solicitor "reserved his defence", i.e., did not argue or provide evidence against the allegations.
Turing was later convinced by the advice of his brother and his own solicitor, and he entered a plea of guilty. The case, "Regina v. Turing and Murray," was brought to trial on 31 March 1952. Turing was convicted and given a choice between imprisonment and probation. His probation would be conditional on his agreement to undergo hormonal physical changes designed to reduce libido. He accepted the option of injections of what was then called stilboestrol (now known as diethylstilbestrol or DES), a synthetic oestrogen; this feminization of his body was continued for the course of one year. The treatment rendered Turing impotent and caused breast tissue to form, fulfilling in the literal sense Turing's prediction that "no doubt I shall emerge from it all a different man, but quite who I've not found out". Murray was given a conditional discharge.
Turing's conviction led to the removal of his security clearance and barred him from continuing with his cryptographic consultancy for the Government Communications Headquarters (GCHQ), the British signals intelligence agency that had evolved from GC&amp;CS in 1946, though he kept his academic job. He was denied entry into the United States after his conviction in 1952, but was free to visit other European countries.
### Death.
On 8 June 1954, at his house at 43 Adlington Road, Wilmslow, Turing's housekeeper found him dead. He had died the previous day at the age of 41. Cyanide poisoning was established as the cause of death. When his body was discovered, an apple lay half-eaten beside his bed, and although the apple was not tested for cyanide, it was speculated that this was the means by which Turing had consumed a fatal dose. An inquest determined that he had committed suicide. Andrew Hodges and another biographer, David Leavitt, have both speculated that Turing was re-enacting a scene from the Walt Disney film "Snow White and the Seven Dwarfs" (1937), his favourite fairy tale. Both men noted that (in Leavitt's words) he took "an especially keen pleasure in the scene where the Wicked Queen immerses her apple in the poisonous brew". Turing's remains were cremated at Woking Crematorium on 12 June 1954, and his ashes were scattered in the gardens of the crematorium, just as his father's had been.
Philosopher Jack Copeland has questioned various aspects of the coroner's historical verdict. He suggested an alternative explanation for the cause of Turing's death: the accidental inhalation of cyanide fumes from an apparatus used to electroplate gold onto spoons. The potassium cyanide was used to dissolve the gold. Turing had such an apparatus set up in his tiny spare room. Copeland noted that the autopsy findings were more consistent with inhalation than with ingestion of the poison. Turing also habitually ate an apple before going to bed, and it was not unusual for the apple to be discarded half-eaten. Furthermore, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) "with good humour" and had shown no sign of despondency prior to his death. He even set down a list of tasks that he intended to complete upon returning to his office after the holiday weekend. Turing's mother believed that the ingestion was accidental, resulting from her son's careless storage of laboratory chemicals. Biographer Andrew Hodges theorised that Turing arranged the delivery of the equipment to deliberately allow his mother plausible deniability with regard to any suicide claims.
It has been suggested that Turing's belief in fortune-telling may have caused his depressed mood. As a youth, Turing had been told by a fortune-teller that he would be a genius. In mid-May 1954, shortly before his death, Turing again decided to consult a fortune-teller during a day-trip to St Annes-on-Sea with the Greenbaum family. According to the Greenbaums' daughter, Barbara:
But it was a lovely sunny day and Alan was in a cheerful mood and off we went... Then he thought it would be a good idea to go to the Pleasure Beach at Blackpool. We found a fortune-teller's tent[,] and Alan said he'd like to go in[,] so we waited around for him to come back... And this sunny, cheerful visage had shrunk into a pale, shaking, horror-stricken face. Something had happened. We don't know what the fortune-teller said[,] but he obviously was deeply unhappy. I think that was probably the last time we saw him before we heard of his suicide.
### Government apology and pardon.
In August 2009, British programmer John Graham-Cumming started a petition urging the British government to apologise for Turing's prosecution as a homosexual. The petition received more than 30,000 signatures. The Prime Minister, Gordon Brown, acknowledged the petition, releasing a statement on 10 September 2009 apologising and describing the treatment of Turing as "appalling":
In December 2011, William Jones and his Member of Parliament, John Leech, created an e-petition requesting that the British government pardon Turing for his conviction of "gross indecency":
The petition gathered over 37,000 signatures, and was submitted to Parliament by the Manchester MP John Leech but the request was discouraged by Justice Minister Lord McNally, who said:
John Leech, the MP for Manchester Withington (2005–15), submitted several bills to Parliament and led a high-profile campaign to secure the pardon. Leech made the case in the House of Commons that Turing's contribution to the war made him a national hero and that it was "ultimately just embarrassing" that the conviction still stood. Leech continued to take the bill through Parliament and campaigned for several years, gaining the public support of numerous leading scientists, including Stephen Hawking. At the British premiere of a film based on Turing's life, "The Imitation Game", the producers thanked Leech for bringing the topic to public attention and securing Turing's pardon. Leech is now regularly described as the "architect" of Turing's pardon and subsequently the Alan Turing Law which went on to secure pardons for 75,000 other men and women convicted of similar crimes.
On 26 July 2012, a bill was introduced in the House of Lords to grant a statutory pardon to Turing for offences under section 11 of the Criminal Law Amendment Act 1885, of which he was convicted on 31 March 1952. Late in the year in a letter to "The Daily Telegraph", the physicist Stephen Hawking and 10 other signatories including the Astronomer Royal Lord Rees, President of the Royal Society Sir Paul Nurse, Lady Trumpington (who worked for Turing during the war) and Lord Sharkey (the bill's sponsor) called on Prime Minister David Cameron to act on the pardon request. The government indicated it would support the bill, and it passed its third reading in the House of Lords in October.
At the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage. The bill was due to return to the House of Commons on 28 February 2014, but before the bill could be debated in the House of Commons, the government elected to proceed under the royal prerogative of mercy. On 24 December 2013, Queen Elizabeth II signed a pardon for Turing's conviction for "gross indecency", with immediate effect. Announcing the pardon, Lord Chancellor Chris Grayling said Turing deserved to be "remembered and recognised for his fantastic contribution to the war effort" and not for his later criminal conviction. The Queen officially pronounced Turing pardoned in August 2014. The Queen's action is only the fourth royal pardon granted since the conclusion of the Second World War. Pardons are normally granted only when the person is technically innocent, and a request has been made by the family or other interested party; neither condition was met in regard to Turing's conviction.
In September 2016, the government announced its intention to expand this retroactive exoneration to other men convicted of similar historical indecency offences, in what was described as an "Alan Turing law". The Alan Turing law is now an informal term for the law in the United Kingdom, contained in the Policing and Crime Act 2017, which serves as an amnesty law to retroactively pardon men who were cautioned or convicted under historical legislation that outlawed homosexual acts. The law applies in England and Wales.
## Legacy.
### Awards, honours, and tributes.
Turing was appointed an officer of the Order of the British Empire in 1946. He was also elected a Fellow of the Royal Society (FRS) in 1951.
Turing has been honoured in various ways in Manchester, the city where he worked towards the end of his life. In 1994, a stretch of the A6010 road (the Manchester city intermediate ring road) was named "Alan Turing Way". A bridge carrying this road was widened, and carries the name Alan Turing Bridge. A statue of Turing was unveiled in Manchester on 23 June 2001 in Sackville Park, between the University of Manchester building on Whitworth Street and Canal Street. The memorial statue depicts the "father of computer science" sitting on a bench at a central position in the park. Turing is shown holding an apple. The cast bronze bench carries in relief the text 'Alan Mathison Turing 1912–1954', and the motto 'Founder of Computer Science' as it could appear if encoded by an Enigma machine: 'IEKYF ROMSI ADXUO KVKZC GUBJ'. However, the meaning of the coded message is disputed, as the 'u' in 'computer' matches up with the 'u' in 'ADXUO'. As a letter encoded by an enigma machine cannot appear as itself, the actual message behind the code is uncertain.
A plaque at the statue's feet reads 'Father of computer science, mathematician, logician, wartime codebreaker, victim of prejudice'. There is also a Bertrand Russell quotation: "Mathematics, rightly viewed, possesses not only truth, but supreme beauty—a beauty cold and austere, like that of sculpture." The sculptor buried his own old Amstrad computer under the plinth as a tribute to "the godfather of all modern computers".
In 1999, "Time" magazine named Turing as one of the and stated, "The fact remains that everyone who taps at a keyboard, opening a spreadsheet or a word-processing program, is working on an incarnation of a Turing machine."
A blue plaque was unveiled at King's College on the centenary of his birth on 23 June 2012 and is now installed at the college's Keynes Building on King's Parade.
On 25 March 2021, the Bank of England publicly unveiled the design for a new £50 note, featuring Turing's portrait, before its official issue on 23 June, Turing's birthday. Turing was selected as the new face of the note in 2019 following a public nomination process.
### Centenary celebrations.
To mark the 100th anniversary of Turing's birth, the Turing Centenary Advisory Committee (TCAC) co-ordinated the Alan Turing Year in 2012, a year-long programme of events around the world honouring Turing's life and achievements. The TCAC, chaired by S. Barry Cooper with Turing's nephew Sir John Dermot Turing acting as Honorary President, worked with the University of Manchester faculty members and a broad spectrum of people from Cambridge University and Bletchley Park.
### Steel sculpture controversy.
In May 2020 it was reported by "Gay Star News" that a high steel sculpture, to honour Turing, designed by Sir Antony Gormley, was planned to be installed at King's College, Cambridge. Historic England, however, was quoted as saying that the abstract work of 19 steel slabs "... would be at odds with the existing character of the College. This would result in harm, of a less than substantial nature, to the significance of the listed buildings and landscape, and by extension the conservation area."

</doc>
<doc id="1209" url="https://en.wikipedia.org/wiki?curid=1209" title="Area">
Area

Area is the quantity that expresses the extent of a two-dimensional region, shape, or planar lamina, in the plane. Surface area is its analog on the two-dimensional surface of a three-dimensional object. Area can be understood as the amount of material with a given thickness that would be necessary to fashion a model of the shape, or the amount of paint necessary to cover the surface with a single coat. It is the two-dimensional analog of the length of a curve (a one-dimensional concept) or the volume of a solid (a three-dimensional concept).
The area of a shape can be measured by comparing the shape to squares of a fixed size. In the International System of Units (SI), the standard unit of area is the square metre (written as m2), which is the area of a square whose sides are one metre long. A shape with an area of three square metres would have the same area as three such squares. In mathematics, the unit square is defined to have area one, and the area of any other shape or surface is a dimensionless real number.
There are several well-known formulas for the areas of simple shapes such as triangles, rectangles, and circles. Using these formulas, the area of any polygon can be found by dividing the polygon into triangles. For shapes with curved boundary, calculus is usually required to compute the area. Indeed, the problem of determining the area of plane figures was a major motivation for the historical development of calculus.
For a solid shape such as a sphere, cone, or cylinder, the area of its boundary surface is called the surface area. Formulas for the surface areas of simple shapes were computed by the ancient Greeks, but computing the surface area of a more complicated shape usually requires multivariable calculus.
Area plays an important role in modern mathematics. In addition to its obvious importance in geometry and calculus, area is related to the definition of determinants in linear algebra, and is a basic property of surfaces in differential geometry. In analysis, the area of a subset of the plane is defined using Lebesgue measure, though not every subset is measurable. In general, area in higher mathematics is seen as a special case of volume for two-dimensional regions.
Area can be defined through the use of axioms, defining it as a function of a collection of certain plane figures to the set of real numbers. It can be proved that such a function exists.
## Formal definition.
An approach to defining what is meant by "area" is through axioms. "Area" can be defined as a function from a collection M of a special kinds of plane figures (termed measurable sets) to the set of real numbers, which satisfies the following properties:
It can be proved that such an area function actually exists.
## Units.
Every unit of length has a corresponding unit of area, namely the area of a square with the given side length. Thus areas can be measured in square metres (m2), square centimetres (cm2), square millimetres (mm2), square kilometres (km2), square feet (ft2), square yards (yd2), square miles (mi2), and so forth. Algebraically, these units can be thought of as the squares of the corresponding length units.
The SI unit of area is the square metre, which is considered an SI derived unit.
### Conversions.
Calculation of the area of a square whose length and width are 1 metre would be:
1 metre × 1 metre = 1 m2
and so, a rectangle with different sides (say length of 3 metres and width of 2 metres) would have an area in square units that can be calculated as:
3 metres × 2 metres = 6 m2. This is equivalent to 6 million square millimetres. Other useful conversions are:
#### Non-metric units.
In non-metric units, the conversion between two square units is the square of the conversion between the corresponding length units.
the relationship between square feet and square inches is
where 144 = 122 = 12 × 12. Similarly:
In addition, conversion factors include:
### Other units including historical.
There are several other common units for area. The are was the original unit of area in the metric system, with:
Though the are has fallen out of use, the hectare is still commonly used to measure land:
Other uncommon metric units of area include the tetrad, the hectad, and the myriad.
The acre is also commonly used to measure land areas, where
An acre is approximately 40% of a hectare.
On the atomic scale, area is measured in units of barns, such that:
The barn is commonly used in describing the cross-sectional area of interaction in nuclear physics.
In India,
## History.
### Circle area.
In the 5th century BCE, Hippocrates of Chios was the first to show that the area of a disk (the region enclosed by a circle) is proportional to the square of its diameter, as part of his quadrature of the lune of Hippocrates, but did not identify the constant of proportionality. Eudoxus of Cnidus, also in the 5th century BCE, also found that the area of a disk is proportional to its radius squared.
Subsequently, Book I of Euclid's "Elements" dealt with equality of areas between two-dimensional figures. The mathematician Archimedes used the tools of Euclidean geometry to show that the area inside a circle is equal to that of a right triangle whose base has the length of the circle's circumference and whose height equals the circle's radius, in his book "Measurement of a Circle". (The circumference is 2"r", and the area of a triangle is half the base times the height, yielding the area "r"2 for the disk.) Archimedes approximated the value of π (and hence the area of a unit-radius circle) with his doubling method, in which he inscribed a regular triangle in a circle and noted its area, then doubled the number of sides to give a regular hexagon, then repeatedly doubled the number of sides as the polygon's area got closer and closer to that of the circle (and did the same with circumscribed polygons).
Swiss scientist Johann Heinrich Lambert in 1761 proved that π, the ratio of a circle's area to its squared radius, is irrational, meaning it is not equal to the quotient of any two whole numbers. In 1794 French mathematician Adrien-Marie Legendre proved that π2 is irrational; this also proves that π is irrational. In 1882, German mathematician Ferdinand von Lindemann proved that π is transcendental (not the solution of any polynomial equation with rational coefficients), confirming a conjecture made by both Legendre and Euler.
### Triangle area.
Heron (or Hero) of Alexandria found what is known as Heron's formula for the area of a triangle in terms of its sides, and a proof can be found in his book, "Metrica", written around 60 CE. It has been suggested that Archimedes knew the formula over two centuries earlier, and since "Metrica" is a collection of the mathematical knowledge available in the ancient world, it is possible that the formula predates the reference given in that work.
In 499 Aryabhata, a great mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, expressed the area of a triangle as one-half the base times the height in the "Aryabhatiya" (section 2.6).
A formula equivalent to Heron's was discovered by the Chinese independently of the Greeks. It was published in 1247 in "Shushu Jiuzhang" ("Mathematical Treatise in Nine Sections"), written by Qin Jiushao.
### Quadrilateral area.
In the 7th century CE, Brahmagupta developed a formula, now known as Brahmagupta's formula, for the area of a cyclic quadrilateral (a quadrilateral inscribed in a circle) in terms of its sides. In 1842 the German mathematicians Carl Anton Bretschneider and Karl Georg Christian von Staudt independently found a formula, known as Bretschneider's formula, for the area of any quadrilateral.
### General polygon area.
The development of Cartesian coordinates by René Descartes in the 17th century allowed the development of the surveyor's formula for the area of any polygon with known vertex locations by Gauss in the 19th century.
### Areas determined using calculus.
The development of integral calculus in the late 17th century provided tools that could subsequently be used for computing more complicated areas, such as the area of an ellipse and the surface areas of various curved three-dimensional objects.
## Area formulas.
### Polygon formulas.
For a non-self-intersecting (simple) polygon, the Cartesian coordinates formula_1 ("i"=0, 1, ..., "n"-1) of whose "n" vertices are known, the area is given by the surveyor's formula:
where when "i"="n"-1, then "i"+1 is expressed as modulus "n" and so refers to 0.
#### Rectangles.
The most basic area formula is the formula for the area of a rectangle. Given a rectangle with length and width , the formula for the area is:
That is, the area of the rectangle is the length multiplied by the width. As a special case, as in the case of a square, the area of a square with side length is given by the formula:
The formula for the area of a rectangle follows directly from the basic properties of area, and is sometimes taken as a definition or axiom. On the other hand, if geometry is developed before arithmetic, this formula can be used to define multiplication of real numbers.
#### Dissection, parallelograms, and triangles.
Most other simple formulas for area follow from the method of dissection.
This involves cutting a shape into pieces, whose areas must sum to the area of the original shape.
For an example, any parallelogram can be subdivided into a trapezoid and a right triangle, as shown in figure to the left. If the triangle is moved to the other side of the trapezoid, then the resulting figure is a rectangle. It follows that the area of the parallelogram is the same as the area of the rectangle:
However, the same parallelogram can also be cut along a diagonal into two congruent triangles, as shown in the figure to the right. It follows that the area of each triangle is half the area of the parallelogram:
Similar arguments can be used to find area formulas for the trapezoid as well as more complicated polygons.
### Area of curved shapes.
#### Circles.
The formula for the area of a circle (more properly called the area enclosed by a circle or the area of a disk) is based on a similar method. Given a circle of radius , it is possible to partition the circle into sectors, as shown in the figure to the right. Each sector is approximately triangular in shape, and the sectors can be rearranged to form an approximate parallelogram. The height of this parallelogram is , and the width is half the circumference of the circle, or . Thus, the total area of the circle is :
Though the dissection used in this formula is only approximate, the error becomes smaller and smaller as the circle is partitioned into more and more sectors. The limit of the areas of the approximate parallelograms is exactly , which is the area of the circle.
This argument is actually a simple application of the ideas of calculus. In ancient times, the method of exhaustion was used in a similar way to find the area of the circle, and this method is now recognized as a precursor to integral calculus. Using modern methods, the area of a circle can be computed using a definite integral:
#### Ellipses.
The formula for the area enclosed by an ellipse is related to the formula of a circle; for an ellipse with semi-major and semi-minor axes and the formula is:
#### Surface area.
Most basic formulas for surface area can be obtained by cutting surfaces and flattening them out. For example, if the side surface of a cylinder (or any prism) is cut lengthwise, the surface can be flattened out into a rectangle. Similarly, if a cut is made along the side of a cone, the side surface can be flattened out into a sector of a circle, and the resulting area computed.
The formula for the surface area of a sphere is more difficult to derive: because a sphere has nonzero Gaussian curvature, it cannot be flattened out. The formula for the surface area of a sphere was first obtained by Archimedes in his work "On the Sphere and Cylinder". The formula is:
where is the radius of the sphere. As with the formula for the area of a circle, any derivation of this formula inherently uses methods similar to calculus.
### General formulas.
#### Bounded area between two quadratic functions.
To find the bounded area between two quadratic functions, we subtract one from the other to write the difference as
where "f"("x") is the quadratic upper bound and "g"("x") is the quadratic lower bound. Define the discriminant of "f"("x")-"g"("x") as
By simplifying the integral formula between the graphs of two functions (as given in the section above) and using Vieta's formula, we can obtain
The above remains valid if one of the bounding functions is linear instead of quadratic.
#### General formula for surface area.
The general formula for the surface area of the graph of a continuously differentiable function formula_36 where formula_37 and formula_38 is a region in the xy-plane with the smooth boundary:
An even more general formula for the area of the graph of a parametric surface in the vector form formula_40 where formula_41 is a continuously differentiable vector function of formula_42 is:
### List of formulas.
The above calculations show how to find the areas of many common shapes.
The areas of irregular (and thus arbitrary) polygons can be calculated using the "Surveyor's formula" (shoelace formula).
### Relation of area to perimeter.
The isoperimetric inequality states that, for a closed curve of length "L" (so the region it encloses has perimeter "L") and for area "A" of the region that it encloses,
and equality holds if and only if the curve is a circle. Thus a circle has the largest area of any closed figure with a given perimeter.
At the other extreme, a figure with given perimeter "L" could have an arbitrarily small area, as illustrated by a rhombus that is "tipped over" arbitrarily far so that two of its angles are arbitrarily close to 0° and the other two are arbitrarily close to 180°.
For a circle, the ratio of the area to the circumference (the term for the perimeter of a circle) equals half the radius "r". This can be seen from the area formula "πr"2 and the circumference formula 2"πr".
The area of a regular polygon is half its perimeter times the apothem (where the apothem is the distance from the center to the nearest point on any side).
### Fractals.
Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). But if the one-dimensional lengths of a fractal drawn in two dimensions are all doubled, the spatial content of the fractal scales by a power of two that is not necessarily an integer. This power is called the fractal dimension of the fractal.
## Area bisectors.
There are an infinitude of lines that bisect the area of a triangle. Three of them are the medians of the triangle (which connect the sides' midpoints with the opposite vertices), and these are concurrent at the triangle's centroid; indeed, they are the only area bisectors that go through the centroid. Any line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter (the center of its incircle). There are either one, two, or three of these for any given triangle.
Any line through the midpoint of a parallelogram bisects the area.
All area bisectors of a circle or other ellipse go through the center, and any chords through the center bisect the area. In the case of a circle they are the diameters of the circle.
## Optimization.
Given a wire contour, the surface of least area spanning ("filling") it is a minimal surface. Familiar examples include soap bubbles.
The question of the filling area of the Riemannian circle remains open.
The circle has the largest area of any two-dimensional object having the same perimeter.
A cyclic polygon (one inscribed in a circle) has the largest area of any polygon with a given number of sides of the same lengths.
A version of the isoperimetric inequality for triangles states that the triangle of greatest area among all those with a given perimeter is equilateral.
The triangle of largest area of all those inscribed in a given circle is equilateral; and the triangle of smallest area of all those circumscribed around a given circle is equilateral.
The ratio of the area of the incircle to the area of an equilateral triangle, formula_45, is larger than that of any non-equilateral triangle.
The ratio of the area to the square of the perimeter of an equilateral triangle, formula_46 is larger than that for any other triangle.

</doc>
<doc id="1210" url="https://en.wikipedia.org/wiki?curid=1210" title="Astronomical unit">
Astronomical unit

The astronomical unit (symbol: au, or or AU) is a unit of length, roughly the distance from Earth to the Sun and equal to about or ~8 light minutes. The actual distance from Earth to the Sun varies by about 3% as Earth orbits the Sun, from a maximum (aphelion) to a minimum (perihelion) and back again once each year. The astronomical unit was originally conceived as the average of Earth's aphelion and perihelion; however, since 2012 it has been defined as exactly (see below for several conversions).
The astronomical unit is used primarily for measuring distances within the Solar System or around other stars. It is also a fundamental component in the definition of another unit of astronomical length, the parsec.
## History of symbol usage.
A variety of unit symbols and abbreviations have been in use for the astronomical unit. In a 1976 resolution, the International Astronomical Union (IAU) had used the symbol "A" to denote a length equal to the astronomical unit. In the astronomical literature, the symbol AU was (and remains) common. In 2006, the International Bureau of Weights and Measures (BIPM) had recommended ua as the symbol for the unit. In the non-normative Annex C to ISO 80000-3:2006 (now withdrawn), the symbol of the astronomical unit was "ua".
In 2012, the IAU, noting "that various symbols are presently in use for the astronomical unit", recommended the use of the symbol "au". The scientific journals published by the American Astronomical Society and the Royal Astronomical Society subsequently adopted this symbol. In the 2014 revision and 2019 edition of the SI Brochure, the BIPM used the unit symbol "au". ISO 80000-3:2019, which replaces ISO 80000-3:2006, does not mention the astronomical unit.
## Development of unit definition.
Earth's orbit around the Sun is an ellipse. The semi-major axis of this elliptic orbit is defined to be half of the straight line segment that joins the perihelion and aphelion. The centre of the Sun lies on this straight line segment, but not at its midpoint. Because ellipses are well-understood shapes, measuring the points of its extremes defined the exact shape mathematically, and made possible calculations for the entire orbit as well as predictions based on observation. In addition, it mapped out exactly the largest straight-line distance that Earth traverses over the course of a year, defining times and places for observing the largest parallax (apparent shifts of position) in nearby stars. Knowing Earth's shift and a star's shift enabled the star's distance to be calculated. But all measurements are subject to some degree of error or uncertainty, and the uncertainties in the length of the astronomical unit only increased uncertainties in the stellar distances. Improvements in precision have always been a key to improving astronomical understanding. Throughout the twentieth century, measurements became increasingly precise and sophisticated, and ever more dependent on accurate observation of the effects described by Einstein's theory of relativity and upon the mathematical tools it used.
Improving measurements were continually checked and cross-checked by means of improved understanding of the laws of celestial mechanics, which govern the motions of objects in space. The expected positions and distances of objects at an established time are calculated (in au) from these laws, and assembled into a collection of data called an ephemeris. NASA Jet Propulsion Laboratory HORIZONS System provides one of several ephemeris computation services.
In 1976, to establish an even precise measure for the astronomical unit, the IAU formally adopted a new definition. Although directly based on the then-best available observational measurements, the definition was recast in terms of the then-best mathematical derivations from celestial mechanics and planetary ephemerides. It stated that "the astronomical unit of length is that length ("A") for which the Gaussian gravitational constant ("k") takes the value when the units of measurement are the astronomical units of length, mass and time". Equivalently, by this definition, one au is "the radius of an unperturbed circular Newtonian orbit about the sun of a particle having infinitesimal mass, moving with an angular frequency of "; or alternatively that length for which the heliocentric gravitational constant (the product "G") is equal to ()2 au3/d2, when the length is used to describe the positions of objects in the Solar System.
Subsequent explorations of the Solar System by space probes made it possible to obtain precise measurements of the relative positions of the inner planets and other objects by means of radar and telemetry. As with all radar measurements, these rely on measuring the time taken for photons to be reflected from an object. Because all photons move at the speed of light in vacuum, a fundamental constant of the universe, the distance of an object from the probe is calculated as the product of the speed of light and the measured time. However, for precision the calculations require adjustment for things such as the motions of the probe and object while the photons are transiting. In addition, the measurement of the time itself must be translated to a standard scale that accounts for relativistic time dilation. Comparison of the ephemeris positions with time measurements expressed in Barycentric Dynamical Time (TDB) leads to a value for the speed of light in astronomical units per day (of ). By 2009, the IAU had updated its standard measures to reflect improvements, and calculated the speed of light at (TDB).
In 1983, the CIPM modified the International System of Units (SI) to make the metre defined as the distance travelled in a vacuum by light in 1 /  second. This replaced the previous definition, valid between 1960 and 1983, which was that the metre equalled a certain number of wavelengths of a certain emission line of krypton-86. (The reason for the change was an improved method of measuring the speed of light.) The speed of light could then be expressed exactly as "c"0 = , a standard also adopted by the IERS numerical standards. From this definition and the 2009 IAU standard, the time for light to traverse an astronomical unit is found to be "τ"A = , which is slightly more than 8 minutes 19 seconds. By multiplication, the best IAU 2009 estimate was "A" = "c"0"τ"A = , based on a comparison of Jet Propulsion Laboratory and IAA–RAS ephemerides.
In 2006, the BIPM reported a value of the astronomical unit as . In the 2014 revision of the SI Brochure, the BIPM recognised the IAU's 2012 redefinition of the astronomical unit as .
This estimate was still derived from observation and measurements subject to error, and based on techniques that did not yet standardize all relativistic effects, and thus were not constant for all observers. In 2012, finding that the equalization of relativity alone would make the definition overly complex, the IAU simply used the 2009 estimate to redefine the astronomical unit as a conventional unit of length directly tied to the metre (exactly ). The new definition also recognizes as a consequence that the astronomical unit is now to play a role of reduced importance, limited in its use to that of a convenience in some applications.
This definition makes the speed of light, defined as exactly , equal to exactly  ×  ÷  or about  au/d, some 60 parts per trillion less than the 2009 estimate.
## Usage and significance.
With the definitions used before 2012, the astronomical unit was dependent on the heliocentric gravitational constant, that is the product of the gravitational constant, "G", and the solar mass, . Neither "G" nor can be measured to high accuracy separately, but the value of their product is known very precisely from observing the relative positions of planets (Kepler's Third Law expressed in terms of Newtonian gravitation). Only the product is required to calculate planetary positions for an ephemeris, so ephemerides are calculated in astronomical units and not in SI units.
The calculation of ephemerides also requires a consideration of the effects of general relativity. In particular, time intervals measured on Earth's surface (Terrestrial Time, TT) are not constant when compared with the motions of the planets: the terrestrial second (TT) appears to be longer near January and shorter near July when compared with the "planetary second" (conventionally measured in TDB). This is because the distance between Earth and the Sun is not fixed (it varies between and ) and, when Earth is closer to the Sun (perihelion), the Sun's gravitational field is stronger and Earth is moving faster along its orbital path. As the metre is defined in terms of the second and the speed of light is constant for all observers, the terrestrial metre appears to change in length compared with the "planetary metre" on a periodic basis.
The metre is defined to be a unit of proper length, but the SI definition does not specify the metric tensor to be used in determining it. Indeed, the International Committee for Weights and Measures (CIPM) notes that "its definition applies only within a spatial extent sufficiently small that the effects of the non-uniformity of the gravitational field can be ignored". As such, the metre is undefined for the purposes of measuring distances within the Solar System. The 1976 definition of the astronomical unit was incomplete because it did not specify the frame of reference in which time is to be measured, but proved practical for the calculation of ephemerides: a fuller definition that is consistent with general relativity was proposed, and "vigorous debate" ensued until August 2012 when the IAU adopted the current definition of 1 astronomical unit = metres.
The astronomical unit is typically used for stellar system scale distances, such as the size of a protostellar disk or the heliocentric distance of an asteroid, whereas other units are used for other distances in astronomy. The astronomical unit is too small to be convenient for interstellar distances, where the parsec and light-year are widely used. The parsec (parallax arcsecond) is defined in terms of the astronomical unit, being the distance of an object with a parallax of . The light-year is often used in popular works, but is not an approved non-SI unit and is rarely used by professional astronomers.
When simulating a numerical model of the Solar System, the astronomical unit provides an appropriate scale that minimizes (overflow, underflow and truncation) errors in floating point calculations.
## History.
The book "On the Sizes and Distances of the Sun and Moon", which is ascribed to Aristarchus, says the distance to the Sun is 18 to 20 times the distance to the Moon, whereas the true ratio is about . The latter estimate was based on the angle between the half-moon and the Sun, which he estimated as (the true value being close to ). Depending on the distance that van Helden assumes Aristarchus used for the distance to the Moon, his calculated distance to the Sun would fall between and Earth radii.
According to Eusebius of Caesarea in the "Praeparatio Evangelica" (Book XV, Chapter 53), Eratosthenes found the distance to the Sun to be "σταδιων μυριαδας τετρακοσιας και οκτωκισμυριας" (literally "of "stadia" myriads 400 and ) but with the additional note that in the Greek text the grammatical agreement is between "myriads" (not "stadia") on the one hand and both "400" and "" on the other, as in Greek, unlike English, all three (or all four if one were to include "stadia") words are inflected. This has been translated either as "stadia" (1903 translation by Edwin Hamilton Gifford), or as "stadia" (edition of , dated 1974–1991). Using the Greek stadium of 185 to 190 metres, the former translation comes to to , which is far too low, whereas the second translation comes to 148.7 to 152.8 million kilometres (accurate within 2%). Hipparchus also gave an estimate of the distance of Earth from the Sun, quoted by Pappus as equal to 490 Earth radii. According to the conjectural reconstructions of Noel Swerdlow and G. J. Toomer, this was derived from his assumption of a "least perceptible" solar parallax of .
A Chinese mathematical treatise, the "Zhoubi Suanjing" (c. 1st century BCE), shows how the distance to the Sun can be computed geometrically, using the different lengths of the noontime shadows observed at three places li apart and the assumption that Earth is flat.
In the 2nd century CE, Ptolemy estimated the mean distance of the Sun as times Earth's radius. To determine this value, Ptolemy started by measuring the Moon's parallax, finding what amounted to a horizontal lunar parallax of 1° 26′, which was much too large. He then derived a maximum lunar distance of Earth radii. Because of cancelling errors in his parallax figure, his theory of the Moon's orbit, and other factors, this figure was approximately correct. He then measured the apparent sizes of the Sun and the Moon and concluded that the apparent diameter of the Sun was equal to the apparent diameter of the Moon at the Moon's greatest distance, and from records of lunar eclipses, he estimated this apparent diameter, as well as the apparent diameter of the shadow cone of Earth traversed by the Moon during a lunar eclipse. Given these data, the distance of the Sun from Earth can be trigonometrically computed to be Earth radii. This gives a ratio of solar to lunar distance of approximately 19, matching Aristarchus's figure. Although Ptolemy's procedure is theoretically workable, it is very sensitive to small changes in the data, so much so that changing a measurement by a few per cent can make the solar distance infinite.
After Greek astronomy was transmitted to the medieval Islamic world, astronomers made some changes to Ptolemy's cosmological model, but did not greatly change his estimate of the Earth–Sun distance. For example, in his introduction to Ptolemaic astronomy, al-Farghānī gave a mean solar distance of Earth radii, whereas in his "zij", al-Battānī used a mean solar distance of Earth radii. Subsequent astronomers, such as al-Bīrūnī, used similar values. Later in Europe, Copernicus and Tycho Brahe also used comparable figures ( and Earth radii), and so Ptolemy's approximate Earth–Sun distance survived through the 16th century.
Johannes Kepler was the first to realize that Ptolemy's estimate must be significantly too low (according to Kepler, at least by a factor of three) in his "Rudolphine Tables" (1627). Kepler's laws of planetary motion allowed astronomers to calculate the relative distances of the planets from the Sun, and rekindled interest in measuring the absolute value for Earth (which could then be applied to the other planets). The invention of the telescope allowed far more accurate measurements of angles than is possible with the naked eye. Flemish astronomer Godefroy Wendelin repeated Aristarchus’ measurements in 1635, and found that Ptolemy's value was too low by a factor of at least eleven.
A somewhat more accurate estimate can be obtained by observing the transit of Venus. By measuring the transit in two different locations, one can accurately calculate the parallax of Venus and from the relative distance of Earth and Venus from the Sun, the solar parallax (which cannot be measured directly due to the brightness of the Sun). Jeremiah Horrocks had attempted to produce an estimate based on his observation of the 1639 transit (published in 1662), giving a solar parallax of , similar to Wendelin's figure. The solar parallax is related to the Earth–Sun distance as measured in Earth radii by
The smaller the solar parallax, the greater the distance between the Sun and Earth: a solar parallax of is equivalent to an Earth–Sun distance of Earth radii.
Christiaan Huygens believed that the distance was even greater: by comparing the apparent sizes of Venus and Mars, he estimated a value of about Earth radii, equivalent to a solar parallax of . Although Huygens' estimate is remarkably close to modern values, it is often discounted by historians of astronomy because of the many unproven (and incorrect) assumptions he had to make for his method to work; the accuracy of his value seems to be based more on luck than good measurement, with his various errors cancelling each other out.
Jean Richer and Giovanni Domenico Cassini measured the parallax of Mars between Paris and Cayenne in French Guiana when Mars was at its closest to Earth in 1672. They arrived at a figure for the solar parallax of , equivalent to an Earth–Sun distance of about Earth radii. They were also the first astronomers to have access to an accurate and reliable value for the radius of Earth, which had been measured by their colleague Jean Picard in 1669 as "toises". This same year saw another estimate for the astronomical unit by John Flamsteed, which accomplished it alone by measuring the martian diurnal parallax. Another colleague, Ole Rømer, discovered the finite speed of light in 1676: the speed was so great that it was usually quoted as the time required for light to travel from the Sun to the Earth, or "light time per unit distance", a convention that is still followed by astronomers today.
A better method for observing Venus transits was devised by James Gregory and published in his "Optica Promata" (1663). It was strongly advocated by Edmond Halley and was applied to the transits of Venus observed in 1761 and 1769, and then again in 1874 and 1882. Transits of Venus occur in pairs, but less than one pair every century, and observing the transits in 1761 and 1769 was an unprecedented international scientific operation including observations by James Cook and Charles Green from Tahiti. Despite the Seven Years' War, dozens of astronomers were dispatched to observing points around the world at great expense and personal danger: several of them died in the endeavour. The various results were collated by Jérôme Lalande to give a figure for the solar parallax of . Karl Rudolph Powalky had made an estimate of in 1864.
Another method involved determining the constant of aberration. Simon Newcomb gave great weight to this method when deriving his widely accepted value of for the solar parallax (close to the modern value of ), although Newcomb also used data from the transits of Venus. Newcomb also collaborated with A. A. Michelson to measure the speed of light with Earth-based equipment; combined with the constant of aberration (which is related to the light time per unit distance), this gave the first direct measurement of the Earth–Sun distance in kilometres. Newcomb's value for the solar parallax (and for the constant of aberration and the Gaussian gravitational constant) were incorporated into the first international system of astronomical constants in 1896, which remained in place for the calculation of ephemerides until 1964. The name "astronomical unit" appears first to have been used in 1903.
The discovery of the near-Earth asteroid 433 Eros and its passage near Earth in 1900–1901 allowed a considerable improvement in parallax measurement. Another international project to measure the parallax of 433 Eros was undertaken in 1930–1931.
Direct radar measurements of the distances to Venus and Mars became available in the early 1960s. Along with improved measurements of the speed of light, these showed that Newcomb's values for the solar parallax and the constant of aberration were inconsistent with one another.
## Developments.
The unit distance (the value of the astronomical unit in metres) can be expressed in terms of other astronomical constants:
where is the Newtonian gravitational constant, is the solar mass, is the numerical value of Gaussian gravitational constant and is the time period of one day.
The Sun is constantly losing mass by radiating away energy, so the orbits of the planets are steadily expanding outward from the Sun. This has led to calls to abandon the astronomical unit as a unit of measurement.
As the speed of light has an exact defined value in SI units and the Gaussian gravitational constant is fixed in the astronomical system of units, measuring the light time per unit distance is exactly equivalent to measuring the product × in SI units. Hence, it is possible to construct ephemerides entirely in SI units, which is increasingly becoming the norm.
A 2004 analysis of radiometric measurements in the inner Solar System suggested that the secular increase in the unit distance was much larger than can be accounted for by solar radiation, + metres per century.
The measurements of the secular variations of the astronomical unit are not confirmed by other authors and are quite controversial.
Furthermore, since 2010, the astronomical unit has not been estimated by the planetary ephemerides.
## Examples.
The following table contains some distances given in astronomical units. It includes some examples with distances that are normally not given in astronomical units, because they are either too short or far too long. Distances normally change over time. Examples are listed by increasing distance.

</doc>
<doc id="1212" url="https://en.wikipedia.org/wiki?curid=1212" title="Artist">
Artist

An artist is a person engaged in an activity related to creating art, practicing the arts, or demonstrating an art. The common usage in both everyday speech and academic discourse refers to a practitioner in the visual arts only. However, the term is also often used in the entertainment business, especially in a business context, for musicians and other performers (although less often for actors). "Artiste" (French for artist) is a variant used in English in this context, but this use has become rare. Use of the term "artist" to describe writers is valid, but less common, and mostly restricted to contexts like used in criticism.
## Dictionary definitions.
The "Oxford English Dictionary" defines the older broad meanings of the term "artist":
## History of the term.
The Greek word "techně", often translated as "art," implies mastery of any sort of craft. The adjectival Latin form of the word, "technicus",
became the source of the English words technique, technology, technical.
In Greek culture each of the nine Muses oversaw a different field of human creation:
No muse was identified with the visual arts of painting and sculpture. In ancient Greece sculptors and painters were held in low regard, somewhere between freemen and slaves, their work regarded as mere manual labour.
The word "art" derives from the Latin "ars" (stem "art-"), which, although literally defined means "skill method" or "technique", also conveys a connotation of beauty.
During the Middle Ages the word "artist" already existed in some countries such as Italy, but the meaning was something resembling "craftsman", while the word "artesan" was still unknown. An artist was someone able to do a work better than others, so the skilled excellency was underlined, rather than the activity field. In this period some "artisanal" products (such as textiles) were much more precious and expensive than paintings or sculptures.
The first division into major and minor arts dates back at least to the works of Leon Battista Alberti (1404–1472): "De re aedificatoria, De statua, De pictura", which focused on the importance of the intellectual skills of the artist rather than the manual skills (even if in other forms of art there was a project behind).
With the Academies in Europe (second half of 16th century) the gap between fine and applied arts was definitely set.
Many contemporary definitions of "artist" and "art" are highly contingent on culture, resisting aesthetic prescription, in much the same way that the features constituting beauty and the beautiful cannot be standardized easily without moving into kitsch.
## Training and employment.
The US Bureau of Labor Statistics classifies many visual artists as either "craft artists" or "fine artists". A craft artist makes handmade functional works of art, such as pottery or clothing. A fine artist makes paintings, illustrations (such as book illustrations or medical illustrations), sculptures, or similar artistic works primarily for their aesthetic value.
The main source of skill for both craft artists and fine artists is long-term repetition and practice. Many fine artists have studied their art form at university and some have a master's degree in fine arts. Artists may also study on their own or receive on-the-job training from an experienced artist.
The number of available jobs as an artist is increasing more slowly than other fields. About half of US artists are self-employed. Others work in a variety of industries. For example, a pottery manufacturer will employ craft artists, and book publishers will hire illustrators.
In the US, fine artists have a median income of approximately US$50,000 per year, and craft artists have a median income of approximately US$33,000 per year. This compares to US$61,000 for all art-related fields, including related jobs such as graphic designers, multimedia artists, animators, and fashion designers. Many artists work part-time as artists and hold a second job.

</doc>
<doc id="1213" url="https://en.wikipedia.org/wiki?curid=1213" title="Actaeon">
Actaeon

Actaeon (; "Aktaion"), in Greek mythology, son of the priestly herdsman Aristaeus and Autonoe in Boeotia, was a famous Theban hero. Like Achilles in a later generation, he was trained by the centaur Chiron.
He fell to the fatal wrath of Artemis, but the surviving details of his transgression vary: "the only certainty is in what Aktaion suffered, his pathos, and what Artemis did: the hunter became the hunted; he was transformed into a stag, and his raging hounds, struck with a 'wolf's frenzy' (Lyssa), tore him apart as they would a stag." This is the iconic motif by which Actaeon is recognized, both in ancient art and in Renaissance and post-Renaissance depictions.
## The plot.
Among others, John Heath has observed, "The unalterable kernel of the tale was a hunter's transformation into a deer and his death in the jaws of his hunting dogs. But authors were free to suggest different motives for his death." In the version that was offered by the Hellenistic poet Callimachus, which has become the standard setting, Artemis was bathing in the woods when the hunter Actaeon stumbled across her, thus seeing her naked. He stopped and stared, amazed at her ravishing beauty. Once seen, Artemis got revenge on Actaeon: she forbade him speech — if he tried to speak, he would be changed into a stag — for the unlucky profanation of her virginity's mystery.
Upon hearing the call of his hunting party, he cried out to them and immediately transformed. At this he fled deep into the woods, and doing so he came upon a pond and, seeing his reflection, groaned. His own hounds then turned upon him and pursued him, not recognizing him. In an endeavour to save himself, he raised his eyes (and would have raised his arms, had he had them) toward Mount Olympus. The gods did not heed his plea, and he was torn to pieces. An element of the earlier myth made Actaeon the familiar hunting companion of Artemis, no stranger. In an embroidered extension of the myth, the hounds were so upset with their master's death, that Chiron made a statue so lifelike that the hounds thought it was Actaeon.
There are various other versions of his transgression: The Hesiodic "Catalogue of Women" and pseudo-Apollodoran "Bibliotheke" state that his offense was that he was a rival of Zeus for Semele, his mother's sister, whereas in Euripides' "Bacchae" he has boasted that he is a better hunter than Artemis:
Further materials, including fragments that belong with the Hesiodic "Catalogue of Women" and at least four Attic tragedies, including a "Toxotides" of Aeschylus, have been lost. Diodorus Siculus (4.81.4), in a variant of Actaeon's "hubris" that has been largely ignored, has it that Actaeon wanted to marry Artemis. Other authors say the hounds were Artemis' own; some lost elaborations of the myth seem to have given them all names and narrated their wanderings after his loss.
According to the Latin version of the story told by the Roman Ovid having accidentally seen Diana (Artemis) on Mount Cithaeron while she was bathing, he was changed by her into a stag, and pursued and killed by his fifty hounds. This version also appears in Callimachus' Fifth Hymn, as a mythical parallel to the blinding of Tiresias after he sees Athena bathing.
The literary testimony of Actaeon's myth is largely lost, but Lamar Ronald Lacy, deconstructing the myth elements in what survives and supplementing it by iconographic evidence in late vase-painting, made a plausible reconstruction of an ancient Actaeon myth that Greek poets may have inherited and subjected to expansion and dismemberment. His reconstruction opposes a too-pat consensus that has an archaic Actaeon aspiring to Semele, a classical Actaeon boasting of his hunting prowess and a Hellenistic Actaeon glimpsing Artemis' bath. Lacy identifies the site of Actaeon's transgression as a spring sacred to Artemis at Plataea where Actaeon was a " hero archegetes" ("hero-founder") The righteous hunter, the companion of Artemis, seeing her bathing naked in the spring, was moved to try to make himself her consort, as Diodorus Siculus noted, and was punished, in part for transgressing the hunter's "ritually enforced deference to Artemis" (Lacy 1990:42).
## Names of dogs.
Notes:
## The "bed of Actaeon".
In the second century AD, the traveller Pausanias was shown a spring on the road in Attica leading to Plataea from Eleutherae, just beyond Megara "and a little farther on a rock. It is called the bed of Actaeon, for it is said that he slept thereon when weary with hunting and that into this spring he looked while Artemis was bathing in it."
## Parallels in Akkadian and Ugarit poems.
In the standard version of the "Epic of Gilgamesh" (tablet vi) there is a parallel, in the series of examples Gilgamesh gives Ishtar of her mistreatment of her serial lovers:
You loved the herdsman, shepherd and chief shepherd&lt;br&gt; Who was always heaping up the glowing ashes for you,&lt;br&gt; And cooked ewe-lambs for you every day.&lt;br&gt; But you hit him and turned him into a wolf,&lt;br&gt; His own herd-boys hunt him down&lt;br&gt;
And his dogs tear at his haunches. Actaeon, torn apart by dogs incited by Artemis, finds another Near Eastern parallel in the Ugaritic hero Aqht, torn apart by eagles incited by Anath who wanted his hunting bow.
The virginal Artemis of classical times is not directly comparable to Ishtar of the many lovers, but the mytheme of Artemis shooting Orion, was linked to her punishment of Actaeon by T.C.W. Stinton; the Greek context of the mortal's reproach to the amorous goddess is translated to the episode of Anchises and Aphrodite. Daphnis too was a herdsman loved by a goddess and punished by her: see Theocritus' First Idyll.
## Symbolism regarding Actaeon.
In Greek Mythology, Actaeon is widely thought to symbolize ritual human sacrifice in attempt to please a God or Goddess: the dogs symbolize the sacrificers and Actaeon symbolizes the sacrifice.
Actaeon may symbolize human curiosity or irreverence.
The myth is seen by Jungian psychologist Wolfgang Giegerich as a symbol of spiritual transformation and/or enlightenment.
Actaeon often symbolizes a cuckold, as when he is turned into a stag, he becomes "horned". This is alluded to in Shakespeare's "Merry Wives", Robert Burton's "Anatomy of Melancholy", and others.
## Cultural depictions.
The two main scenes are Actaeon surprising Artemis/Diana, and his death. In classical art Actaeon is normally shown as fully human, even as his hounds are killing him (sometimes he has small horns), but in Renaissance art he is often given a deer's head with antlers even in the scene with Diana, and by the time he is killed he has at the least this head, and has often completely transformed into the shape of a deer. 

</doc>
<doc id="1214" url="https://en.wikipedia.org/wiki?curid=1214" title="Anglicanism">
Anglicanism

Anglicanism is a Western Christian tradition that has developed from the practices, liturgy, and identity of the Church of England following the English Reformation, in the context of the Protestant Reformation in Europe. It is one of the largest branches of Christianity, with around 110 million adherents worldwide .
Adherents of Anglicanism are called "Anglicans"; they are also called "Episcopalians" in some countries. The majority of Anglicans are members of national or regional ecclesiastical provinces of the international Anglican Communion, which forms the third-largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Church. These provinces are in full communion with the See of Canterbury and thus with the British Monarch’s personal choice of the Archbishop of Canterbury, whom the communion refers to as its "primus inter pares" (Latin, 'first among equals'). The Archbishop calls the decennial Lambeth Conference, chairs the meeting of primates, and is the president of the Anglican Consultative Council. Some churches that are not part of the Anglican Communion or recognised by it also call themselves Anglican, including those that are within the Continuing Anglican movement and Anglican realignment.
Anglicans base their Christian faith on the Bible, traditions of the apostolic Church, apostolic succession ("historic episcopate"), and the writings of the Church Fathers. Anglicanism forms one of the branches of Western Christianity, having definitively declared its independence from the Holy See at the time of the Elizabethan Religious Settlement. Many of the new Anglican formularies of the mid-16th century corresponded closely to those of contemporary Protestantism. These reforms in the Church of England were understood by one of those most responsible for them, Thomas Cranmer, the Archbishop of Canterbury, and others as navigating a middle way between two of the emerging Protestant traditions, namely Lutheranism and Calvinism.
In the first half of the 17th century, the Church of England and its associated Church of Ireland were presented by some Anglican divines as comprising a distinct Christian tradition, with theologies, structures, and forms of worship representing a different kind of middle way, or "via media", between Protestantism and Catholicism – a perspective that came to be highly influential in later theories of Anglican identity and expressed in the description of Anglicanism as "catholic and reformed". The degree of distinction between Protestant and Catholic tendencies within the Anglican tradition is routinely a matter of debate both within specific Anglican churches and throughout the Anglican Communion. Unique to Anglicanism is the Book of Common Prayer, the collection of services in one Book used for centuries. The Book is acknowledged as a principal tie that binds the Anglican Communion together as a liturgical rather than a confessional tradition or one possessing a magisterium as in the Roman Catholic Church.
After the American Revolution, Anglican congregations in the United States and British North America (which would later form the basis for the modern country of Canada) were each reconstituted into autonomous churches with their own bishops and self-governing structures; these were known as the American Episcopal Church and the Church of England in the Dominion of Canada. Through the expansion of the British Empire and the activity of Christian missions, this model was adopted as the model for many newly formed churches, especially in Africa, Australasia, and Asia-Pacific. In the 19th century, the term "Anglicanism" was coined to describe the common religious tradition of these churches; as also that of the Scottish Episcopal Church, which, though originating earlier within the Church of Scotland, had come to be recognised as sharing this common identity.
## Terminology.
The word "Anglican" originates in , a phrase from the Magna Carta dated 15 June 1215, meaning "the Anglican Church shall be free". Adherents of Anglicanism are called "Anglicans". As an adjective, "Anglican" is used to describe the people, institutions, and churches, as well as the liturgical traditions and theological concepts developed by the Church of England.
As a noun, an Anglican is a member of a church in the Anglican Communion. The word is also used by followers of separated groups which have left the communion or have been founded separately from it, although this is considered as a misuse by the Anglican Communion. The word "Anglicanism" came into being in the 19th century. The word originally referred only to the teachings and rites of Christians throughout the world in communion with the see of Canterbury, but has come to sometimes be extended to any church following those traditions rather than actual membership in the modern Anglican Communion.
Although the term "Anglican" is found referring to the Church of England as far back as the 16th century, its use did not become general until the latter half of the 19th century. In British parliamentary legislation referring to the English Established Church, there is no need for a description; it is simply the Church of England, though the word "Protestant" is used in many legal acts specifying the succession to the Crown and qualifications for office. When the Union with Ireland Act created the United Church of England and Ireland, it is specified that it shall be one "Protestant Episcopal Church", thereby distinguishing its form of church government from the Presbyterian polity that prevails in the Church of Scotland.
The word "Episcopal" is preferred in the title of the Episcopal Church (the province of the Anglican Communion covering the United States) and the Scottish Episcopal Church, though the full name of the former is "The Protestant Episcopal Church of the United States of America". Elsewhere, however, the term "Anglican Church" came to be preferred as it distinguished these churches from others that maintain an episcopal polity.
### Definition.
Anglicanism, in its structures, theology, and forms of worship, is a Christian tradition representing, in the words of Thomas Cranmer, Archbishop of Canterbury at the time of the Reformation, "a middle way between Zurich and Wittenberg", that is, between the Reformed Christianity of Calvin and Lutheranism. This "via media" (or "middle way") was later reinterpreted by John Henry Newman and the Tractarians as meaning the middle ground between Roman Catholicism and Protestantism, however, Anglicanism is a Protestant denomination, not a separate group.
The faith of Anglicans is founded in the Scriptures and the Gospels, the traditions of the Apostolic Church, the historical episcopate, the first four ecumenical councils, and the early Church Fathers (among these councils, especially the premier four ones, and among these Fathers, especially those active during the five initial centuries of Christianity, according to the "quinquasaecularist" principle proposed by the English bishop Lancelot Andrewes and the Lutheran dissident Georg Calixtus). Anglicans understand the Old and New Testaments as "containing all things necessary for salvation" and as being the rule and ultimate standard of faith. Reason and tradition are seen as valuable means to interpret scripture (a position first formulated in detail by Richard Hooker), but there is no full mutual agreement among Anglicans about "exactly how" scripture, reason, and tradition interact (or ought to interact) with each other. Anglicans understand the Apostles' Creed as the baptismal symbol and the Nicene Creed as the sufficient statement of the Christian faith.
Anglicans believe the catholic and apostolic faith is revealed in Holy Scripture and the Catholic creeds and interpret these in light of the Christian tradition of the historic church, scholarship, reason, and experience.
Anglicans celebrate the traditional sacraments, with special emphasis being given to the Eucharist, also called Holy Communion, the Lord's Supper or the Mass. The Eucharist is central to worship for most Anglicans as a communal offering of prayer and praise in which the life, death, and resurrection of Jesus Christ are proclaimed through prayer, reading of the Bible, singing, giving God thanks over the bread and wine for the innumerable benefits obtained through the passion of Christ, the breaking of the bread, the blessing of the cup, and the partaking of the body and blood of Christ as instituted at the Last Supper, however one wished to define the Presence. The consecrated bread and wine, which are the true body and blood of Christ after a spiritual manner, are outward symbols of an inner grace given by Christ, which to the repentant conveys forgiveness and cleaning from sin. While many Anglicans celebrate the Eucharist in similar ways to the predominant western Catholic tradition, a considerable degree of liturgical freedom is permitted, and worship styles range from the simple to elaborate.
Unique to Anglicanism is the "Book of Common Prayer" (BCP), the collection of services that worshippers in most Anglican churches have used for centuries. It was called "common prayer" originally because it was intended for use in all Church of England churches, which had previously followed differing local liturgies. The term was kept when the church became international, because all Anglicans used to share in its use around the world.
In 1549, the first "Book of Common Prayer" was compiled by Thomas Cranmer, who was then Archbishop of Canterbury. While it has since undergone many revisions and Anglican churches in different countries have developed other service books, the Prayer Book is still acknowledged as one of the ties that bind Anglicans together.
## Anglican identity.
### Early history.
The founding of Christianity in Britain is commonly attributed to Joseph of Arimathea, according to Anglican legend, and is commemorated in Glastonbury Abbey. Many of the early Church Fathers wrote of the presence of Christianity in Roman Britain, with Tertullian stating "those parts of Britain into which the Roman arms had never penetrated were become subject to Christ". Saint Alban, who was executed in AD 209, is the first Christian martyr in the British Isles. For this reason he is venerated as the British protomartyr. The historian Heinrich Zimmer writes that "Just as Britain was a part of the Roman Empire, so the British Church formed (during the fourth century) a branch of the Catholic Church of the West; and during the whole of that century, from the Council of Arles (316) onward, took part in all proceedings concerning the Church."
After Roman troops withdrew from Britain, the "absence of Roman military and governmental influence and overall decline of Roman imperial political power enabled Britain and the surrounding isles to develop distinctively from the rest of the West. A new culture emerged around the Irish Sea among the Celtic peoples with Celtic Christianity at its core. What resulted was a form of Christianity distinct from Rome in many traditions and practices."
The historian Charles Thomas, in addition to the Celticist Heinrich Zimmer, writes that the distinction between sub-Roman and post-Roman Insular Christianity, also known as Celtic Christianity, began to become apparent around AD 475, with the Celtic churches allowing married clergy, observing Lent and Easter according to their own calendar, and having a different tonsure; moreover, like the Eastern Orthodox and the Oriental Orthodox Churches, the Celtic churches operated independently of the Pope's authority, as a result of their isolated development in the British Isles.
In what is known as the Gregorian mission, Pope Gregory I sent Augustine of Canterbury to the British Isles in AD 596, with the purpose of evangelising the pagans there (who were largely Anglo-Saxons), as well as to reconcile the Celtic churches in the British Isles to the See of Rome. In Kent, Augustine persuaded the Anglo-Saxon king "Æthelberht and his people to accept Christianity". Augustine, on two occasions, "met in conference with members of the Celtic episcopacy, but no understanding was reached between them."
Eventually, the "Christian Church of the Anglo-Saxon kingdom of Northumbria convened the Synod of Whitby in 663/664 to decide whether to follow Celtic or Roman usages." This meeting, with King Oswiu as the final decision maker, "led to the acceptance of Roman usage elsewhere in England and brought the English Church into close contact with the Continent". As a result of assuming Roman usages, the Celtic Church surrendered its independence, and, from this point on, the Church in England "was no longer purely Celtic, but became Anglo-Roman-Celtic". The theologian Christopher L. Webber writes that, although "the Roman form of Christianity became the dominant influence in Britain as in all of western Europe, Anglican Christianity has continued to have a distinctive quality because of its Celtic heritage."
The Church in England remained united with Rome until the English Parliament, through the Act of Supremacy (1534), declared King Henry VIII to be the Supreme Head of the Church of England to fulfill the "English desire to be independent from continental Europe religiously and politically." As the change was mostly political, done in order to allow for the annulment of Henry VIII's marriage, the English Church under Henry VIII continued to maintain Roman Catholic doctrines and the sacraments despite the separation from Rome. With little exception, Henry VIII allowed no changes during his lifetime. Under King Edward VI (1547–1553), however, the church in England underwent what is known as the English Reformation, in the course of which it acquired a number of characteristics that would subsequently become recognised as constituting its distinctive "Anglican" identity.
### Development.
With the Elizabethan Settlement of 1559, the Protestant identity of the English and Irish churches was affirmed by means of parliamentary legislation which mandated allegiance and loyalty to the English Crown in all their members. The Elizabethan church began to develop distinct religious traditions, assimilating some of the theology of Reformed churches with the services in the "Book of Common Prayer" (which drew extensively on the Sarum Rite native to England), under the leadership and organisation of a continuing episcopate. Over the years, these traditions themselves came to command adherence and loyalty. The Elizabethan Settlement stopped the radical Protestant tendencies under Edward VI by combining the more radical elements of the Second Prayer Book of 1552 with the conservative "Catholic" First Prayer Book of 1549. From then on, Protestantism was in a "state of arrested development", regardless of the attempts to detach the Church of England from its "idiosyncratic anchorage in the medieval past" by various groups which tried to push it towards a more Reformed theology and governance in the years 1560–1660.
Although two important constitutive elements of what later would emerge as Anglicanism were present in 1559 – scripture, the historic episcopate, the Book of Common Prayer, the teachings of the First Four Ecumenical Councils as the yardstick of catholicity, the teaching of the Church Fathers and Catholic bishops, and informed reason – neither the laypeople nor the clergy perceived themselves as Anglicans at the beginning of Elizabeth I's reign, as there was no such identity. Neither does the term "via media" appear until the 1627 to describe a church which refused to identify itself definitely as Catholic or Protestant, or as both, "and had decided in the end that this is virtue rather than a handicap".
Historical studies on the period 1560–1660 written before the late 1960s tended to project the predominant conformist spirituality and doctrine of the 1660s on the ecclesiastical situation one hundred years before, and there was also a tendency to take polemically binary partitions of reality claimed by contestants studied (such as the dichotomies Protestant-"Popish" or "Laudian"-"Puritan") at face value. Since the late 1960s, these interpretations have been criticised. Studies on the subject written during the last forty-five years have, however, not reached any consensus on how to interpret this period in English church history. The extent to which one or several positions concerning doctrine and spirituality existed alongside the more well-known and articulate Puritan movement and the Durham House Party, and the exact extent of continental Calvinism among the English elite and among the ordinary churchgoers from the 1560s to the 1620s are subjects of current and ongoing debate.
In 1662, under King Charles II, a revised "Book of Common Prayer" was produced, which was acceptable to high churchmen as well as some Puritans, and is still considered authoritative to this day.
In so far as Anglicans derived their identity from both parliamentary legislation and ecclesiastical tradition, a crisis of identity could result wherever secular and religious loyalties came into conflict – and such a crisis indeed occurred in 1776 with the American Declaration of Independence, most of whose signatories were, at least nominally, Anglican. For these American patriots, even the forms of Anglican services were in doubt, since the Prayer Book rites of Matins, Evensong, and Holy Communion all included specific prayers for the British Royal Family. Consequently, the conclusion of the War of Independence eventually resulted in the creation of two new Anglican churches, the Episcopal Church in the United States in those states that had achieved independence; and in the 1830s The Church of England in Canada became independent from the Church of England in those North American colonies which had remained under British control and to which many Loyalist churchmen had migrated.
Reluctantly, legislation was passed in the British Parliament (the Consecration of Bishops Abroad Act 1786) to allow bishops to be consecrated for an American church outside of allegiance to the British Crown (since no dioceses had ever been established in the former American colonies). Both in the United States and in Canada, the new Anglican churches developed novel models of self-government, collective decision-making, and self-supported financing; that would be consistent with separation of religious and secular identities.
In the following century, two further factors acted to accelerate the development of a distinct Anglican identity. From 1828 and 1829, Dissenters and Catholics could be elected to the House of Commons, which consequently ceased to be a body drawn purely from the established churches of Scotland, England, and Ireland; but which nevertheless, over the following ten years, engaged in extensive reforming legislation affecting the interests of the English and Irish churches; which, by the Acts of Union of 1800, had been reconstituted as the United Church of England and Ireland. The propriety of this legislation was bitterly contested by the Oxford Movement (Tractarians), who in response developed a vision of Anglicanism as religious tradition deriving ultimately from the ecumenical councils of the patristic church. Those within the Church of England opposed to the Tractarians, and to their revived ritual practices, introduced a stream of bills in parliament aimed to control innovations in worship. This only made the dilemma more acute, with consequent continual litigation in the secular and ecclesiastical courts.
Over the same period, Anglican churches engaged vigorously in Christian missions, resulting in the creation, by the end of the century, of over ninety colonial bishoprics, which gradually coalesced into new self-governing churches on the Canadian and American models. However, the case of John Colenso, Bishop of Natal, reinstated in 1865 by the English Judicial Committee of the Privy Council over the heads of the Church in South Africa, demonstrated acutely that the extension of episcopacy had to be accompanied by a recognised Anglican ecclesiology of ecclesiastical authority, distinct from secular power.
Consequently, at the instigation of the bishops of Canada and South Africa, the first Lambeth Conference was called in 1867; to be followed by further conferences in 1878 and 1888, and thereafter at ten-year intervals. The various papers and declarations of successive Lambeth Conferences have served to frame the continued Anglican debate on identity, especially as relating to the possibility of ecumenical discussion with other churches. This ecumenical aspiration became much more of a possibility, as other denominational groups rapidly followed the example of the Anglican Communion in founding their own transnational alliances: the Alliance of Reformed Churches, the Ecumenical Methodist Council, the International Congregational Council, and the Baptist World Alliance.
### Theories.
Anglicanism was seen as a middle way, or "via media", between two branches of Protestantism, Lutheranism and Reformed Christianity. In their rejection of absolute parliamentary authority, the Tractarians – and in particular John Henry Newman – looked back to the writings of 17th-century Anglican divines, finding in these texts the idea of the English church as a "via media" between the Protestant and Catholic traditions. This view was associated – especially in the writings of Edward Bouverie Pusey – with the theory of Anglicanism as one of three "branches" (alongside the Roman Catholic Church and the Orthodox Church) historically arising out of the common tradition of the earliest ecumenical councils. Newman himself subsequently rejected his theory of the "via media", as essentially historicist and static and hence unable to accommodate any dynamic development within the church. Nevertheless, the aspiration to ground Anglican identity in the writings of the 17th-century divines and in faithfulness to the traditions of the Church Fathers reflects a continuing theme of Anglican ecclesiology, most recently in the writings of Henry Robert McAdoo.
The Tractarian formulation of the theory of the "via media" between Protestantism and Roman Catholicism was essentially a party platform, and not acceptable to Anglicans outside the confines of the Oxford Movement. However, this theory of the "via media" was reworked in the ecclesiological writings of Frederick Denison Maurice, in a more dynamic form that became widely influential. Both Maurice and Newman saw the Church of England of their day as sorely deficient in faith; but whereas Newman had looked back to a distant past when the light of faith might have appeared to burn brighter, Maurice looked forward to the possibility of a brighter revelation of faith in the future. Maurice saw the Protestant and Catholic strands within the Church of England as contrary but complementary, both maintaining elements of the true church, but incomplete without the other; such that a true catholic and evangelical church might come into being by a union of opposites.
Central to Maurice's perspective was his belief that the collective elements of family, nation, and church represented a divine order of structures through which God unfolds his continuing work of creation. Hence, for Maurice, the Protestant tradition had maintained the elements of national distinction which were amongst the marks of the true universal church, but which had been lost within contemporary Roman Catholicism in the internationalism of centralised papal authority. Within the coming universal church that Maurice foresaw, national churches would each maintain the six signs of Catholicity: baptism, Eucharist, the creeds, Scripture, an episcopal ministry, and a fixed liturgy (which could take a variety of forms in accordance with divinely ordained distinctions in national characteristics). Not surprisingly, this vision of a becoming universal church as a congregation of autonomous national churches proved highly congenial in Anglican circles; and Maurice's six signs were adapted to form the Chicago-Lambeth Quadrilateral of 1888.
In the latter decades of the 20th century, Maurice's theory, and the various strands of Anglican thought that derived from it, have been criticised by Stephen Sykes, who argues that the terms "Protestant" and "Catholic" as used in these approaches are synthetic constructs denoting ecclesiastic identities unacceptable to those to whom the labels are applied. Hence, the Catholic Church does not regard itself as a party or strand within the universal church – but rather identifies itself as the universal church. Moreover, Sykes criticises the proposition, implicit in theories of "via media", that there is no distinctive body of Anglican doctrines, other than those of the universal church; accusing this of being an excuse not to undertake systematic doctrine at all.
Contrariwise, Sykes notes a high degree of commonality in Anglican liturgical forms and in the doctrinal understandings expressed within those liturgies. He proposes that Anglican identity might rather be found within a shared consistent pattern of prescriptive liturgies, established and maintained through canon law, and embodying both a historic deposit of formal statements of doctrine, and also framing the regular reading and proclamation of scripture. Sykes nevertheless agrees with those heirs of Maurice who emphasise the incompleteness of Anglicanism as a positive feature, and quotes with qualified approval the words of Michael Ramsey:
## Doctrine.
### "Catholic and reformed".
The distinction between Reformed and Catholic, and the coherence of the two, is a matter of debate within the Anglican Communion. The Oxford Movement of the mid-19th century revived and extended doctrinal, liturgical, and pastoral practices similar to those of Roman Catholicism. This extends beyond the ceremony of high church services to even more theologically significant territory, such as sacramental theology (see Anglican sacraments). While Anglo-Catholic practices, particularly liturgical ones, have become more common within the tradition over the last century, there are also places where practices and beliefs resonate more closely with the evangelical movements of the 1730s (see Sydney Anglicanism).
### Guiding principles.
For high-church Anglicans, doctrine is neither established by a magisterium, nor derived from the theology of an eponymous founder (such as Calvinism), nor summed up in a confession of faith beyond the ecumenical creeds (such as the Lutheran Book of Concord). For them, the earliest Anglican theological documents are its prayer books, which they see as the products of profound theological reflection, compromise, and synthesis. They emphasise the "Book of Common Prayer" as a key expression of Anglican doctrine. The principle of looking to the prayer books as a guide to the parameters of belief and practice is called by the Latin name "lex orandi, lex credendi" ("the law of prayer is the law of belief").
Within the prayer books are the fundamentals of Anglican doctrine: the Apostles' and Nicene creeds, the Athanasian Creed (now rarely used), the scriptures (via the lectionary), the sacraments, daily prayer, the catechism, and apostolic succession in the context of the historic threefold ministry. For some low-church and evangelical Anglicans, the 16th-century Reformed Thirty-Nine Articles form the basis of doctrine.
#### Distinctives of Anglican belief.
The Thirty-Nine Articles played a significant role in Anglican doctrine and practice. Following the passing of the 1604 canons, all Anglican clergy had to formally subscribe to the articles. Today, however, the articles are no longer binding, but are seen as a historical document which has played a significant role in the shaping of Anglican identity. The degree to which each of the articles has remained influential varies.
On the doctrine of justification, for example, there is a wide range of beliefs within the Anglican Communion, with some Anglo-Catholics arguing for a faith with good works and the sacraments. At the same time, however, some evangelical Anglicans ascribe to the Reformed emphasis on "sola fide" ("faith alone") in their doctrine of justification (see Sydney Anglicanism). Still other Anglicans adopt a nuanced view of justification, taking elements from the early Church Fathers, Catholicism, Protestantism, liberal theology, and latitudinarian thought.
Arguably, the most influential of the original articles has been Article VI on the "sufficiency of scripture", which says that "Scripture containeth all things necessary to salvation: so that whatsoever is not read therein, nor may be proved thereby, is not to be required of any man, that it should be believed as an article of the Faith, or be thought requisite or necessary to salvation." This article has informed Anglican biblical exegesis and hermeneutics since earliest times.
Anglicans look for authority in their "standard divines" (see below). Historically, the most influential of these – apart from Cranmer – has been the 16th-century cleric and theologian Richard Hooker, who after 1660 was increasingly portrayed as the founding father of Anglicanism. Hooker's description of Anglican authority as being derived primarily from scripture, informed by reason (the intellect and the experience of God) and tradition (the practices and beliefs of the historical church), has influenced Anglican self-identity and doctrinal reflection perhaps more powerfully than any other formula. The analogy of the "three-legged stool" of scripture, reason, and tradition is often incorrectly attributed to Hooker. Rather, Hooker's description is a hierarchy of authority, with scripture as foundational and reason and tradition as vitally important, but secondary, authorities.
Finally, the extension of Anglicanism into non-English cultures, the growing diversity of prayer books, and the increasing interest in ecumenical dialogue have led to further reflection on the parameters of Anglican identity. Many Anglicans look to the Chicago-Lambeth Quadrilateral of 1888 as the "sine qua non" of communal identity. In brief, the quadrilateral's four points are the scriptures as containing all things necessary to salvation; the creeds (specifically, the Apostles' and Nicene Creeds) as the sufficient statement of Christian faith; the dominical sacraments of Baptism and Holy Communion; and the historic episcopate.
### Anglican divines.
Within the Anglican tradition, "divines" are clergy of the Church of England whose theological writings have been considered standards for faith, doctrine, worship, and spirituality, and whose influence has permeated the Anglican Communion in varying degrees through the years. While there is no authoritative list of these Anglican divines, there are some whose names would likely be found on most lists – those who are commemorated in lesser feasts of the Anglican churches and those whose works are frequently anthologised.
The corpus produced by Anglican divines is diverse. What they have in common is a commitment to the faith as conveyed by scripture and the "Book of Common Prayer", thus regarding prayer and theology in a manner akin to that of the Apostolic Fathers. On the whole, Anglican divines view the "via media" of Anglicanism not as a compromise, but as "a positive position, witnessing to the universality of God and God's kingdom working through the fallible, earthly "ecclesia Anglicana"".
These theologians regard scripture as interpreted through tradition and reason as authoritative in matters concerning salvation. Reason and tradition, indeed, is extant in and presupposed by scripture, thus implying co-operation between God and humanity, God and nature, and between the sacred and secular. Faith is thus regarded as incarnational and authority as dispersed.
Amongst the early Anglican divines of the 16th and 17th centuries, the names of Thomas Cranmer, John Jewel, Matthew Parker, Richard Hooker, Lancelot Andrewes, and Jeremy Taylor predominate. The influential character of Hooker's "Of the Laws of Ecclesiastical Polity" cannot be overestimated. Published in 1593 and subsequently, Hooker's eight-volume work is primarily a treatise on church-state relations, but it deals comprehensively with issues of biblical interpretation, soteriology, ethics, and sanctification. Throughout the work, Hooker makes clear that theology involves prayer and is concerned with ultimate issues and that theology is relevant to the social mission of the church.
The 18th century saw the rise of two important movements in Anglicanism: Cambridge Platonism, with its mystical understanding of reason as the "candle of the Lord", and the evangelical revival, with its emphasis on the personal experience of the Holy Spirit. The Cambridge Platonist movement evolved into a school called Latitudinarianism, which emphasised reason as the barometer of discernment and took a stance of indifference towards doctrinal and ecclesiological differences.
The evangelical revival, influenced by such figures as John Wesley and Charles Simeon, re-emphasised the importance of justification through faith and the consequent importance of personal conversion. Some in this movement, such as Wesley and George Whitefield, took the message to the United States, influencing the First Great Awakening and creating an Anglo-American movement called Methodism that would eventually break away, structurally, from the Anglican churches after the American Revolution.
By the 19th century, there was a renewed interest in pre-Reformation English religious thought and practice. Theologians such as John Keble, Edward Bouverie Pusey, and John Henry Newman had widespread influence in the realm of polemics, homiletics and theological and devotional works, not least because they largely repudiated the old high-church tradition and replaced it with a dynamic appeal to antiquity which looked beyond the Reformers and Anglican formularies. Their work is largely credited with the development of the Oxford Movement, which sought to reassert Catholic identity and practice in Anglicanism.
In contrast to this movement, clergy such as the Bishop of Liverpool, J. C. Ryle, sought to uphold the distinctly Reformed identity of the Church of England. He was not a servant of the status quo, but argued for a lively religion which emphasised grace, holy and charitable living, and the plain use of the 1662 "Book of Common Prayer" (interpreted in a partisan evangelical way) without additional rituals. Frederick Denison Maurice, through such works as "The Kingdom of Christ", played a pivotal role in inaugurating another movement, Christian socialism. In this, Maurice transformed Hooker's emphasis on the incarnational nature of Anglican spirituality to an imperative for social justice.
In the 19th century, Anglican biblical scholarship began to assume a distinct character, represented by the so-called "Cambridge triumvirate" of Joseph Lightfoot, F. J. A. Hort, and Brooke Foss Westcott. Their orientation is best summed up by Westcott's observation that "Life which Christ is and which Christ communicates, the life which fills our whole beings as we realise its capacities, is active fellowship with God."
The earlier part of the 20th century is marked by Charles Gore, with his emphasis on natural revelation, and William Temple's focus on Christianity and society, while, from outside England, Robert Leighton, Archbishop of Glasgow, and several clergy from the United States have been suggested, such as William Porcher DuBose, John Henry Hobart (1775–1830, Bishop of New York 1816–30), William Meade, Phillips Brooks, and Charles Brent.
### Churchmanship.
"Churchmanship" can be defined as the manifestation of theology in the realms of liturgy, piety and, to some extent, spirituality. Anglican diversity in this respect has tended to reflect the diversity in the tradition's Reformed and Catholic identity. Different individuals, groups, parishes, dioceses and provinces may identify more closely with one or the other, or some mixture of the two.
The range of Anglican belief and practice became particularly divisive during the 19th century, when some clergy were disciplined and even imprisoned on charges of introducing illegal ritual while, at the same time, others were criticised for engaging in public worship services with ministers of Reformed churches. Resistance to the growing acceptance and restoration of traditional Catholic ceremonial by the mainstream of Anglicanism ultimately led to the formation of small breakaway churches such as the Free Church of England in England (1844) and the Reformed Episcopal Church in North America (1873).
Anglo-Catholic (and some broad-church) Anglicans celebrate public liturgy in ways that understand worship to be something very special and of utmost importance. Vestments are worn by the clergy, sung settings are often used, and incense may be used. Nowadays, in most Anglican churches, the Eucharist is celebrated in a manner similar to the usage of Roman Catholics and some Lutherans, though, in many churches, more traditional, "pre–Vatican II" models of worship are common (e.g., an "eastward orientation" at the altar). Whilst many Anglo-Catholics derive much of their liturgical practice from that of the pre-Reformation English church, others more closely follow traditional Roman Catholic practices.
The Eucharist may sometimes be celebrated in the form known as High Mass, with a priest, deacon and subdeacon (usually actually a layperson) dressed in traditional vestments, with incense and sanctus bells and prayers adapted from the Roman Missal or other sources by the celebrant. Such churches may also have forms of eucharistic adoration such as Benediction of the Blessed Sacrament. In terms of personal piety, some Anglicans may recite the Rosary and Angelus, be involved in a devotional society dedicated to "Our Lady" (the Blessed Virgin Mary) and seek the intercession of the saints.
In recent decades, the prayer books of several provinces have, out of deference to a greater agreement with Eastern Conciliarism (and a perceived greater respect accorded Anglicanism by Eastern Orthodoxy than by Roman Catholicism), instituted a number of historically Eastern and Oriental Orthodox elements in their liturgies, including introduction of the Trisagion and deletion of the filioque clause from the Nicene Creed.
For their part, those evangelical (and some broad-church) Anglicans who emphasise the more Protestant aspects of the Church stress the Reformation theme of salvation by grace through faith. They emphasise the two dominical sacraments of Baptism and Eucharist, viewing the other five as "lesser rites". Some evangelical Anglicans may even tend to take the inerrancy of scripture literally, adopting the view of Article VI that it contains all things necessary to salvation in an explicit sense. Worship in churches influenced by these principles tends to be significantly less elaborate, with greater emphasis on the Liturgy of the Word (the reading of the scriptures, the sermon, and the intercessory prayers).
The Order for Holy Communion may be celebrated bi-weekly or monthly (in preference to the daily offices), by priests attired in choir habit, or more regular clothes, rather than Eucharistic vestments. Ceremony may be in keeping with their view of the provisions of the 17th-century Puritans – being a Reformed interpretation of the Ornaments Rubric – no candles, no incense, no bells, and a minimum of manual actions by the presiding celebrant (such as touching the elements at the Words of Institution).
In recent decades, there has been a growth of charismatic worship among Anglicans. Both Anglo-Catholics and evangelicals have been affected by this movement such that it is not uncommon to find typically charismatic postures, music, and other themes evident during the services of otherwise Anglo-Catholic or evangelical parishes.
The spectrum of Anglican beliefs and practice is too large to be fit into these labels. Many Anglicans locate themselves somewhere in the spectrum of the broad-church tradition and consider themselves an amalgam of evangelical and Catholic. Such Anglicans stress that Anglicanism is the "via media" (middle way) between the two major strains of Western Christianity and that Anglicanism is like a "bridge" between the two strains.
### Sacramental doctrine and practice.
In accord with its prevailing self-identity as a "via media" or "middle path" of Western Christianity, Anglican sacramental theology expresses elements in keeping with its status as being both a church in the Catholic tradition as well as a Reformed church. With respect to sacramental theology, the Catholic heritage is perhaps most strongly asserted in the importance Anglicanism places on the sacraments as a means of grace, sanctification, and salvation, as expressed in the church's liturgy and doctrine.
Of the seven sacraments, all Anglicans recognise Baptism and the Eucharist as being directly instituted by Christ. The other five – Confession/Absolution, Matrimony, Confirmation, Holy Orders (also called Ordination), and Anointing of the Sick (also called Unction) – are regarded variously as full sacraments by Anglo-Catholics and many high church and some broad-church Anglicans, but merely as "sacramental rites" by other broad-church and low-church Anglicans, especially evangelicals associated with Reform UK and the Diocese of Sydney.
#### Eucharistic theology.
Anglican eucharistic theology is divergent in practice, reflecting the essential comprehensiveness of the tradition. A few low-church Anglicans take a strictly memorialist (Zwinglian) view of the sacrament. In other words, they see Holy Communion as a memorial to Christ's suffering, and participation in the Eucharist as both a re-enactment of the Last Supper and a foreshadowing of the heavenly banquet – the fulfilment of the eucharistic promise.
Other low-church Anglicans believe in the real presence of Christ in the Eucharist but deny that the presence of Christ is carnal or is necessarily localised in the bread and wine. Despite explicit criticism in the Thirty-Nine Articles, many high-church or Anglo-Catholic Anglicans hold, more or less, the Catholic view of the real presence as expressed in the doctrine of transubstantiation, seeing the Eucharist as a liturgical representation of Christ's atoning sacrifice with the elements actually transformed into Christ's body and blood.
The majority of Anglicans, however, have in common a belief in the real presence, defined in one way or another. To that extent, they are in the company of the continental reformer Martin Luther and Calvin rather than Ulrich Zwingli. The Catechism of the American BCP of 1976 repeats the standard Anglican view ("The outward and visible sign in the Eucharist is the bread and wine"..."The inward and spiritual grace in the Holy Communion is the Body and Blood of Christ given to his people, and received by faith") without further definition. It should be remembered that Anglicanism has no official doctrine on this matter, believing it is wiser to leave the Presence a mystery. The faithful can believe privately whatever explanation they favor, be it transubstantiation, consubstantiation, receptionism, or virtualism (the two most congenial to Anglicans for centuries until the Oxford Movement), each of which espouses belief in the real presence in one way or another, or memorialism, which has never been an option with Anglicans.
A famous Anglican aphorism regarding Christ's presence in the sacrament, commonly misattributed to Queen Elizabeth I, is first found in print in a poem by John Donne:
&lt;poem&gt;
He was the word that spake it,
He took the bread and brake it:
And what that word did make it,
I do believe and take it.&lt;/poem&gt;
An Anglican position on the eucharistic sacrifice ("Sacrifice of the Mass") was expressed in the response "Saepius officio" of the Archbishops of Canterbury and York to Pope Leo XIII's Papal Encyclical "Apostolicae curae": viz. that the Prayer Book contained a strong sacrificial theology. Later revisions of the Prayer Book influenced by the Scottish Canon of 1764 first adopted by the Protestant Episcopal Church in 1789 made this assertion quite evident: "we do make and celebrate before thy Divine Majesty with these thy holy gifts, which we now OFFER unto thee, the memorial thy Son has commanded us to make", which is repeated in the 1929 English BCP and included in such words or others such as "present" or "show forth" in subsequent revisions.
Anglican and Roman Catholic representatives declared that they had "substantial agreement on the doctrine of the Eucharist" in the "Windsor Statement on Eucharistic Doctrine" by the Anglican-Roman Catholic International Consultation (1971) and the Elucidation of the ARCIC Windsor Statement (1979). The final response (1991) to these documents by the Vatican made it plain that it did not consider the degree of agreement reached to be satisfactory.
## Practices.
In Anglicanism, there is a distinction between liturgy, which is the formal public and communal worship of the Church, and personal prayer and devotion, which may be public or private. Liturgy is regulated by the prayer books and consists of the Holy Eucharist (some call it Holy Communion or Mass), the other six Sacraments, and the Divine Office or Liturgy of the Hours.
### Book of Common Prayer.
The "Book of Common Prayer" (BCP) is the foundational prayer book of Anglicanism. The original book of 1549 (revised in 1552) was one of the instruments of the English Reformation, replacing the various "uses" or rites in Latin that had been used in different parts of the country with a single compact volume in the language of the people, so that "now from henceforth all the Realm shall have but one use". Suppressed under Queen Mary I, it was revised in 1559, and then again in 1662, after the Restoration of Charles II. This version was made mandatory in England and Wales by the Act of Uniformity and was in standard use until the mid-20th century.
With British colonial expansion from the 17th century onwards, Anglican churches were planted around the globe. These churches at first used and then revised the "Book of Common Prayer" until they, like their parent church, produced prayer books which took into account the developments in liturgical study and practice in the 19th and 20th centuries, which come under the general heading of the Liturgical Movement.
### Worship.
Anglican worship services are open to all visitors. Anglican worship originates principally in the reforms of Thomas Cranmer, who aimed to create a set order of service like that of the pre-Reformation church but less complex in its seasonal variety and said in English rather than Latin. This use of a set order of service is not unlike the Catholic tradition. Traditionally, the pattern was that laid out in the "Book of Common Prayer". Although many Anglican churches now use a wide range of modern service books written in the local language, the structures of the "Book of Common Prayer" are largely retained. Churches which call themselves Anglican will have identified themselves so because they use some form or variant of the "Book of Common Prayer" in the shaping of their worship.
Anglican worship, however, is as diverse as Anglican theology. A contemporary "low-church" service may differ little from the worship of many mainstream non-Anglican Protestant churches. The service is constructed around a sermon focused on Biblical exposition and opened with one or more Bible readings and closed by a series of prayers (both set and extemporised) and hymns or songs. A "high church" or Anglo-Catholic service, by contrast, is usually a more formal liturgy celebrated by clergy in distinctive vestments and may be almost indistinguishable from a Roman Catholic service, often resembling the "pre–Vatican II" Tridentine rite.
Between these extremes are a variety of styles of worship, often involving a robed choir and the use of the organ to accompany the singing and to provide music before and after the service. Anglican churches tend to have pews or chairs, and it is usual for the congregation to kneel for some prayers but to stand for hymns and other parts of the service such as the Gloria, Collect, Gospel reading, Creed and either the Preface or all of the Eucharistic Prayer. Anglicans may genuflect or cross themselves in the same way as Roman Catholics.
Other more traditional Anglicans tend to follow the 1662 "Book of Common Prayer", and retain the use of the King James Bible. This is typical in many Anglican cathedrals and particularly in Royal Peculiars such as the Savoy Chapel and the Queen's Chapel. These services reflect older Anglican liturgies and differ from the Traditional Anglican Communion in that they are in favour of women priests and the ability of clergy to marry. These Anglican church services include classical music instead of songs, hymns from the New English Hymnal (usually excluding modern hymns such as "Lord of the Dance"), and are generally non-evangelical and formal in practice.
Until the mid-20th century the main Sunday service was typically morning prayer, but the Eucharist has once again become the standard form of Sunday worship in many Anglican churches; this again is similar to Roman Catholic practice. Other common Sunday services include an early morning Eucharist without music, an abbreviated Eucharist following a service of morning prayer, and a service of evening prayer, sometimes in the form of sung Evensong, usually celebrated between 3 and 6 pm. The late-evening service of Compline was revived in parish use in the early 20th century. Many Anglican churches will also have daily morning and evening prayer, and some have midweek or even daily celebration of the Eucharist.
An Anglican service (whether or not a Eucharist) will include readings from the Bible that are generally taken from a standardised lectionary, which provides for much of the Bible (and some passages from the Apocrypha) to be read out loud in the church over a cycle of one, two, or three years (depending on which eucharistic and office lectionaries are used, respectively). The sermon (or homily) is typically about ten to twenty minutes in length, often comparably short to sermons in evangelical churches. Even in the most informal Anglican services, it is common for set prayers such as the weekly Collect to be read. There are also set forms for intercessory prayer, though this is now more often extemporaneous. In high and Anglo-Catholic churches there are generally prayers for the dead.
Although Anglican public worship is usually ordered according to the canonically approved services, in practice many Anglican churches use forms of service outside these norms. Liberal churches may use freely structured or experimental forms of worship, including patterns borrowed from ecumenical traditions such as those of the Taizé Community or the Iona Community.
Anglo-Catholic parishes might use the modern Roman Catholic liturgy of the Mass or more traditional forms, such as the Tridentine Mass (which is translated into English in the English Missal), the Anglican Missal, or, less commonly, the Sarum Rite. Catholic devotions such as the Rosary, Angelus, and Benediction of the Blessed Sacrament are also common among Anglo-Catholics.
#### Eucharistic discipline.
Only baptised persons are eligible to receive communion, although in many churches communion is restricted to those who have not only been baptised but also confirmed. In many Anglican provinces, however, all baptised Christians are now often invited to receive communion and some dioceses have regularised a system for admitting baptised young people to communion before they are confirmed.
The discipline of fasting before communion is practised by some Anglicans. Most Anglican priests require the presence of at least one other person for the celebration of the Eucharist (referring back to Christ's statement in Matthew 18:20, "When two or more are gathered in my name, I will be in the midst of them."), though some Anglo-Catholic priests (like Roman Catholic priests) may say private Masses. As in the Roman Catholic Church, it is a canonical requirement to use fermented wine for communion.
Unlike in Roman Catholicism, the consecrated bread and wine are always offered to the congregation at a eucharistic service ("communion in both kinds"). This practice is becoming more frequent in the Roman Catholic Church as well, especially through the Neocatechumenal Way. In some churches, the sacrament is reserved in a tabernacle or aumbry with a lighted candle or lamp nearby. In Anglican churches, only a priest or a bishop may be the celebrant at the Eucharist.
### Divine office.
All Anglican prayer books contain offices for Morning Prayer (Matins) and Evening Prayer (Evensong). In the original "Book of Common Prayer", these were derived from combinations of the ancient monastic offices of Matins and Lauds; and Vespers and Compline, respectively. The prayer offices have an important place in Anglican history.
Prior to the Catholic revival of the 19th century, which eventually restored the Holy Eucharist as the principal Sunday liturgy, and especially during the 18th century, a morning service combining Matins, the Litany, and ante-Communion comprised the usual expression of common worship, while Matins and Evensong were sung daily in cathedrals and some collegiate chapels. This nurtured a tradition of distinctive Anglican chant applied to the canticles and psalms used at the offices (although plainsong is often used as well).
In some official and many unofficial Anglican service books, these offices are supplemented by other offices such as the Little Hours of Prime and prayer during the day such as (Terce, Sext, None, and Compline). Some Anglican monastic communities have a Daily Office based on that of the "Book of Common Prayer" but with additional antiphons and canticles, etc., for specific days of the week, specific psalms, etc. See, for example, Order of the Holy Cross and Order of St Helena, editors, "A Monastic Breviary" (Wilton, Conn.: Morehouse-Barlow, 1976). The All Saints Sisters of the Poor, with convents in Catonsville, Maryland, and elsewhere, use an elaborated version of the Anglican Daily Office. The Society of St. Francis publishes "Celebrating Common Prayer", which has become especially popular for use among Anglicans.
In England, the United States, Canada, Australia, New Zealand, and some other Anglican provinces, the modern prayer books contain four offices:
In addition, most prayer books include a section of prayers and devotions for family use. In the US, these offices are further supplemented by an "Order of Worship for the Evening", a prelude to or an abbreviated form of Evensong, partly derived from Orthodox prayers. In the United Kingdom, the publication of "Daily Prayer", the third volume of "Common Worship", was published in 2005. It retains the services for Morning and Evening Prayer and Compline and includes a section entitled "Prayer during the Day". "A New Zealand Prayer Book" of 1989 provides different outlines for Matins and Evensong on each day of the week, as well as "Midday Prayer", "Night Prayer" and "Family Prayer".
Some Anglicans who pray the office on daily basis use the present Divine Office of the Roman Catholic Church. In many cities, especially in England, Anglican and Roman Catholic priests and lay people often meet several times a week to pray the office in common. A small but enthusiastic minority use the Anglican Breviary, or other translations and adaptations of the pre–Vatican II Roman Rite and Sarum Rite, along with supplemental material from cognate western sources, to provide such things as a common of Octaves, a common of Holy Women, and other additional material. Others may privately use idiosyncratic forms borrowed from a wide range of Christian traditions.
#### "Quires and Places where they sing".
In the late medieval period, many English cathedrals and monasteries had established small choirs of trained lay clerks and boy choristers to perform polyphonic settings of the Mass in their Lady chapels. Although these "Lady Masses" were discontinued at the Reformation, the associated musical tradition was maintained in the Elizabethan Settlement through the establishment of choral foundations for daily singing of the Divine Office by expanded choirs of men and boys. This resulted from an explicit addition by Elizabeth herself to the injunctions accompanying the 1559 "Book of Common Prayer" (that had itself made no mention of choral worship) by which existing choral foundations and choir schools were instructed to be continued, and their endowments secured. Consequently, some thirty-four cathedrals, collegiate churches, and royal chapels maintained paid establishments of lay singing men and choristers in the late 16th century.
All save four of these have – with interruptions during the Commonwealth and the COVID-19 pandemic – continued daily choral prayer and praise to this day. In the Offices of Matins and Evensong in the 1662 "Book of Common Prayer", these choral establishments are specified as "Quires and Places where they sing".
For nearly three centuries, this round of daily professional choral worship represented a tradition entirely distinct from that embodied in the intoning of Parish Clerks, and the singing of "west gallery choirs" which commonly accompanied weekly worship in English parish churches. In 1841, the rebuilt Leeds Parish Church established a surpliced choir to accompany parish services, drawing explicitly on the musical traditions of the ancient choral foundations. Over the next century, the Leeds example proved immensely popular and influential for choirs in cathedrals, parish churches, and schools throughout the Anglican communion. More or less extensively adapted, this choral tradition also became the direct inspiration for robed choirs leading congregational worship in a wide range of Christian denominations.
In 1719, the cathedral choirs of Gloucester, Hereford, and Worcester combined to establish the annual Three Choirs Festival, the precursor for the multitude of summer music festivals since. By the 20th century, the choral tradition had become for many the most accessible face of worldwide Anglicanism – especially as promoted through the regular broadcasting of choral evensong by the BBC; and also in the annual televising of the festival of Nine Lessons and Carols from King's College, Cambridge. Composers closely concerned with this tradition include Edward Elgar, Ralph Vaughan Williams, Gustav Holst, Charles Villiers Stanford, and Benjamin Britten. A number of important 20th-century works by non-Anglican composers were originally commissioned for the Anglican choral tradition – for example, the "Chichester Psalms" of Leonard Bernstein and the "Nunc dimittis" of Arvo Pärt.
## Organisation of the Anglican Communion.
### Principles of governance.
Contrary to popular misconception, the British monarch is not the constitutional "head" but in law the "Supreme Governor" of the Church of England, nor does he or she have any role in provinces outside England. The role of the crown in the Church of England is practically limited to the appointment of bishops, including the Archbishop of Canterbury, and even this role is limited, as the Church presents the government with a short list of candidates from which to choose. This process is accomplished through collaboration with and consent of ecclesial representatives "(see Ecclesiastical Commissioners)". The monarch has no constitutional role in Anglican churches in other parts of the world, although the prayer books of several countries where she is head of state maintain prayers for her as sovereign.
A characteristic of Anglicanism is that it has no international juridical authority. All 39 provinces of the Anglican Communion are autonomous, each with their own primate and governing structure. These provinces may take the form of national churches (such as in Canada, Uganda or Japan) or a collection of nations (such as the West Indies, Central Africa or South Asia), or geographical regions (such as Vanuatu and Solomon Islands) etc. Within these provinces there may exist subdivisions, called ecclesiastical provinces, under the jurisdiction of a metropolitan archbishop.
All provinces of the Anglican Communion consist of dioceses, each under the jurisdiction of a bishop. In the Anglican tradition, bishops must be consecrated according to the strictures of apostolic succession, which Anglicans consider one of the marks of Catholicity. Apart from bishops, there are two other orders of ordained ministry: deacon and priest.
No requirement is made for clerical celibacy, though many Anglo-Catholic priests have traditionally been bachelors. Because of innovations that occurred at various points after the latter half of the 20th century, women may be ordained as deacons in almost all provinces, as priests in most and as bishops in many. Anglican religious orders and communities, suppressed in England during the Reformation, have re-emerged, especially since the mid-19th century, and now have an international presence and influence.
Government in the Anglican Communion is synodical, consisting of three houses of laity (usually elected parish representatives), clergy and bishops. National, provincial and diocesan synods maintain different scopes of authority, depending on their canons and constitutions. Anglicanism is not congregational in its polity: it is the diocese, not the parish church, which is the smallest unit of authority in the church. "(See Episcopal polity)".
### Archbishop of Canterbury.
The Archbishop of Canterbury has a precedence of honour over the other primates of the Anglican Communion, and for a province to be considered a part of the communion means specifically to be in full communion with the see of Canterbury – though this principle is currently subject to considerable debate, especially among those in the so-called Global South, including American Anglicans. The archbishop is, therefore, recognised as "primus inter pares" ("first amongst equals"), even though he does not exercise any direct authority in any province outside England, of which he is chief primate. Rowan Williams, the Archbishop of Canterbury from 2002 to 2012, was the first archbishop appointed from outside the Church of England since the Reformation: he was formerly the Archbishop of Wales.
As "spiritual head" of the Communion, the Archbishop of Canterbury maintains a certain moral authority, and has the right to determine which churches will be in communion with his see. He hosts and chairs the Lambeth Conferences of Anglican Communion bishops, and decides who will be invited to them. He also hosts and chairs the Anglican Communion Primates' Meeting and is responsible for the invitations to it. He acts as president of the secretariat of the Anglican Communion Office and its deliberative body, the Anglican Consultative Council.
### Conferences.
The Anglican Communion has no international juridical organisation. All international bodies are consultative and collaborative, and their resolutions are not legally binding on the autonomous provinces of the Communion. There are three international bodies of note.
### Ordained ministry.
Like the Roman Catholic Church and the Orthodox churches, the Anglican Communion maintains the threefold ministry of deacons, presbyters (usually called "priests"), and bishops.
#### Episcopate.
Bishops, who possess the fullness of Christian priesthood, are the successors of the apostles. Primates, archbishops, and metropolitans are all bishops and members of the historical episcopate who derive their authority through apostolic succession – an unbroken line of bishops that can be traced back to the 12 apostles of Jesus.
#### Priesthood.
Bishops are assisted by priests and deacons. Most ordained ministers in the Anglican Communion are priests, who usually work in parishes within a diocese. Priests are in charge of the spiritual life of parishes and are usually called the rector or vicar. A curate (or, more correctly, an "assistant curate") is a priest or deacon who assists the parish priest. Non-parochial priests may earn their living by any vocation, although employment by educational institutions or charitable organisations is most common. Priests also serve as chaplains of hospitals, schools, prisons, and in the armed forces.
An archdeacon is a priest or deacon responsible for administration of an archdeaconry, which is often the name given to the principal subdivisions of a diocese. An archdeacon represents the diocesan bishop in his or her archdeaconry. In the Church of England, the position of archdeacon can only be held by someone in priestly orders who has been ordained for at least six years. In some other parts of the Anglican Communion, the position can also be held by deacons. In parts of the Anglican Communion where women cannot be ordained as priests or bishops but can be ordained as deacons, the position of archdeacon is effectively the most senior office to which an ordained woman can be appointed.
A dean is a priest who is the principal cleric of a cathedral or other collegiate church and the head of the chapter of canons. If the cathedral or collegiate church has its own parish, the dean is usually also rector of the parish. However, in the Church of Ireland, the roles are often separated, and most cathedrals in the Church of England do not have associated parishes. In the Church in Wales, however, most cathedrals are parish churches and their deans are now also vicars of their parishes.
The Anglican Communion recognises Roman Catholic and Eastern Orthodox ordinations as valid. Outside the Anglican Communion, Anglican ordinations (at least of male priests) are recognised by the Old Catholic Church, Porvoo Communion Lutherans, and various Independent Catholic churches.
#### Diaconate.
In Anglican churches, including the Free Church of England, deacons often work directly in ministry to the marginalised inside and outside the church: the poor, the sick, the hungry, the imprisoned. Unlike Orthodox and most Roman Catholic deacons who may be married only before ordination, deacons are permitted to marry freely both before and after ordination, as are priests. Most deacons are preparing for priesthood and usually only remain as deacons for about a year before being ordained priests. However, there are some deacons who remain so.
Many provinces of the Anglican Communion ordain both men and women as deacons. Many of those provinces that ordain women to the priesthood previously allowed them to be ordained only to the diaconate. The effect of this was the creation of a large and overwhelmingly female diaconate for a time, as most men proceeded to be ordained priest after a short time as a deacon.
Deacons, in some dioceses, can be granted licences to solemnise matrimony, usually under the instruction of their parish priest and bishop. They sometimes officiate at Benediction of the Blessed Sacrament in churches which have this service. Deacons are not permitted to preside at the Eucharist (but can lead worship with the distribution of already consecrated communion where this is permitted), absolve sins, or pronounce a blessing. It is the prohibition against deacons pronouncing blessings that leads some to believe that deacons cannot solemnise matrimony.
### Laity.
All baptised members of the church are called Christian faithful, truly equal in dignity and in the work to build the church. Some non-ordained people also have a formal public ministry, often on a full-time and long-term basis – such as lay readers (also known as readers), churchwardens, vergers, and sextons. Other lay positions include acolytes (male or female, often children), lay eucharistic ministers (also known as chalice bearers), and lay eucharistic visitors (who deliver consecrated bread and wine to "shut-ins" or members of the parish who are unable to leave home or hospital to attend the Eucharist). Lay people also serve on the parish altar guild (preparing the altar and caring for its candles, linens, flowers, etc.), in the choir and as cantors, as ushers and greeters, and on the church council (called the "vestry" in some countries), which is the governing body of a parish.
### Religious orders.
A small yet influential aspect of Anglicanism is its religious orders and communities. Shortly after the beginning of the Catholic Revival in the Church of England, there was a renewal of interest in re-establishing religious and monastic orders and communities. One of Henry VIII's earliest acts was their dissolution and seizure of their assets. In 1841, Marian Rebecca Hughes became the first woman to take the vows of religion in communion with the Province of Canterbury since the Reformation. In 1848, Priscilla Lydia Sellon became the superior of the Society of the Most Holy Trinity at Devonport, Plymouth, the first organised religious order. Sellon is called "the restorer, after three centuries, of the religious life in the Church of England". For the next one hundred years, religious orders for both men and women proliferated throughout the world, becoming a numerically small but disproportionately influential feature of global Anglicanism.
Anglican religious life at one time boasted hundreds of orders and communities, and thousands of religious. An important aspect of Anglican religious life is that most communities of both men and women lived their lives consecrated to God under the vows of poverty, chastity, and obedience (or, in Benedictine communities, Stability, Conversion of Life, and Obedience) by practising a mixed life of reciting the full eight services of the Breviary in choir, along with a daily Eucharist, plus service to the poor. The mixed life, combining aspects of the contemplative orders and the active orders, remains to this day a hallmark of Anglican religious life. Another distinctive feature of Anglican religious life is the existence of some mixed-gender communities.
Since the 1960s, there has been a sharp decline in the number of professed religious in most parts of the Anglican Communion, especially in North America, Europe, and Australia. Many once large and international communities have been reduced to a single convent or monastery with memberships of elderly men or women. In the last few decades of the 20th century, novices have for most communities been few and far between. Some orders and communities have already become extinct. There are, however, still thousands of Anglican religious working today in approximately 200 communities around the world, and religious life in many parts of the Communion – especially in developing nations – flourishes.
The most significant growth has been in the Melanesian countries of the Solomon Islands, Vanuatu, and Papua New Guinea. The Melanesian Brotherhood, founded at Tabalia, Guadalcanal, in 1925 by Ini Kopuria, is now the largest Anglican Community in the world, with over 450 brothers in the Solomon Islands, Vanuatu, Papua New Guinea, the Philippines, and the United Kingdom. The Sisters of the Church, started by Mother Emily Ayckbowm in England in 1870, has more sisters in the Solomons than all their other communities. The Community of the Sisters of Melanesia, started in 1980 by Sister Nesta Tiboe, is a growing community of women throughout the Solomon Islands.
The Society of Saint Francis, founded as a union of various Franciscan orders in the 1920s, has experienced great growth in the Solomon Islands. Other communities of religious have been started by Anglicans in Papua New Guinea and in Vanuatu. Most Melanesian Anglican religious are in their early to mid-20s – vows may be temporary and it is generally assumed that brothers, at least, will leave and marry in due course – making the average age 40 to 50 years younger than their brothers and sisters in other countries. Growth of religious orders, especially for women, is marked in certain parts of Africa.
### Worldwide distribution.
Anglicanism represents the third largest Christian communion in the world, after the Roman Catholic Church and the Eastern Orthodox Church. The number of Anglicans in the world is over 85 million . The 11 provinces in Africa saw growth in the last two decades. They now include 36.7 million members, more Anglicans than there are in England. England remains the largest single Anglican province, with 26 million members. In most industrialised countries, church attendance has decreased since the 19th century. Anglicanism's presence in the rest of the world is due to large-scale emigration, the establishment of expatriate communities, or the work of missionaries.
The Church of England has been a church of missionaries since the 17th century, when the Church first left English shores with colonists who founded what would become the United States, Australia, Canada, New Zealand, and South Africa, and established Anglican churches. For example, an Anglican chaplain, Robert Wolfall, with Martin Frobisher's Arctic expedition, celebrated the Eucharist in 1578 in Frobisher Bay.
The first Anglican church in the Americas was built at Jamestown, Virginia, in 1607. By the 18th century, missionaries worked to establish Anglican churches in Asia, Africa, and Latin America. The great Church of England missionary societies were founded; for example, the Society for Promoting Christian Knowledge (SPCK) in 1698, the Society for the Propagation of the Gospel in Foreign Parts (SPG) in 1701, and the Church Mission Society (CMS) in 1799.
The 19th century saw the founding and expansion of social-oriented evangelism with societies such as the Church Pastoral Aid Society (CPAS) in 1836, Mission to Seafarers in 1856, Girls' Friendly Society (GFS) in 1875, Mothers' Union in 1876, and Church Army in 1882, all carrying out a personal form of evangelism.
The 20th century saw the Church of England developing new forms of evangelism such as the Alpha course in 1990, which was developed and propagated from Holy Trinity Brompton Church in London. In the 21st century, there has been renewed effort to reach children and youth. Fresh expressions is a Church of England missionary initiative to youth begun in 2005, and has ministries at a skate park through the efforts of St George's Church, Benfleet, Essex – Diocese of Chelmsford – or youth groups with evocative names, like the C.L.A.W (Christ Little Angels – Whatever!) youth group at Coventry Cathedral. And for the unchurched who do not actually wish to visit a brick and mortar church, there are Internet ministries such as the Diocese of Oxford's online Anglican i-Church, which appeared on the web in 2005.
### Ecumenism.
Anglican interest in ecumenical dialogue can be traced back to the time of the Reformation and dialogues with both Orthodox and Lutheran churches in the 16th century. In the 19th century, with the rise of the Oxford Movement, there arose greater concern for reunion of the churches of "Catholic confession". This desire to work towards full communion with other denominations led to the development of the Chicago-Lambeth Quadrilateral, approved by the third Lambeth Conference of 1888. The four points (the sufficiency of scripture, the historic creeds, the two dominical sacraments, and the historic episcopate) were proposed as a basis for discussion, although they have frequently been taken as a non-negotiable bottom-line for any form of reunion.
### Theological diversity.
Anglicanism in general has always sought a balance between the emphases of Catholicism and Protestantism, while tolerating a range of expressions of evangelicalism and ceremony. Clergy and laity from all Anglican churchmanship traditions have been active in the formation of the Continuing movement.
While there are high church, broad-church and low-church Continuing Anglicans, many Continuing churches are Anglo-Catholic with highly ceremonial liturgical practices. Others belong to a more evangelical or low-church tradition and tend to support the Thirty-nine Articles and simpler worship services. Morning Prayer, for instance, is often used instead of the Holy Eucharist for Sunday worship services, although this is not necessarily true of all low-church parishes.
Most Continuing churches in the United States reject the 1979 revision of the "Book of Common Prayer" by the Episcopal Church and use the 1928 version for their services instead. In addition, Anglo-Catholic bodies may use the Anglican Missal, Anglican Service Book or English Missal when celebrating Mass.
#### Conflicts within Anglicanism.
A changing focus on social issues after the Second World War led to Lambeth Conference resolutions countenancing contraception and the remarriage of divorced persons. Eventually, most provinces approved the ordination of women. In more recent years, some jurisdictions have permitted the ordination of people in same-sex relationships and authorised rites for the blessing of same-sex unions (see Homosexuality and Anglicanism). "The more liberal provinces that are open to changing Church doctrine on marriage in order to allow for same-sex unions include Brazil, Canada, New Zealand, Scotland, South India, South Africa, the US and Wales," while the more conservative provinces are primarily located in the Global South.
The lack of social consensus among and within provinces of diverse cultural traditions has resulted in considerable conflict and even schism concerning some or all of these developments (see Anglican realignment). More conservative elements within and outside of Anglicanism (primarily African churches and factions within North American Anglicanism) have opposed these changes, while some liberal and moderate Anglicans see this opposition as representing a new fundamentalism within Anglicanism and "believe a split is inevitable and preferable to continued infighting and paralysis." Some Anglicans opposed to various liberalising changes, in particular the ordination of women, have become Roman Catholics or Orthodox. Others have, at various times, joined the Continuing Anglican movement.
## Continuing Anglican movement.
The term "Continuing Anglicanism" refers to a number of church bodies which have formed outside of the Anglican Communion in the belief that traditional forms of Anglican faith, worship, and order have been unacceptably revised or abandoned within some Anglican Communion churches in recent decades. They therefore claim that they are "continuing" traditional Anglicanism.
The modern Continuing Anglican movement principally dates to the Congress of St. Louis, held in the United States in 1977, where participants rejected changes that had been made in the Episcopal Church's "Book of Common Prayer" and also the Episcopal Church's approval of the ordination of women to the priesthood. More recent changes in the North American churches of the Anglican Communion, such as the introduction of same-sex marriage rites and the ordination of gay and lesbian people to the priesthood and episcopate, have created further separations.
Continuing churches have generally been formed by people who have left the Anglican Communion. The original Anglican churches are charged by the Continuing Anglicans with being greatly compromised by secular cultural standards and liberal theology. Many Continuing Anglicans believe that the faith of some churches in communion with the Archbishop of Canterbury has become unorthodox and therefore have not sought to also be in communion with him.
The original continuing parishes in the United States were found mainly in metropolitan areas. Since the late 1990s, a number have appeared in smaller communities, often as a result of a division in the town's existing Episcopal churches. The 2007–08 "Directory of Traditional Anglican and Episcopal Parishes", published by the Fellowship of Concerned Churchmen, contained information on over 900 parishes affiliated with either the Continuing Anglican churches or the Anglican realignment movement, a more recent wave of Anglicans withdrawing from the Anglican Communion's North American provinces.
## Social activism.
A concern for social justice can be traced to very early Anglican beliefs, relating to an intertwined theology of God, nature, and humanity. The Anglican theologian Richard Hooker wrote in his book "The Works of that Learned and Judicious Divine" that "God hath created nothing simply for itself, but each thing in all things, and of every thing each part in other have such interest, that in the whole world nothing is found whereunto any thing created can say, 'I need thee not.'" Such statements demonstrate a theological Anglican interest in social activism, which has historically appeared in movements such as evangelical Anglican William Wilberforce's campaign against slavery in the 18th century, or 19th century issues concerning industrialisation.
### Working conditions and Christian socialism.
Lord Shaftesbury, a devout evangelical, campaigned to improve the conditions in factories, in mines, for chimney sweeps, and for the education of the very poor. For years, he was chairman of the Ragged School Board. Frederick Denison Maurice was a leading figure advocating reform, founding so-called "producer's co-operatives" and the Working Men's College. His work was instrumental in the establishment of the Christian socialist movement, although he himself was not in any real sense a socialist but "a Tory paternalist with the unusual desire to theories his acceptance of the traditional obligation to help the poor", influenced Anglo-Catholics such as Charles Gore, who wrote that "the principle of the incarnation is denied unless the Christian spirit can be allowed to concern itself with everything that interests and touches human life." Anglican focus on labour issues culminated in the work of William Temple in the 1930s and 1940s."
### Pacifism.
A question of whether or not Christianity is a pacifist religion has remained a matter of debate for Anglicans. The leading Anglican spokesman for pacifist ideas, from 1914 to 1945, was Ernest Barnes, bishop of Birmingham from 1924 to 1953. He opposed both world wars. In 1937, the Anglican Pacifist Fellowship emerged as a distinct reform organisation, seeking to make pacifism a clearly defined part of Anglican theology. The group rapidly gained popularity amongst Anglican intellectuals, including Vera Brittain, Evelyn Underhill, and the former British political leader George Lansbury. Furthermore, Dick Sheppard, who during the 1930s was one of Britain's most famous Anglican priests due to his landmark sermon broadcasts for BBC Radio, founded the Peace Pledge Union, a secular pacifist organisation for the non-religious that gained considerable support throughout the 1930s.
Whilst never actively endorsed by Anglican churches, many Anglicans unofficially have adopted the Augustinian "Just War" doctrine. The Anglican Pacifist Fellowship remains highly active throughout the Anglican world. It rejects this doctrine of "just war" and seeks to reform the Church by reintroducing the pacifism inherent in the beliefs of many of the earliest Christians and present in their interpretation of Christ's Sermon on the Mount. The principles of the Anglican Pacifist Fellowship are often formulated as a statement of belief that "Jesus' teaching is incompatible with the waging of war ... that a Christian church should never support or justify war ... [and] that our Christian witness should include opposing the waging or justifying of war."
Confusing the matter was the fact that the 37th Article of Religion in the "Book of Common Prayer" states that "it is lawful for Christian men, at the commandment of the Magistrate, to wear weapons, and serve in the wars." Therefore, the Lambeth Council in the modern era has sought to provide a clearer position by repudiating modern war and developed a statement that has been affirmed at each subsequent meeting of the council.
This statement was strongly reasserted when "the 67th General Convention of the Episcopal Church reaffirms the statement made by the Anglican Bishops assembled at Lambeth in 1978 and adopted by the 66th General Convention of the Episcopal Church in 1979, calling "Christian people everywhere ... to engage themselves in non-violent action for justice and peace and to support others so engaged, recognising that such action will be controversial and may be personally very costly... this General Convention, in obedience to this call, urges all members of this Church to support by prayer and by such other means as they deem appropriate, those who engaged in such non-violent action, and particularly those who suffer for conscience' sake as a result; and be it further Resolved, that this General Convention calls upon all members of this Church seriously to consider the implications for their own lives of this call to resist war and work for peace for their own lives."
### After World War II.
The focus on other social issues became increasingly diffuse after the Second World War. On the one hand, the growing independence and strength of Anglican churches in the Global South brought new emphasis to issues of global poverty, the inequitable distribution of resources, and the lingering effects of colonialism. In this regard, figures such as Desmond Tutu and Ted Scott were instrumental in mobilising Anglicans worldwide against the apartheid policies of South Africa. Rapid social change in the industrialised world during the 20th century compelled the church to examine issues of gender, sexuality, and marriage.
## Ordinariates within the Roman Catholic Church.
On 4 November 2009, Pope Benedict XVI issued an apostolic constitution, "Anglicanorum Coetibus", to allow groups of former Anglicans to enter into full communion with the Roman Catholic Church as members of personal ordinariates. 20 October 2009 announcement of the imminent constitution mentioned:
For each personal ordinariate, the ordinary may be a former Anglican bishop or priest. It was expected that provision would be made to allow the retention of aspects of Anglican liturgy; cf. Anglican Use.

</doc>
<doc id="1216" url="https://en.wikipedia.org/wiki?curid=1216" title="Athens">
Athens

Athens ( ; ; ) is the capital and largest city of Greece. Athens dominates the Attica region and is one of the world's oldest cities, with its recorded history spanning over 3,400 years and its earliest human presence beginning somewhere between the 11th and 7th millennia BC.
Classical Athens was a powerful city-state. It was a centre for the arts, learning and philosophy, and the home of Plato's Academy and Aristotle's Lyceum. It is widely referred to as the cradle of Western civilization and the birthplace of democracy, largely because of its cultural and political impact on the European continent—particularly Ancient Rome. In modern times, Athens is a large cosmopolitan metropolis and central to economic, financial, industrial, maritime, political and cultural life in Greece. In 2021, Athens' urban area hosted more than three and a half million people, which is around 35% of the entire population of Greece.
Athens is a Beta global city according to the Globalization and World Cities Research Network, and is one of the biggest economic centers in Southeastern Europe. It also has a large financial sector, and its port Piraeus is both the largest passenger port in Europe, and the second largest in the world.
The Municipality of Athens (also City of Athens), which actually constitutes a small administrative unit of the entire city, had a population of 664,046 (in 2011) within its official limits, and a land area of . The Athens Urban Area or Greater Athens extends beyond its administrative municipal city limits, with a population of 3,090,508 (in 2011) over an area of . According to Eurostat in 2011, the functional urban area of Athens was the 9th most populous in the European Union (the 6th most populous capital city of the EU), with a population of 3.8 million people. Athens is also the southernmost capital on the European mainland and the warmest major city in Europe.
The heritage of the Classical Era is still evident in the city, represented by ancient monuments, and works of art. The most famous of all being the Parthenon, considered a key landmark of early Western civilization. The city also retains Roman and Byzantine monuments, as well as a smaller number of Ottoman monuments, while its historical urban core features elements of continuity through its millennia of history. Athens is home to two UNESCO World Heritage Sites, the Acropolis of Athens and the medieval Daphni Monastery. Landmarks of the modern era, dating back to the establishment of Athens as the capital of the independent Greek state in 1834, includes the Hellenic Parliament and the so-called "Architectural Trilogy of Athens", consisting of the National Library of Greece, the National and Kapodistrian University of Athens, and the Academy of Athens. Athens is also home to several museums and cultural institutions, such as the National Archeological Museum, featuring the world's largest collection of ancient Greek antiquities, the Acropolis Museum, the Museum of Cycladic Art, the Benaki Museum, and the Byzantine and Christian Museum. Athens was the host city of the first modern-day Olympic Games in 1896, and 108 years later it hosted the 2004 Summer Olympics, making it one of the few cities to have hosted the Olympics more than once.
## Etymology and names.
In Ancient Greek, the name of the city was ("Athênai", in Classical Attic) a plural. In earlier Greek, such as Homeric Greek, the name had been current in the singular form though, as ("Athḗnē"). It was possibly rendered in the plural later on, like those of ("Thêbai") and ("Μukênai"). The root of the word is probably not of Greek or Indo-European origin, and is possibly a remnant of the Pre-Greek substrate of Attica. In antiquity, it was debated whether Athens took its name from its patron goddess Athena (Attic , "Athēnâ", Ionic , "Athḗnē", and Doric , "Athā́nā") or Athena took her name from the city. Modern scholars now generally agree that the goddess takes her name from the city, because the ending -"ene" is common in names of locations, but rare for personal names.
According to the ancient Athenian founding myth, Athena, the goddess of wisdom, competed against Poseidon, the God of the Seas, for patronage of the yet-unnamed city; they agreed that whoever gave the Athenians the better gift would become their patron and appointed Cecrops, the king of Athens, as the judge. According to the account given by Pseudo-Apollodorus, Poseidon struck the ground with his trident and a salt water spring welled up. In an alternative version of the myth from Vergil's "Georgics", Poseidon instead gave the Athenians the first horse. In both versions, Athena offered the Athenians the first domesticated olive tree. Cecrops accepted this gift and declared Athena the patron goddess of Athens. Eight different etymologies, now commonly rejected, have been proposed since the 17th century. Christian Lobeck proposed as the root of the name the word ("áthos") or ("ánthos") meaning "flower", to denote Athens as the "flowering city". Ludwig von Döderlein proposed the stem of the verb , stem θη- ("tháō", "thē-", "to suck") to denote Athens as having fertile soil. Athenians were called cicada-wearers () because they used to wear pins of golden cicadas. A symbol of being autochthon (earth-born), because the legendary founder of Athens, Erechtheus was an autochthon or of being musicians, because the cicada is a "musician" insect. In classical literature, the city was sometimes referred to as the City of the Violet Crown, first documented in Pindar's ἰοστέφανοι Ἀθᾶναι ("iostéphanoi Athânai"), or as ("tò kleinòn ásty", "the glorious city").
During the medieval period, the name of the city was rendered once again in the singular as . Variant names included Setines, Satine, and Astines, all derivations involving false splitting of prepositional phrases. King Alphonse X of Castile gives the pseudo-etymology 'the one without death/ignorance'. In Ottoman Turkish, it was called آتينا "Ātīnā", and in modern Turkish, it is "Atina".
After the establishment of the modern Greek state, and partly due to the conservatism of the written language, again became the official name of the city and remained so until the abandonment of Katharevousa in the 1970s, when Ἀθήνα, "Athína", became the official name. Today it is often simply called "ī protévousa" ; 'the capital'.
## History.
The oldest known human presence in Athens is the Cave of Schist, which has been dated to between the 11th and 7th millennia BC. Athens has been continuously inhabited for at least 5,000 years (3000 BC). By 1400 BC, the settlement had become an important centre of the Mycenaean civilization, and the Acropolis was the site of a major Mycenaean fortress, whose remains can be recognised from sections of the characteristic Cyclopean walls. Unlike other Mycenaean centers, such as Mycenae and Pylos, it is not known whether Athens suffered destruction in about 1200 BC, an event often attributed to a Dorian invasion, and the Athenians always maintained that they were pure Ionians with no Dorian element. However, Athens, like many other Bronze Age settlements, went into economic decline for around 150 years afterwards.
Iron Age burials, in the Kerameikos and other locations, are often richly provided for and demonstrate that from 900 BC onwards Athens was one of the leading centres of trade and prosperity in the region. The leading position of Athens may well have resulted from its central location in the Greek world, its secure stronghold on the Acropolis and its access to the sea, which gave it a natural advantage over inland rivals such as Thebes and Sparta.
By the 6th century BC, widespread social unrest led to the reforms of Solon. These would pave the way for the eventual introduction of democracy by Cleisthenes in 508 BC. Athens had by this time become a significant naval power with a large fleet, and helped the rebellion of the Ionian cities against Persian rule. In the ensuing Greco-Persian Wars Athens, together with Sparta, led the coalition of Greek states that would eventually repel the Persians, defeating them decisively at Marathon in 490 BC, and crucially at Salamis in 480 BC. However, this did not prevent Athens from being captured and sacked twice by the Persians within one year, after a heroic but ultimately failed resistance at Thermopylae by Spartans and other Greeks led by King Leonidas, after both Boeotia and Attica fell to the Persians.
The decades that followed became known as the Golden Age of Athenian democracy, during which time Athens became the leading city of Ancient Greece, with its cultural achievements laying the foundations for Western civilization. The playwrights Aeschylus, Sophocles and Euripides flourished in Athens during this time, as did the historians Herodotus and Thucydides, the physician Hippocrates, and the philosopher Socrates. Guided by Pericles, who promoted the arts and fostered democracy, Athens embarked on an ambitious building program that saw the construction of the Acropolis of Athens (including the Parthenon), as well as empire-building via the Delian League. Originally intended as an association of Greek city-states to continue the fight against the Persians, the league soon turned into a vehicle for Athens's own imperial ambitions. The resulting tensions brought about the Peloponnesian War (431–404 BC), in which Athens was defeated by its rival Sparta.
By the mid-4th century BC, the northern Greek kingdom of Macedon was becoming dominant in Athenian affairs. In 338 BC the armies of Philip II defeated an alliance of some of the Greek city-states including Athens and Thebes at the Battle of Chaeronea, effectively ending Athenian independence. Later, under Rome, Athens was given the status of a free city because of its widely admired schools. The Roman emperor Hadrian, in the 2nd century AD, ordered the construction of a library, a gymnasium, an aqueduct which is still in use, several temples and sanctuaries, a bridge and financed the completion of the Temple of Olympian Zeus.
By the end of Late Antiquity, Athens had shrunk due to sacks by the Herulians, Visigoths, and Early Slavs which caused massive destruction in the city. In this era, the first Christian churches were built in Athens, and the Parthenon and other temples were converted into churches. Athens expanded its settlement in the second half of the Middle Byzantine Period, in the 9th to 10th centuries AD, and was relatively prosperous during the Crusades, benefiting from Italian trade. After the Fourth Crusade the Duchy of Athens was established. In 1458 it was conquered by the Ottoman Empire and entered a long period of decline.
Following the Greek War of Independence and the establishment of the Greek Kingdom, Athens was chosen as the capital of the newly independent Greek state in 1834, largely because of historical and sentimental reasons. At the time, after the extensive destruction it had suffered during the war of independence, it was reduced to a town of about 4,000 people (less than half its earlier population ) in a loose swarm of houses along the foot of the Acropolis. The first King of Greece, Otto of Bavaria, commissioned the architects Stamatios Kleanthis and Eduard Schaubert to design a modern city plan fit for the capital of a state.
The first modern city plan consisted of a triangle defined by the Acropolis, the ancient cemetery of Kerameikos and the new palace of the Bavarian king (now housing the Greek Parliament), so as to highlight the continuity between modern and ancient Athens. Neoclassicism, the international style of this epoch, was the architectural style through which Bavarian, French and Greek architects such as Hansen, Klenze, Boulanger or Kaftantzoglou designed the first important public buildings of the new capital. In 1896, Athens hosted the first modern Olympic Games. During the 1920s a number of Greek refugees, expelled from Asia Minor after the Greco-Turkish War and Greek genocide, swelled Athens's population; nevertheless it was most particularly following World War II, and from the 1950s and 1960s, that the population of the city exploded, and Athens experienced a gradual expansion.
In the 1980s it became evident that smog from factories and an ever-increasing fleet of automobiles, as well as a lack of adequate free space due to congestion, had evolved into the city's most important challenge. A series of anti-pollution measures taken by the city's authorities in the 1990s, combined with a substantial improvement of the city's infrastructure (including the Attiki Odos motorway, the expansion of the Athens Metro, and the new Athens International Airport), considerably alleviated pollution and transformed Athens into a much more functional city. In 2004 Athens hosted the 2004 Summer Olympics.
## Geography.
Athens sprawls across the central plain of Attica that is often referred to as the Athens Basin or the Attica Basin (). The basin is bounded by four large mountains: Mount Aigaleo to the west, Mount Parnitha to the north, Mount Pentelicus to the northeast and Mount Hymettus to the east. Beyond Mount Aegaleo lies the Thriasian plain, which forms an extension of the central plain to the west. The Saronic Gulf lies to the southwest. Mount Parnitha is the tallest of the four mountains (), and has been declared a national park. The Athens urban area spreads over 50 kilometres (31 mi) from Agios Stefanos in the north to Varkiza in the south. The city is located in the north temperate zone, 38 degrees north of the equator.
Athens is built around a number of hills. Lycabettus is one of the tallest hills of the city proper and provides a view of the entire Attica Basin. The meteorology of Athens is deemed to be one of the most complex in the world because its mountains cause a temperature inversion phenomenon which, along with the Greek Government's difficulties controlling industrial pollution, was responsible for the air pollution problems the city has faced. This issue is not unique to Athens; for instance, Los Angeles and Mexico City also suffer from similar atmospheric inversion problems.
The Cephissus river, the Ilisos and the Eridanos stream are the historical rivers of Athens.
### Environment.
By the late 1970s, the pollution of Athens had become so destructive that according to the then Greek Minister of Culture, Constantine Trypanis, "...the carved details on the five the caryatids of the Erechtheum had seriously degenerated, while the face of the horseman on the Parthenon's west side was all but obliterated." A series of measures taken by the authorities of the city throughout the 1990s resulted in the improvement of air quality; the appearance of smog (or "nefos" as the Athenians used to call it) has become less common.
Measures taken by the Greek authorities throughout the 1990s have improved the quality of air over the Attica Basin. Nevertheless, air pollution still remains an issue for Athens, particularly during the hottest summer days. In late June 2007, the Attica region experienced a number of brush fires, including a blaze that burned a significant portion of a large forested national park in Mount Parnitha, considered critical to maintaining a better air quality in Athens all year round. Damage to the park has led to worries over a stalling in the improvement of air quality in the city.
The major waste management efforts undertaken in the last decade (particularly the plant built on the small island of Psytalia) have greatly improved water quality in the Saronic Gulf, and the coastal waters of Athens are now accessible again to swimmers.
### Safety.
Athens ranks in the lowest percentage for the risk on frequency and severity of terrorist attacks according to the EU Global Terrorism Database (EIU 2007–2016 calculations). The city also ranked 35th in Digital Security, 21st on Health Security, 29th on Infrastructure Security and 41st on Personal Security globally in a 2017 The Economist Intelligence Unit report. It also ranks as a very safe city (39th globally out of 162 cities overall) on the ranking of the safest and most dangerous countries. A 2019 crime index from Numbeo places Athens at 130th position, rating safer than Tampa, Florida or Dublin, Ireland. According to a Mercer 2019 Quality of Living Survey, Athens ranks 89th on the Mercer Quality of Living Survey ranking.
### Climate.
Athens has a hot-summer Mediterranean climate (Köppen climate classification: "Csa"). The dominant feature of Athens' climate is alternation between prolonged hot and dry summers because of the dry and hot winds blowing from the Sahara, and mild, wetter winters with moderate rainfall, due to the westerly winds. With an average of of yearly precipitation, rainfall occurs largely between the months of October and April. July and August are the driest months when thunderstorms occur sparsely. Furthermore, some coastal areas such as Piraeus in the Athens Riviera, have a hot semi-arid climate ("BSh") according to the climate atlas published by the Hellenic National Meteorological Service. However, places like Elliniko, which are classified as hot semi-arid ("BSh") because of the low annual rainfall, have not recorded temperatures as high as other places in the city. This happens due to the moderating influence of the sea and the fact that there is not as much industrialization as in other regions of the city.
Owing to the rain shadow of the Pindus Mountains, annual precipitation of Athens is lower than most other parts of Greece, especially western Greece. As an example, Ioannina receives around per year, and Agrinio around per year. Daily average highs for July have been measured around in downtown Athens, but some parts of the city may be even hotter for the higher density of buildings, and the lower density of vegetation, such as the center, in particular, western areas due to a combination of industrialization and a number of natural factors, knowledge of which has existed since the mid-19th century. Due to the large area covered by Athens Metropolitan Area, there are notable climatic differences between parts of the urban conglomeration. The northern suburbs tend to be wetter and cooler in winter, whereas the southern suburbs are some of the driest locations in Greece and record very high minimum temperatures in summer. Heavy snowfall is infrequent. The last time heavy snow fell in Greater Athens area and Athens itself, was between 14-17 February 2021, when snow blanketed the entire city and its suburbs from the north to the furthest south, coastal suburbs , with depth ranges up to in Central Athens. , and with even the Acropolis of Athens completely covered with snow. The National Meteorological Service (EMY) described it was one of the most intense snow storms over the past 40 years.
Athens is affected by the urban heat island effect in some areas which is caused by human activity, altering its temperatures compared to the surrounding rural areas, and leaving detrimental effects on energy usage, expenditure for cooling, and health. The urban heat island of the city has also been found to be partially responsible for alterations of the climatological temperature time-series of specific Athens meteorological stations, because of its impact on the temperatures and the temperature trends recorded by some meteorological stations. On the other hand, specific meteorological stations, such as the National Garden station and Thiseio meteorological station, are less affected or do not experience the urban heat island.
Athens holds the World Meteorological Organization record for the highest temperature ever recorded in Europe, at , which was recorded in the Elefsina and Tatoi suburbs of Athens on 10 July 1977.
### Locations.
#### Neighbourhoods of the center of Athens (Municipality of Athens).
The Municipality of Athens, the City Centre of the Athens Urban Area, is divided into several districts: Omonoia, Syntagma, Exarcheia, Agios Nikolaos, Neapolis, Lykavittos, Lofos Strefi, Lofos Finopoulou, Lofos Filopappou, Pedion Areos, Metaxourgeio, Aghios Kostantinos, Larissa Station, Kerameikos, Psiri, Monastiraki, Gazi, Thission, Kapnikarea, Aghia Irini, Aerides, Anafiotika, Plaka, Acropolis, Pnyka, Makrygianni, Lofos Ardittou, Zappeion, Aghios Spyridon, Pangrati, Kolonaki, Dexameni, Evaggelismos, Gouva, Aghios Ioannis, Neos Kosmos, Koukaki, Kynosargous, Fix, Ano Petralona, Kato Petralona, Rouf, Votanikos, Profitis Daniil, Akadimia Platonos, Kolonos, Kolokynthou, Attikis Square, Lofos Skouze, Sepolia, Kypseli, Aghios Meletios, Nea Kypseli, Gyzi, Polygono, Ampelokipoi, Panormou-Gerokomeio, Pentagono, Ellinorosson, Nea Filothei, Ano Kypseli, Tourkovounia-Lofos Patatsou, Lofos Elikonos, Koliatsou, Thymarakia, Kato Patisia, Treis Gefyres, Aghios Eleftherios, Ano Patisia, Kypriadou, Menidi, Prompona, Aghios Panteleimonas, Pangrati, Goudi, Vyronas and Ilisia.
#### Parks and zoos.
Parnitha National Park is punctuated by well-marked paths, gorges, springs, torrents and caves dotting the protected area. Hiking and mountain-biking in all four mountains are popular outdoor activities for residents of the city. The National Garden of Athens was completed in 1840 and is a green refuge of 15.5 hectares in the centre of the Greek capital. It is to be found between the Parliament and Zappeion buildings, the latter of which maintains its own garden of seven hectares.
Parts of the City Centre have been redeveloped under a masterplan called the "Unification of Archeological Sites of Athens", which has also gathered funding from the EU to help enhance the project. The landmark Dionysiou Areopagitou Street has been pedestrianised, forming a scenic route. The route starts from the Temple of Olympian Zeus at Vasilissis Olgas Avenue, continues under the southern slopes of the Acropolis near Plaka, and finishes just beyond the Temple of Hephaestus in Thiseio. The route in its entirety provides visitors with views of the Parthenon and the Agora (the meeting point of ancient Athenians), away from the busy City Centre.
The hills of Athens also provide green space. Lycabettus, Philopappos hill and the area around it, including Pnyx and Ardettos hill, are planted with pines and other trees, with the character of a small forest rather than typical metropolitan parkland. Also to be found is the Pedion tou Areos ("Field of Mars") of 27.7 hectares, near the National Archaeological Museum.
Athens' largest zoo is the Attica Zoological Park, a 20-hectare (49-acre) private zoo located in the suburb of Spata. The zoo is home to around 2000 animals representing 400 species, and is open 365 days a year. Smaller zoos exist within public gardens or parks, such as the zoo within the National Garden of Athens.
### Urban and suburban municipalities.
The Athens Metropolitan Area consists of 58 densely populated municipalities, sprawling around the Municipality of Athens (the City Centre) in virtually all directions. For the Athenians, all the urban municipalities surrounding the City Centre are called suburbs. According to their geographic location in relation to the City of Athens, the suburbs are divided into four zones; the northern suburbs (including Agios Stefanos, Dionysos, Ekali, Nea Erythraia, Kifissia, Kryoneri, Maroussi, Pefki, Lykovrysi, Metamorfosi, Nea Ionia, Nea Filadelfeia, Irakleio, Vrilissia, Melissia, Penteli, Chalandri, Agia Paraskevi, Gerakas, Pallini, Galatsi, Psychiko and Filothei); the southern suburbs (including Alimos, Nea Smyrni, Moschato, Tavros, Agios Ioannis Rentis, Kallithea, Piraeus, Agios Dimitrios, Palaio Faliro, Elliniko, Glyfada, Lagonisi, Saronida, Argyroupoli, Ilioupoli, Varkiza, Voula, Vari and Vouliagmeni); the eastern suburbs (including Zografou, Dafni, Vyronas, Kaisariani, Cholargos and Papagou); and the western suburbs (including Peristeri, Ilion, Egaleo, Koridallos, Agia Varvara, Keratsini, Perama, Nikaia, Drapetsona, Chaidari, Petroupoli, Agioi Anargyroi, Ano Liosia, Aspropyrgos, Eleusina, Acharnes and Kamatero).
The Athens city coastline, extending from the major commercial port of Piraeus to the southernmost suburb of Varkiza for some , is also connected to the City Centre by tram.
In the northern suburb of Maroussi, the upgraded main Olympic Complex (known by its Greek acronym OAKA) dominates the skyline. The area has been redeveloped according to a design by the Spanish architect Santiago Calatrava, with steel arches, landscaped gardens, fountains, futuristic glass, and a landmark new blue glass roof which was added to the main stadium. A second Olympic complex, next to the sea at the beach of Palaio Faliro, also features modern stadia, shops and an elevated esplanade. Work is underway to transform the grounds of the old Athens Airport – named Elliniko – in the southern suburbs, into one of the largest landscaped parks in Europe, to be named the Hellenikon Metropolitan Park.
Many of the southern suburbs (such as Alimos, Palaio Faliro, Elliniko, Glyfada, Voula, Vouliagmeni and Varkiza) known as the Athens Riviera, host a number of sandy beaches, most of which are operated by the Greek National Tourism Organisation and require an entrance fee. Casinos operate on both Mount Parnitha, some from downtown Athens (accessible by car or cable car), and the nearby town of Loutraki (accessible by car via the Athens – Corinth National Highway, or the suburban rail service Proastiakos).
## Administration.
The large City Centre () of the Greek capital falls directly within the Municipality of Athens or Athens Municipality ()—also City of Athens. Athens Municipality is the largest in population size in Greece. Piraeus also forms a significant city centre on its own, within the Athens Urban Area and it is the second largest in population size within it, with Peristeri, Kallithea and Kypseli following.
### Athens Urban Area.
The Athens Urban Area (), also known as Urban Area of the Capital () or Greater Athens (), today consists of 40 municipalities, 35 of which make up what was referred to as the former Athens Prefecture municipalities, located within 4 regional units (North Athens, West Athens, Central Athens, South Athens); and a further 5 municipalities, which make up the former Piraeus Prefecture municipalities, located within the regional unit of Piraeus as mentioned above. The densely built up urban area of the Greek capital sprawls across throughout the Attica Basin and has a total population of 3,074,160 (in 2011).
The Athens Municipality forms the core and center of Greater Athens, which in its turn consists of the Athens Municipality and 40 more municipalities, divided in four regional units (Central, North, South and West Athens), accounting for 2,641,511 people (in 2011) within an area of . Until 2010, which made up the abolished Athens Prefecture and the municipality of Piraeus, the historic Athenian port, with 4 other municipalities make up the regional unit of Piraeus.
The regional units of Central Athens, North Athens, South Athens, West Athens and Piraeus with part of East and West Attica regional units combined make up the continuous Athens Urban Area, also called the "Urban Area of the Capital" or simply "Athens" (the most common use of the term), spanning over , with a population of 3,090,508 people as of 2011. The Athens Urban Area is considered to form the city of Athens as a whole, despite its administrative divisions, which is the largest in Greece and one of the most populated urban areas in Europe.
### Athens Metropolitan Area.
The Athens Metropolitan Area () spans within the Attica region and includes a total of 58 municipalities, which are organized in seven regional units (those outlined above, along with East Attica and West Attica), having reached a population of 3,737,550 based on the preliminary results of the 2011 census. Athens and Piraeus municipalities serve as the two metropolitan centres of the Athens Metropolitan Area. There are also some inter-municipal centres serving specific areas. For example, Kifissia and Glyfada serve as inter-municipal centres for northern and southern suburbs respectively.
## Demographics.
### Population in modern times.
The Municipality of Athens has an official population of 664,046 people. The four regional units that make up what is referred to as Greater Athens have a combined population of 2,640,701. They together with the regional unit of Piraeus (Greater Piraeus) make up the dense Athens Urban Area which reaches a total population of 3,090,508 inhabitants (in 2011). According to Eurostat, in 2013 the functional urban area of Athens had 3,828,434 inhabitants, being apparently decreasing compared with the pre-economic crisis date of 2009 (4,164,175)
The municipality (Center) of Athens is the most populous in Greece, with a population of 664,046 people (in 2011) and an area of , forming the core of the Athens Urban Area within the Attica Basin. The incumbent Mayor of Athens is Kostas Bakoyannis of New Democracy. The municipality is divided into seven municipal districts which are mainly used for administrative purposes.
As of the 2011 census, the population for each of the seven municipal districts of Athens is as follows:
For the Athenians the most popular way of dividing the downtown is through its neighbourhoods such as Pagkrati, Ambelokipi, Goudi, Exarcheia, Patissia, Ilissia, Petralona, Plaka, Anafiotika, Koukaki, Kolonaki and Kypseli, each with its own distinct history and characteristics.
### Population of the Athens Metropolitan Area.
The Athens Metropolitan Area, with an area of and inhabited by 3,753,783 people in 2011, consists of the Athens Urban Area with the addition of the towns and villages of East and West Attica, which surround the dense urban area of the Greek capital. It actually sprawls over the whole peninsula of Attica, which is the best part of the region of Attica, excluding the islands.
### Population in ancient times.
Mycenean Athens in 1600–1100 BC could have equalled the size of Tiryns, with an estimated population of up to 10,000–15,000. During the Greek Dark Ages the population of Athens was around 4,000 people, rising to an estimated 10,000 by 700 BC.
During the Classical period Athens denotes both the urban area of the city proper and its subject territory (the Athenian city-state) extending across most of the modern Attica region except the territory of the city-state of Megaris and the island section. In 500 BC the Athenian territory probably contained around 200,000 people. Thucydides indicates a 5th-century total of 150,000-350,000 and up to 610,000. A census ordered by Demetrius of Phalerum in 317 BC is said to have recorded 21,000 free citizens, 10,000 resident aliens and 400,000 slaves, a total population of 431,000, but this figure is highly suspect because of the improbably high number of slaves and does not include free women and children and resident foreigners. An estimate based on Thucydides is 40,000 male citizens, 100,000 family members, 70,000 metics (resident foreigners) and 150,000-400,000 slaves, though modern historians again hesitate to take such high numbers at face value, most estimates now preferring a total in the 200–350,000 range. The urban area of Athens proper (excluding the port of Piraeus) covered less than a thousandth of the area of the city-state, though its population density was of course far higher: modern estimates for the population of the built-up area tend to indicate around 35–45,000 inhabitants, though density of occupation, household size and whether there was a significant suburban population beyond the walls remain uncertain.
The ancient site of the main city is centred on the rocky hill of the acropolis. In the whole of Athenian territory they existed many towns. Acharnae, Afidnes, Cytherus, Colonus, Corydallus, Cropia, Decelea, Euonymos, Vravron among others was important towns in Athenian countryside. The new port of Piraeus was a prototype harbour with the infrastructure and housing located in the site between modern passenger section of the port (named Kantharos in ancient times) and the Pasalimani harbour (named Zea in ancient times). The old one Phaliro was in the site of modern Palaio Faliro and gradually declined after the construction of the new prototype port but remained as a minor port and important settlement with historic significance in late Classical times. The rapid expansion of the modern city, which continues to this day, took off with industrial growth in the 1950s and 1960s. The expansion is now particularly toward the East and North East (a tendency greatly related to the new Eleftherios Venizelos International Airport and the Attiki Odos, the freeway that cuts across Attica). By this process Athens has engulfed many former suburbs and villages in Attica, and continues to do so. The table below shows the historical population of Athens in recent times.
## Government and politics.
Athens became the capital of Greece in 1834, following Nafplion, which was the provisional capital from 1829. The municipality (City) of Athens is also the capital of the Attica region. The term "Athens" can refer either to the Municipality of Athens, to Greater Athens or urban area, or to the entire Athens Metropolitan Area.
### International relations and influence.
#### Twin towns – sister cities.
Athens is twinned with:
## Economy and infrastructure.
Athens is the financial capital of Greece. According to data from 2014, Athens as a metropolitan economic area produced 130 billion US-dollars as GDP in PPP, which consists nearly a half of the production for the whole country. Athens was ranked 102nd in that year's list of global economic metropolises, while GDP per capita for the same year was 32,000 US-dollars.
Athens is one of the major economic centres in south-eastern Europe and is considered a regional economic power. The port of Piraeus, where big investments by COSCO have already been delivered during the recent decade, the completion of the new Cargo Centre in Thriasion, the expansion of the Athens Metro and the Athens Tram, as well as the Hellenikon metropolitan park redevelopment in Elliniko and other urban projects, are the economic landmarks of the upcoming years.
Prominent Greek companies such as Hellas Sat, Hellenic Aerospace Industry, Mytilineos Holdings, Titan Cement, Hellenic Petroleum, Papadopoulos E.J., Folli Follie, Jumbo S.A., OPAP, and Cosmote have their headquarters in the metropolitan area of Athens. Multinational companies such as Ericsson, Sony, Siemens, Motorola, Samsung, Microsoft, Novartis, Mondelez and Coca-Cola also have their regional research and development headquarters in the city. 
The banking sector is represented by National Bank of Greece, Alpha Bank, Eurobank, and Piraeus Bank, while the Bank of Greece is also situated in the City Centre. The Athens Stock Exchange was severely hit by the Greek government-debt crisis and the decision of the government to proceed into capital controls during summer 2015. As a whole the economy of Athens and Greece was strongly affected, while data showed a change from long recession to growth of 1.4% from 2017.
Tourism is also a leading contributor to the economy of the city, as one of Europe's top destinations for city-break tourism, and also the gateway for excursions to both the islands and other parts of the mainland. Greece attracted 26.5 million visitors in 2015, 30.1 million visitors in 2017, and over 33 million in 2018, making Greece one of the most visited countries in Europe and the world, and contributing 18% to the country's GDP. Athens welcomed more than 5 million tourists in 2018, and 1.4 million were "city-breakers" ; this was an increase by over a million city-breakers since 2013.
### Transport.
Athens is the country's major transportation hub. The city has Greece's largest airport and its largest port ; Piraeus, too, is the largest container transport port in the Mediterranean, and the largest passenger port in Europe. 
Athens is a major national hub for Intercity (Ktel) and international buses, as well as for domestic and international rail transport. Public transport is serviced by a variety of transportation means, making up the country's largest mass transit system. The Athens Mass Transit System consists of a large bus and trolleybus fleet, the city's Metro, a commuter rail service and a tram network, connecting the southern suburbs to the city centre.
#### Bus transport.
OSY () (Odikes Sygkoinonies S.A.), a subsidiary company of OASA (Athens urban transport organisation), is the main operator of buses and trolleybuses in Athens. As of 2017, its network consists of around 322 bus lines, spanning the Athens Metropolitan Area, and making up a fleet of 2,375 buses buses and trolleybuses. Of those 2,375, 619 buses run on compressed natural gas, making up the largest fleet of natural gas-powered buses in Europe, and 354 are electric-powered (trolleybuses). All of the 354 trolleybuses are equipped to run on diesel in case of power failure.
International links are provided by a number of private companies. National and regional bus links are provided by KTEL from two InterCity Bus Terminals ; Kifissos Bus Terminal A and Liosion Bus Terminal B, both located in the north-western part of the city. "Kifissos" provides connections towards Peloponnese, North Greece, West Greece and some Ionian Islands, whereas "Liosion" is used for most of Central Greece.
#### Athens Metro.
The Athens Metro is operated by STASY S.A () (Statheres Sygkoinonies S.A), a subsidiary company of OASA (Athens urban transport organisation), which provides public transport throughout the Athens Urban Area. While its main purpose is transport, it also houses Greek artifacts found during construction of the system. The Athens Metro runs three metro lines, namely Line 1 (Green Line), Line 2 (Red Line) and Line 3 (Blue Line) lines, of which the first was constructed in 1869, and the other two largely during the 1990s, with the initial new sections opened in January 2000. Line 1 mostly runs at ground level and the other two (Line 2 &amp; 3) routes run entirely underground. A fleet of 42 trains, using 252 carriages, operates on the network, with a daily occupancy of 1,353,000 passengers.
"Line 1" (Green Line) serves 24 stations, and is the oldest line of the Athens metro network. It runs from Piraeus station to Kifissia station and covers a distance of . There are transfer connections with the Blue Line 3 at Monastiraki station and with the Red Line 2 at Omonia and Attiki stations.
"Line 2" (Red Line) runs from Anthoupoli station to Elliniko station and covers a distance of 

</doc>
<doc id="1217" url="https://en.wikipedia.org/wiki?curid=1217" title="Anguilla">
Anguilla

Anguilla ( ) is a British overseas territory in the Caribbean. It is one of the most northerly of the Leeward Islands in the Lesser Antilles, lying east of Puerto Rico and the Virgin Islands and directly north of Saint Martin. The territory consists of the main island of Anguilla, approximately long by wide at its widest point, together with a number of much smaller islands and cays with no permanent population. The territory's capital is The Valley. The total land area of the territory is , with a population of approximately ().
## Etymology.
The native Arawak name for the island was "Malliouhana".
In reference to the island's shape, the Italian "", meaning "eel" (in turn, from the Latin diminutive of "anguis", “snake”) was used as its name. 
## History.
Anguilla was first settled by Indigenous Amerindian peoples who migrated from South America. The earliest Native American artefacts found on Anguilla have been dated to around 1300 BC; remains of settlements date from AD 600.
Precisely when Anguilla was first seen by Europeans is uncertain: some sources claim that Columbus sighted the island during his second voyage in 1493, while others state that the first European explorer was the French Huguenot nobleman and merchant René Goulaine de Laudonnière in 1564. The Dutch West India Company established a fort on the island in 1631. However, the Company later withdrew after its fort was destroyed by the Spanish in 1633.
Traditional accounts state that Anguilla was first colonised by English settlers from Saint Kitts beginning in 1650. The settlers focused on planting tobacco, and to a lesser extent cotton. The French temporarily took over the island in 1666 but returned it to English control under the terms of the Treaty of Breda the next year. Major John Scott who visited in September 1667, wrote of leaving the island "in good condition" and noted that in July 1668, "200 or 300 people fled thither in time of war". The French attacked again in 1688, 1745 and 1798, causing much destruction but failing to capture the island.
It is likely that the early European settlers brought enslaved Africans with them. Historians confirm that African slaves lived in the region in the early 17th century, such as slaves from Senegal living on St Kitts in the mid 1600s. By 1672 a slave depot existed on the island of Nevis, serving the Leeward Islands. While the time of African arrival in Anguilla is difficult to place precisely, archival evidence indicates a substantial African presence of at least 100 enslaved people by 1683; these seem to have come from Central Africa as well as West Africa. The slaves were forced to work on the sugar plantations which had begun to replace tobacco as Anguilla's main crop. Over time the African slaves and their descendants came to vastly outnumber the white settlers. The African slave trade was eventually terminated within the British Empire in 1807, and slavery outlawed completely in 1834. Many planters subsequently sold up or left the island.
During the early colonial period, Anguilla was administered by the British through Antigua; in 1825, it was placed under the administrative control of nearby Saint Kitts. Anguilla was federated with St Kitts and Nevis in 1882, against the wishes of many Anguillans. Economic stagnation, and the severe effects of several droughts in the 1890s and later the Great Depression of the 1930s led many Anguillans to emigrate for better prospects elsewhere.
Full adult suffrage was introduced to Anguilla in 1952. After a brief period as part of the West Indies Federation (1958–62), the island of Anguilla became part of the associated state of Saint Kitts-Nevis-Anguilla with full internal autonomy in 1967. However many Anguillans had no wish to be a part of this union, and resented the dominance of St Kitts within it. On 30 May 1967 Anguillans forcibly ejected the St Kitts police force from the island and declared their separation from St Kitts following a referendum. The events, led by Atlin Harrigan and Ronald Webster among others, became known as the Anguillan Revolution; its goal was not independence per se, but rather independence from Saint Kitts and Nevis and a return to being a British colony.
With negotiations failing to break the deadlock, a second referendum confirming Anguillans' desire for separation from St Kitts was held and the Republic of Anguilla was declared unilaterally, with Ronald Webster as president. Efforts by British envoy William Whitlock failed to break the impasse and 300 British troops were subsequently sent in March 1969. British authority was restored, and confirmed by the Anguilla Act of July 1971. In 1980, Anguilla was finally allowed to formally secede from Saint Kitts and Nevis and become a separate British Crown colony (now a British overseas territory). Since then, Anguilla has been politically stable, and has seen a large growth in its tourism and offshore financing sectors.
## Geography and geology.
Anguilla is a flat, low-lying island of coral and limestone in the Caribbean Sea, measuring some 16 miles (26 km) long and 3.5 miles (6 km) in width. It lies to the east of Puerto Rico and the Virgin Islands and directly north of Saint Martin, separated from that island by the Anguilla Channel. The soil is generally thin and poor, supporting scrub, tropical and forest vegetation. The terrain is generally low-lying, with the highest terrain located in the vicinity of The Valley; Crocus Hill, Anguilla's highest peak at , lies in the western regions of the town.
Anguilla is noted for its ecologically important coral reefs and beaches. Apart from the main island of Anguilla itself, the territory includes a number of other smaller islands and cays, mostly tiny and uninhabited:
### Geology.
Anguilla has a volcanic origin and has been submerged repeatedly from climate change.
### Climate.
#### Temperature.
Northeastern trade winds keep this tropical island relatively cool and dry. Average annual temperature is . July–October is its hottest period, December–February, its coolest.
#### Rainfall.
Rainfall averages annually, although the figures vary from season to season and year to year. The island is subject to both sudden tropical storms and hurricanes, which occur in the period from July to November. The island suffered damage in 1995 from Hurricane Luis and severe flooding from Hurricane Lenny.
## Governance.
### Political system.
Anguilla is an internally self-governing overseas territory of the United Kingdom. Its politics take place in a framework of a parliamentary representative democratic dependency, whereby the Premier is the head of government, and of a pluriform multi-party system.
The United Nations Committee on Decolonization includes Anguilla on the United Nations list of Non-Self-Governing Territories. The territory's constitution is the Anguilla Constitutional Order 1 April 1982 (amended 1990). Executive power is exercised by the government, with legislative power being vested in both the government and the House of Assembly. The judiciary is independent of the executive and the legislature.
### Defence.
As a dependency of the UK, the UK is responsible for Anguilla's military defence, although there are no active garrisons or armed forces present. Anguilla has a small marine police force, comprising around 32 personnel, which operates one VT Halmatic M160-class 52-foot fast patrol boat. Policing on the island is the responsibility of the Royal Anguilla Police Force.
## Population.
### Demographics.
The majority of residents (90.08%) are black, most of whom are the descendants of enslaved people transported from Africa. Minorities include whites at 3.74% and people of mixed race at 4.65% (figures from 2001 census).
72% of the population is Anguillan while 28% is non-Anguillan (2001 census). Of the non-Anguillan population, many are citizens of the United States, United Kingdom, St Kitts &amp; Nevis, the Dominican Republic, Jamaica and Nigeria.
2006 and 2007 saw an influx of large numbers of Chinese, Indian and Mexican workers, brought in as labour for major tourist developments due to the local population not being large enough to support the labour requirements.
### Religion.
Christian churches did not have a consistent or strong presence during the initial period of English colonisation; spiritual and religious practices of Europeans and Africans tended to reflect their regional origins. As early as 1813, Christian ministers formally ministered to enslaved Africans and promoted literacy among converts. The Wesleyan (Methodist) Missionary Society of England built churches and schools from 1817.
According to the 2001 census, Christianity is Anguilla's predominant religion, with 29% of the population practising Anglicanism; another 23.9% are Methodist. Other churches on the island include Seventh-day Adventist, Baptist, Roman Catholic (served by the Diocese of Saint John's–Basseterre, with the See at Saint John on Antigua and Barbuda) and a small community of Jehovah's Witnesses (0.7%). Between 1992 and 2001, the number of followers of the Church of God and Pentecostals increased considerably. There are at least 15 churches on the island. Although a minority on the island, Anguilla is an important location to followers of Rastafarian religion as the birthplace of Robert Athlyi Rogers, author of the "Holy Piby" which had a strong influence on Rastafarian and other Africa-centre belief systems. More recently, a Muslim cultural centre has opened on the island.
### Languages.
Today most people in Anguilla speak a British-influenced variety of standard English. Other languages are also spoken on the island, including varieties of Spanish, Chinese and the languages of other immigrant communities. However, the most common language other than Standard English is the island's own English-lexifier Creole language (not to be confused with Antillean Creole ('French Creole'), spoken in French islands such as Martinique and Guadeloupe). It is referred to locally by terms such as "dialect" (pronounced "dialek"), Anguilla Talk or "Anguillian". It has its main roots in early varieties of English and West African languages, and is similar to the dialects spoken in English-speaking islands throughout the Eastern Caribbean in terms of its structural features.
Linguists who are interested in the origins of Anguillan and other Caribbean Creoles point out that some of its grammatical features can be traced to African languages while others can be traced to European languages. Three areas have been identified as significant for the identification of the linguistic origins of those forced migrants who arrived before 1710: the Gold Coast, the Slave Coast and the Windward Coast.
Sociohistorical information from Anguilla's archives suggest that Africans and Europeans formed two distinct, but perhaps overlapping speech communities in the early phases of the island's colonisation. "Anguillian" is believed to have emerged as the language of the masses as time passed, slavery was abolished and locals began to see themselves as "belonging" to Anguillan society.
## Education.
There are six government primary schools, one government secondary school (Albena Lake Hodge Comprehensive School), and two private schools. There is a single library, the Edison L. Hughes Education &amp; Library Complex of the Anguilla Public Library. A branch of the Saint James School of Medicine was established in 2011 in Anguilla. It is a private, for-profit medical school headquartered in Park Ridge, Illinois.
There is a University of the West Indies Open campus site in the island.
## Culture.
The island's cultural history begins with the native Taino, Arawak and Carib. Their artefacts have been found around the island, telling of life before European settlers arrived.
The Anguilla National Trust (ANT) was established in 1988 and opened its offices in 1993 charged with the responsibility of preserving the heritage of the island, including its cultural heritage.
As throughout the Caribbean, holidays are a cultural fixture. Anguilla's most important holidays are of historic as much as cultural importance – particularly the anniversary of the emancipation (previously August Monday in the Park), celebrated as the Summer Festival, or Carnival. British festivities, such as the Queen's Birthday, are also celebrated.
### Cuisine.
Anguillan cuisine is influenced by native Caribbean, African, Spanish, French and English cuisines. Seafood is abundant, including prawns, shrimp, crab, spiny lobster, conch, mahi-mahi, red snapper, marlin and grouper. Salt cod is a staple food eaten on its own and used in stews, casseroles and soups. Livestock is limited due to the small size of the island and people there use poultry, pork, goat and mutton, along with imported beef. Goat is the most commonly eaten meat, used in a variety of dishes. The official national food of Anguilla is pigeon peas and rice.
A significant amount of the island's produce is imported due to limited land suitable for agriculture production; much of the soil is sandy and infertile. Among the agriculture produced in Anguilla includes tomatoes, peppers, limes and other citrus fruits, onion, garlic, squash, pigeon peas and callaloo. Starch staple foods include imported rice and other foods that are imported or locally grown, including yams, sweet potatoes and breadfruit.
### Literature.
The Anguilla National Trust has programmes encouraging Anguillan writers and the preservation of the island's history. In 2015, "Where I See The Sun – Contemporary Poetry in Anguilla" A New Anthology by Lasana M. Sekou was published by House of Nehesi Publishers. Among the forty three poets in the collection are Rita Celestine-Carty, Bankie Banx, John T. Harrigan, Patricia J. Adams, Fabian Fahie, Dr. Oluwakemi Linda Banks, and Reuel Ben Lewi.
### Music.
Various Caribbean musical genres are popular on the island, such as reggae, soca and calypso.
### Sports.
Boat racing has deep roots in Anguillan culture and is the national sport. There are regular sailing regattas on national holidays, such as Carnival, which are contested by locally built and designed boats. These boats have names and have sponsors that print their logo on their sails.
As in many other former British colonies, cricket is also a popular sport. Anguilla is the home of Omari Banks, who played for the West Indies Cricket Team, while Cardigan Connor played first-class cricket for English county side Hampshire and was 'chef de mission' (team manager) for Anguilla's Commonwealth Games team in 2002. Other noted players include Chesney Hughes, who played for Derbyshire County Cricket Club in England.
Rugby union is represented in Anguilla by the Anguilla Eels RFC, who were formed in April 2006. The Eels have been finalists in the St. Martin tournament in November 2006 and semi-finalists in 2007, 2008, 2009 and Champions in 2010. The Eels were formed in 2006 by Scottish club national second row Martin Welsh, Club Sponsor and President of the AERFC Ms. Jacquie Ruan, and Canadian standout Scrumhalf Mark Harris (Toronto Scottish RFC).
Anguilla is the birthplace of sprinter Zharnel Hughes who has represented Great Britain since 2015, and England at the 2018 Commonwealth Games. He won the 100 metres at the 2018 European Athletics Championships, the 4 x 100 metres at the same championships, and the 4 x 100 metres for England at the 2018 Commonwealth Games.
Shara Proctor, British Long Jump Silver Medalist in World Championships in Beijing first represented Anguilla in the event until 2010 when she began to represent Great Britain and England. Under the Anguillan Flag she achieved several medals in the NACAC games.
Keith Connor, triple jumper, is also an Anguillan. He represented Great Britain and England and achieved several international titles including Commonwealth and European Games gold medals and an Olympic bronze medal. Keith later became Head Coach of Australia Athletics.
## Natural history.
### Wildlife.
Anguilla has habitat for the Cuban tree frogs ("Osteopilus septentrionalis"). The red-footed tortoise ("Chelonoidis carbonaria") is a species of tortoise found here, which originally came from South America. Hurricanes led to over-water dispersal for the green iguanas ("Iguana iguana") to colonise Anguilla in the mid-90s. All three animals are introduced.
Five species of bats are known in the literature from Anguilla – the threatened insular single leaf bat ("Monophyllus plethodon"), the Antillean fruit-eating bat ("Brachyphylla cavernarum"), the Jamaican fruit bat ("Artibeus jamaicensis"), the Mexican funnel-eared bat ("Natalus stramineus"), and the velvety free-tailed bat ("Molossus molossus").
## Economy.
Anguilla's thin arid soil being largely unsuitable for agriculture, the island has few land-based natural resources. Its main industries are tourism, offshore incorporation and management, offshore banking, captive insurance and fishing.
Anguilla's currency is the East Caribbean dollar, though the US dollar is also widely accepted. The exchange rate is fixed to the US dollar at US$1 = EC$2.70.
The economy, and especially the tourism sector, suffered a setback in late 1995 due to the effects of Hurricane Luis in September. Hotels were hit particularly hard but a recovery occurred the following year. Another economic setback occurred during the aftermath of Hurricane Lenny in 2000. Before the 2008 worldwide crisis, the economy of Anguilla was growing strongly, especially the tourism sector which was driving major new developments in partnerships with multi-national companies. Anguilla's tourism industry received a major boost when it was selected to host the World Travel Awards in December 2014. Known as "the Oscars of the travel industry", the awards ceremony was held at the CuisinArt Resort and Spa and was hosted by Vivica A. Fox. Anguilla was voted the World's Leading Luxury Island Destination from a short list of top-tier candidates such as St. Barts, the Maldives and Mauritius.
Anguilla's financial system comprises seven banks, two money services businesses, more than 40 company managers, more than 50 insurers, 12 brokers, more than 250 captive intermediaries, more than 50 mutual funds and eight trust companies.
Anguilla has become a popular tax haven, having no capital gains, estate, profit, sales, or corporate taxes. In April 2011, faced with a mounting deficit, it introduced a 3% "Interim Stabilisation Levy", Anguilla's first form of income tax. Anguilla also has a 0.75% property tax.
Anguilla aims to obtain 15% of its energy from solar power to become less reliant on expensive imported diesel. The Climate &amp; Development Knowledge Network is helping the government gather the information it needs to change the territory's legislation, so that it can integrate renewables into its grid. Barbados has also made good progress in switching to renewables, but many other Small Island Developing States are still at the early stages of planning how to integrate renewable energy into their grids. "For a small island we're very far ahead," said Beth Barry, Coordinator of the Anguilla Renewable Energy Office. "We've got an Energy Policy and a draft Climate Change policy and have been focusing efforts on the question of sustainable energy supply for several years now. As a result, we have a lot of information we can share with other islands."
## Transportation.
### Air.
Anguilla is served by Clayton J. Lloyd International Airport (prior to 4 July 2010 known as Wallblake Airport). The primary runway at the airport is in length and can accommodate moderate-sized aircraft. Services connect to various other Caribbean islands via local charter airlines and others. Although there are no direct scheduled flights to or from continental America or Europe, Tradewind Aviation and Cape Air provide scheduled air service to San Juan, Puerto Rico. The airport can handle large narrow-body jets such as the Boeing 727, Boeing 737 and Airbus 220.
### Road.
Aside from taxis, there is no public transport on the island. Cars drive on the left.
### Boat.
There are regular ferries from Saint Martin to Anguilla. It is a 20-minute crossing from Marigot, St. Martin to Blowing Point, Anguilla. Ferries commence service from 7:00 am. There is also a charter service, from Blowing Point, Anguilla to Princess Juliana Airport to make travel easier. This way of travel is the most common method of transport between Anguilla and St. Martin.

</doc>
<doc id="1220" url="https://en.wikipedia.org/wiki?curid=1220" title="Anguilla/Transnational issues">
Anguilla/Transnational issues



</doc>
<doc id="1221" url="https://en.wikipedia.org/wiki?curid=1221" title="Anguilla/Military">
Anguilla/Military



</doc>
<doc id="1223" url="https://en.wikipedia.org/wiki?curid=1223" title="Telecommunications in Anguilla">
Telecommunications in Anguilla

This article is about communications systems in Anguilla.
## Telephone.
Telephones – main lines in use: 6,200 (2002)
Telephones – mobile cellular: 1,800 (2002)
Telephone system:
&lt;br&gt;"Domestic:" Modern internal telephone system
&lt;br&gt;"International:" EAST CARIBBEAN FIBRE SYSTEM ECFS (cable system)
" microwave radio relay to island of Saint Martin (Guadeloupe and Netherlands Antilles)"
## Mobile phone (GSM).
Mobile phone operators:
FLOW (Anguilla) Ltd. – GSM and UMTS 850 and 1900 MHz, LTE 700 MHz with Island-wide coverage 
Digicel (Anguilla) Ltd. – GSM and UMTS 850 to 1900 MHz, LTE 700 MHz 
Mobiles: ? (2007)
## Radio.
Radio broadcast stations: AM 3, FM 7, shortwave 0 (2007)
Radios: 3,000 (1997)
## Television.
Television broadcast stations: 1 (1997)
Televisions: 1,000 (1997)
## Internet.
Internet country code: .ai (Top level domain)
Internet Service Providers (ISPs): 2 (FLOW – , Digicel Anguilla – )
Internet hosts: 269 (2012) 
Internet: users: 12,377 (2018) 

</doc>
<doc id="1227" url="https://en.wikipedia.org/wiki?curid=1227" title="Ashmore and Cartier Islands">
Ashmore and Cartier Islands

The Territory of Ashmore and Cartier Islands is an uninhabited external territory of Australia consisting of four low-lying tropical islands in two separate reefs, and the territorial sea generated by the islands. The territory is located in the Indian Ocean situated on the edge of the continental shelf, about off the northwest coast of Australia and south of the Indonesian island of Rote.
Ashmore Reef is called "Pulau Pasir" by Indonesians and "Nusa Solokaek" in the Rotenese language. Both names have the meaning "sand island".
## Geography.
The Territory comprises Ashmore Reef, which includes West, Middle, and East Islands, and two lagoons, and Cartier Reef, which includes Cartier Island. Ashmore Reef covers approximately and Cartier Reef , both measurements extending to the limits of the reefs.
West, Middle, and East Islands have a combined land area variously reported as , and . Cartier Island has a reported land area of .
## History.
According to Australian literature, Cartier Island was discovered by Captain Nash in 1800, and named after his ship "Cartier". Ashmore Island was discovered by Captain Samuel Ashmore in 1811 from his ship and named after him. Ashmore Island was annexed by the United Kingdom in 1878, as was Cartier Island in 1909.
A British order-in-council dated 23 July 1931 stated that Ashmore and Cartier Islands would be placed under the authority of the Commonwealth of Australia when Australia passed legislation to accept them, and formal administration began two years later. The Commonwealth's resulting "Ashmore and Cartier Islands Acceptance Act 1933" came into operation on 10 May 1934, when the islands formally became a territory. The act authorised the Governor of Western Australia to make ordinances for the territory. In July 1938 the territory was annexed to the Northern Territory, then also administered by the Commonwealth, whose laws, ordinances and regulations applied to the Northern Territory. When self-government was granted to the Northern Territory on 1 July 1978, administration of Ashmore and Cartier Islands was retained by the Commonwealth.
In 1983, the territory was declared a nature reserve under the "National Parks and Wildlife Conservation Act 1975", now replaced by the "Environment Protection and Biodiversity Conservation Act 1999". Cartier Island, which was a former bombing range, became a marine reserve in 2000.
After the islands became a first point of contact with the Australian migration zone, in September 2001, the Australian government excised the Ashmore and Cartier Islands from the Australian migration zone.
### Indonesian heritage and memorandum.
Ashmore has been regularly visited and fished by Indonesian fishermen since the early eighteenth century. A 1974 Memorandum of Understanding (MOU) between Australia and Indonesia sets out arrangements by which traditional fishers can access resources in Australia's territorial sea in the region. This allows traditional Indonesian fishermen to access parts of Ashmore for shelter, freshwater and to visit grave sites. The area, known as the MOU Box, contains the Ashmore and Cartier Islands Territory.
## Governance.
Today, the Territory is administered from Canberra by the Department of Infrastructure, Regional Development and Cities, which is also responsible for the administration of the territories of Christmas Island, Cocos (Keeling) Islands, the Coral Sea Islands, Jervis Bay Territory and Norfolk Island.
The Attorney-General's Department had been responsible for the administration of Australian territories until the 2010 federal election. In that year the responsibility for Australian territories was transferred to the then Department of Regional Australia, Local Government, Arts and Sport, and from 18 September 2013 the Department of Infrastructure and Regional Development has administered Australian territories.
Defence of Ashmore and Cartier Islands is the responsibility of Australia, with periodic visits by the Royal Australian Navy, Royal Australian Air Force and Australian Customs and Border Protection Service.
Nearby Hibernia Reef, northeast of Ashmore Reef, is not part of the Territory, but belongs to Western Australia. It has no permanently dry land area, although large parts of the reef become exposed during low tide.
## Environment and protection.
The Ashmore Reef Marine Park and Cartier Island Marine Park are both classed as strict nature reserves (IUCN Ia) and protect biodiverse areas of significant and international importance, as well as cultural heritage.
## Economy.
There is no economic activity in the Territory, Ashmore and Cartier Islands being uninhabited. Cartier Island is an unvegetated sand island. Access to Cartier Island is prohibited because of the risk of unexploded ordnances. There are no ports or harbours, only offshore anchorage. The Australian Border Force vessel is stationed off the reef for up to 300 days per year. The islands are also visited by seasonal caretakers and occasional scientific researchers.
The area has been a traditional fishing ground of Indonesian fishermen for centuries, and continues. In the 1850s, American whalers operated in the region. Mining of phosphate deposits took place on Ashmore Island in the latter half of the 19th century. Today, all the wells in the Territory are infected with cholera or contaminated and undrinkable.
Petroleum extraction activities take place at the Jabiru and Challis oil fields, which are adjacent to the Territory, and which are administered by the Northern Territory Department of Mines and Energy on behalf of the Commonwealth.
## Migration.
As Ashmore Reef is the closest point of Australian territory to Indonesia, it was a popular target for people smugglers transporting asylum seekers en route to Australia. Once they had landed on Ashmore Island, asylum seekers could claim to have entered Australian migration zone and request to be processed as refugees. The use of Ashmore Island for this purpose created great notoriety during late 2001, when refugee arrivals became a major political issue in Australia. The Australian Government argued that as Australia was not the country of first asylum for these "boat people", Australia did not have a responsibility to accept them.
A number of things were done to discourage the use of the Territory for this purpose, such as attempting to have the people smugglers arrested in Indonesia; the so-called Pacific Solution of processing them in third countries; the boarding and forced turnaround of the boats by Australian military forces; and finally excising the Territory and many other small islands from the Australian migration zone.
Two boatloads of asylum seekers were each detained for several days in the lagoon at Ashmore Island after failed attempts by the Royal Australian Navy to turn them back to Indonesia in October 2001.

</doc>
<doc id="1228" url="https://en.wikipedia.org/wiki?curid=1228" title="Ashmore and Cartier Islands/Geography">
Ashmore and Cartier Islands/Geography



</doc>
<doc id="1229" url="https://en.wikipedia.org/wiki?curid=1229" title="Ashmore and Cartier Islands/People">
Ashmore and Cartier Islands/People



</doc>
<doc id="1230" url="https://en.wikipedia.org/wiki?curid=1230" title="Ashmore and Cartier Islands/Government">
Ashmore and Cartier Islands/Government



</doc>
<doc id="1231" url="https://en.wikipedia.org/wiki?curid=1231" title="Ashmore and Cartier Islands/Transportation">
Ashmore and Cartier Islands/Transportation



</doc>
<doc id="1232" url="https://en.wikipedia.org/wiki?curid=1232" title="Ashmore and Cartier Islands/Economy">
Ashmore and Cartier Islands/Economy



</doc>
<doc id="1233" url="https://en.wikipedia.org/wiki?curid=1233" title="Ashmore and Cartier Islands/Military">
Ashmore and Cartier Islands/Military



</doc>
<doc id="1234" url="https://en.wikipedia.org/wiki?curid=1234" title="Acoustic theory">
Acoustic theory

Acoustic theory is a scientific field that relates to the description of sound waves. It derives from fluid dynamics. See acoustics for the engineering approach.
For sound waves of any magnitude of a disturbance in velocity, pressure, and density we have
In the case that the fluctuations in velocity, density, and pressure are small, we can approximate these as
Where formula_3 is the perturbed velocity of the fluid, formula_4 is the pressure of the fluid at rest, formula_5 is the perturbed pressure of the system as a function of space and time, formula_6 is the density of the fluid at rest, and formula_7 is the variance in the density of the fluid over space and time.
In the case that the velocity is irrotational (formula_8), we then have the acoustic wave equation that describes the system:
Where we have
## Derivation for a medium at rest.
Starting with the Continuity Equation and the Euler Equation:
If we take small perturbations of a constant pressure and density:
Then the equations of the system are
Noting that the equilibrium pressures and densities are constant, this simplifies to
### A Moving Medium.
Starting with
We can have these equations work for a moving medium by setting formula_16, where formula_17 is the constant velocity that the whole fluid is moving at before being disturbed (equivalent to a moving observer) and formula_18 is the fluid velocity.
In this case the equations look very similar:
Note that setting formula_20 returns the equations at rest.
## Linearized Waves.
Starting with the above given equations of motion for a medium at rest:
Let us now take formula_22 to all be small quantities.
In the case that we keep terms to first order, for the continuity equation, we have the formula_23 term going to 0. This similarly applies for the density perturbation times the time derivative of the velocity. Moreover, the spatial components of the material derivative go to 0. We thus have, upon rearranging the equilibrium density:
Next, given that our sound wave occurs in an ideal fluid, the motion is adiabatic, and then we can relate the small change in the pressure to the small change in the density by
Under this condition, we see that we now have
Defining the speed of sound of the system:
Everything becomes
### For Irrotational Fluids.
In the case that the fluid is irrotational, that is formula_29, we can then write formula_30 and thus write our equations of motion as
The second equation tells us that
And the use of this equation in the continuity equation tells us that
This simplifies to
Thus the velocity potential formula_35 obeys the wave equation in the limit of small disturbances. The boundary conditions required to solve for the potential come from the fact that the velocity of the fluid must be 0 normal to the fixed surfaces of the system.
Taking the time derivative of this wave equation and multiplying all sides by the unperturbed density, and then using the fact that formula_36 tells us that
Similarly, we saw that formula_38. Thus we can multiply the above equation appropriately and see that
Thus, the velocity potential, pressure, and density all obey the wave equation. Moreover, we only need to solve one such equation to determine all other three. In particular, we have
### For a moving medium.
Again, we can derive the small-disturbance limit for sound waves in a moving medium. Again, starting with
We can linearize these into
#### For Irrotational Fluids in a Moving Medium.
Given that we saw that
If we make the previous assumptions of the fluid being ideal and the velocity being irrotational, then we have
Under these assumptions, our linearized sound equations become
Importantly, since formula_17 is a constant, we have formula_47, and then the second equation tells us that
Or just that
Now, when we use this relation with the fact that formula_50, alongside cancelling and rearranging terms, we arrive at
We can write this in a familiar form as
This differential equation must be solved with the appropriate boundary conditions. Note that setting formula_53 returns us the wave equation. Regardless, upon solving this equation for a moving medium, we then have

</doc>
<doc id="1235" url="https://en.wikipedia.org/wiki?curid=1235" title="Alexander Mackenzie (politician)">
Alexander Mackenzie (politician)

Alexander Mackenzie, (January 28, 1822 – April 17, 1892) was a Canadian politician who served as the second prime minister of Canada, in office from 1873 to 1878.
Mackenzie was born in Logierait, Perthshire, Scotland. He left school at the age of 13, following his father's death to help his widowed mother, and trained as a stonemason. Mackenzie immigrated to Canada when he was 19, settling in what became Ontario. His masonry business prospered, allowing him to pursue other interests – such as the editorship of a pro-Reformist newspaper called the" Lambton Shield". Mackenzie was elected to the Legislative Assembly of the Province of Canada in 1862, as a supporter of George Brown.
In 1867, Mackenzie was elected to the new House of Commons of Canada for the Liberal Party. He became leader of the party (thus Leader of the Opposition) in mid-1873, and a few months later succeeded John A. Macdonald as prime minister, following Macdonald's resignation in the aftermath of the Pacific Scandal. Mackenzie and the Liberals won a clear majority at the 1874 election. He was popular among the general public for his humble background and apparent democratic tendencies.
As prime minister, Mackenzie continued the nation-building programme that had been begun by his predecessor. His government established the Supreme Court of Canada and Royal Military College of Canada, and created the District of Keewatin to better administer Canada's newly acquired western territories. However, it made little progress on the transcontinental railway, and struggled to deal with the aftermath of the Panic of 1873. At the 1878 election, Mackenzie's government suffered a landslide defeat. He remained leader of the Liberal Party for another two years, and continued on as a Member of Parliament (MP) until his death, due to a stroke.
## Early life.
Mackenzie was born on January 28, 1822, in Logierait, Perthshire, Scotland, the son of Mary Stewart (Fleming) and Alexander Mackenzie, Sr (born 1784), who were married in 1817. The site of his birthplace is known as Clais-'n-deoir "The Hollow of the Weeping", where families said their goodbyes as the convicted were led to nearby Gallows Hill. The house in which he was born was built by his father and is still standing in 2019. He was the third of 10 boys, seven of whom survived infancy. Alexander Mackenzie, Sr., was a carpenter and ship's joiner who had to move around frequently for work after the end of the Napoleonic Wars in 1815. Mackenzie's father died on March 7, 1836 and at the age of 13, Alexander Mackenzie, Jr., was thus forced to end his formal education to help support his family. He apprenticed as a stonemason and met his future wife, Helen Neil, in Irvine, where her father was also a stonemason. The Neils were Baptist and shortly thereafter, Mackenzie converted from Presbyterianism to Baptist beliefs. Together with the Neils, he immigrated to Canada in 1842 to seek a better life. Mackenzie's faith was to link him to the increasingly influential temperance cause, particularly strong in Canada West where he lived, a constituency of which he was to represent in the Parliament of Canada.
The Neils and Mackenzie settled in Kingston, Ontario. The limestone in the area proved too hard for his stonemason tools, and not having money to buy new tools, Mackenzie took a job as a labourer constructing a building on Princess Street. The contractor on the job claimed financial difficulty, so Mackenzie accepted a promissory note for summer wages. The note later proved to be worthless. Subsequently, Mackenzie won a contract building a bomb-proof arch at Fort Henry. He later became a foreman on the construction of Kingston's four Martello Towers – Murney Tower, Fort Frederick, Cathcart Tower, and Shoal Tower. He was also a foreman on the construction of the Welland Canal and the Lachine Canal. While working on the Beauharnois Canal, a one-ton stone fell and crushed one of his legs. He recovered, but never regained the strength in that leg. While in Kingston, Mackenzie became a vocal opponent of religious and political entitlement and corruption in government.
Mackenzie married Helen Neil (1826–52) in 1845 and with her had three children, with only one girl, Mary, surviving infancy. Helen and he moved to Sarnia, Ontario (known as Canada West) in 1847 and Mary was born in 1848. They were soon joined from Scotland by the rest of Mackenzie's brothers and his mother. He began working as a general contractor, earning a reputation for being a hard-working, honest man, as well as having a working man's view on fiscal policy. Mackenzie helped construct many courthouses and jails across southern Ontario. A number of these still stand today, including the Sandwich Courthouse and Jail now known as the Mackenzie Hall Cultural Centre in Windsor, Ontario, and the Kent County Courthouse and Jail in Chatham, Ontario. He even bid, unsuccessfully, on the construction of the Parliament buildings in Ottawa in 1859. Helen died in 1852, finally succumbing to the effects of excessive doses of mercury-based calomel used to treat a fever while in Kingston. In 1853, he married Jane Sym (1825–93).
## Early political involvement.
Mackenzie involved himself in politics almost from the moment he arrived in Canada. He fought passionately for equality and the elimination of all forms of class distinction. In 1851, he became the secretary for the Reform Party for Lambton. After convincing him to run in Kent/Lambton, Mackenzie campaigned relentlessly for George Brown, owner of the Reformist paper "The Globe" in the 1851 election, helping Brown to win his first seat in the Legislative Assembly. Mackenzie and Brown remained the closest of friends and colleagues for the rest of their lives. In 1852, Mackenzie became editor of another reformist paper, the "Lambton Shield". As an editor, Mackenzie was perhaps a little too vocal, leading the paper to a lawsuit for libel against the local conservative candidate. Because a key witness claimed Cabinet Confidence and would not testify, the paper lost the suit and was forced to fold due to financial hardship. After his brother, Hope Mackenzie, declined to run, Alexander was petitioned to run and won his first seat in the Legislative Assembly as a supporter of George Brown in 1861. When Brown resigned from the Great Coalition in 1865 over reciprocity negotiations with the United States, Mackenzie was invited to replace him as the president of the council. Wary of Macdonald's motivations and true to his principles, Mackenzie declined.
He entered the House of Commons of Canada in 1867, representing the Lambton, Ontario, riding. No cohesive national Liberal Party of Canada existed at the time and with Brown not winning his seat, no official leader emerged. Mackenzie did not believe he was the best qualified for the position, and although he resisted offers of the position, he nevertheless sat as the "de facto" leader of the Official Opposition.
## Prime Minister (1873–1878).
When the Macdonald government fell due to the Pacific Scandal in 1873, the Governor General, Lord Dufferin, called upon Mackenzie, who had been chosen as the leader of the Liberal Party a few months earlier, to form a new government. Mackenzie formed a government and asked the Governor General to call an election for January 1874. The Liberals won a majority of the seats in the House of Commons having garnered 40% of the popular vote.
Mackenzie remained prime minister until the 1878 election when Macdonald's Conservatives returned to power with a majority government.
For a man of Mackenzie's humble origins to attain such a position was unusual in an age which generally offered such opportunity only to the privileged. Lord Dufferin expressed early misgivings about a stonemason taking over government, but on meeting Mackenzie, Dufferin revised his opinions:
Mackenzie served concurrently as Minister of Public Works and oversaw the completion of the Parliament buildings. While drawing up the plans for the West Block, he included a circular staircase leading directly from his office to the outside of the building, which allowed him to escape the patronage-seekers waiting for him in his ante-chamber. Proving Dufferin's reflections on his character to be true, Mackenzie disliked intensely the patronage inherent in politics. Nevertheless, he found it a necessary evil to maintain party unity and ensure the loyalty of his fellow Liberals.
In keeping with his democratic ideals, Mackenzie refused the offer of a knighthood three times, and was thus the only one of Canada's first eight Prime Ministers not to be knighted. He also declined appointment to the UK Privy Council and hence does not bear the title "Right Honourable". His pride in his working class origins never left him. Once, while touring Fort Henry as prime minister, he asked the soldier accompanying him if he knew the thickness of the wall beside them. The embarrassed escort confessed that he didn't and Mackenzie replied, "I do. It is five feet, ten inches. I know, because I built it myself!"
As Prime Minister, Alexander Mackenzie strove to reform and simplify the machinery of government, achieving a remarkable record of reform legislation. He introduced the secret ballot; advised the creation of the Supreme Court of Canada; the establishment of the Royal Military College of Canada in Kingston in 1874; and the creation of the Office of the Auditor General in 1878. He completed the Intercolonial Railway, but struggled to progress on the national railway due to a worldwide economic depression, almost coming to blows with the then Governor General Lord Dufferin over imperial interference. Mackenzie stood up for the rights of Canada as a nation and fought for the supremacy of Parliament and honesty in government. Above all else, he was known and loved for his honesty and integrity.
However, his term was marked by economic depression that had grown out of the Panic of 1873, which Mackenzie's government was unable to alleviate. In 1874, Mackenzie negotiated a new free trade agreement with the United States, eliminating the high protective tariffs on Canadian goods in US markets. However, this action did not bolster the economy, and construction of the CPR slowed drastically due to lack of funding. In 1876, the Conservative opposition announced a National Policy of protective tariffs, which resonated with voters. When an election was held at the conclusion of Mackenzie's five-year term, the Conservatives were swept back into office in a landslide victory.
### Supreme Court appointments.
Mackenzie chose the following jurists to be appointed as justices of the Supreme Court of Canada by the Governor General:
## Later life.
After his government's defeat, Mackenzie remained Leader of the Opposition for another two years, until 1880. He was soon struck with a mysterious ailment that sapped his strength and all but took his voice. Sitting in silence, he nevertheless remained an undefeated MP until his death in 1892 from a stroke that resulted from hitting his head during a fall. He died in Toronto and was buried in Lakeview Cemetery in Sarnia, Ontario.
## Character.
Mackenzie's first biography in 1892 referred to him as Canada's Stainless Statesman. He was a devout Baptist and teetotaller who found refuge in, and drew strength from, his family, friends, and faith. He was also a loyal friend and an incorrigible prankster (stuffed chimney on young in-laws; rolled boulder down Thunder Cape towards friend A. McKellar; burned Tory campaign placards in hotel woodstove early in morning). Unpretentious and down to earth, his public official austerity was in striking contrast to private compassion and giving nature. He was the soul of honour and integrity, a proud man who sought no recognition or personal enrichment and accepted gifts reluctantly. He preferred to follow than lead (unreferenced – many times he refused leadership offers) and often found duty outweighed heavy burden of office. He was uncompromising on his principles, perhaps too much so. An historian at the time said, "He was, and ever will remain, the Sir Galahad of Canadian politics."
Very proud of his Scottish heritage, he was forever a Scot: "Nemo me impune lacessit" (no one attacks me with impunity). The Upper Canada rebellion leader W.L. Mackenzie referred to him, "He is every whit a self-made, self-educated man. Has large mental capacity and indomitable energy." Canada's Governor General, Lord Dufferin, said of him, he is "as pure as crystal, and as true as steel, with lots of common sense." A close friend, Chief Justice Sir Louis Davies, said he was "the best debater the House of Commons has ever known." A friend and colleague in Cabinet who went on to become prime minister of Canada, Sir Wilfrid Laurier, said he was "one of the truest and strongest characters to be met within Canadian history. He was endowed with a warm heart and a copious and rich fancy, though veiled by a somewhat reticent exterior, and he was of friends the most tender and true." Another friend and colleague, who went on to become premier of Ontario, Sir George Ross, said, "Mackenzie was "sui generis" a debater. His humorous sallies blistered like a blast from a flaming smelter. His sterling honesty is a great heritage, and will keep his memory green to all future generations." At his eulogy, Rev. Dr. Thomas compared him to the Duke of Wellington, who "stood four square, to all the winds that blow."
Newspaper around the world and in Canada had this to say about him. "The London Times" – the untiring energy, the business-like accuracy, the keen perception and reliable judgment, and above all the inflexible integrity, which marked his private life, he carried without abatement of one jot into his public career. "The Westminster Review" – a man, who although, through failing health and failing voice, he had virtually passed out of public life, yet retained to the last the affectionate veneration of the Canadian people as no other man of the time can be said to have done. The "Charlottetown Patriot" – in all that constitutes the real man, the honest statesman, the true patriot, the warm friend, and sincere Christian, he had few equals. Possessed of a clear intellect, a retentive memory, and a ready command of appropriate words, he was one of the most logical and powerful speakers we have ever heard. The "St. John Telegraph" – he was loved by the people and his political opponents were compelled to respect him even above their own chosen leader. As a statesman, he has had few equals. The "Montreal Star" – it is one of the very foremost architects of the Canadian nationality that we mourn. In the dark days of ’73, Canadians were in a state of panic, distrusting the stability of their newly-built Dominion; no one can tell what would have happened had not the stalwart form of Alexander Mackenzie lifted itself above the screaming, vociferating and denying mass of politicians, and all Canada felt at once, there was a man who could be trusted. The "Toronto Globe" – he was a man who loved the people and fought for their rights against privilege and monopoly in every form. The "Philadelphia Record" – Like Caesar, who twice refused a knightly crown, Alexander Mackenzie refused knighthood three times. Unlike Caesar, he owed his political overthrow to his incorruptible honesty and unswerving integrity.
## Legacy.
In their 1999 study of the Prime Ministers of Canada, which included the results of a survey of Canadian historians, J. L. Granatstein and Norman Hillmer found that Mackenzie was in 11th place just after John Sparrow David Thompson.
### Namesakes.
The following are named in honour of Alexander Mackenzie:

</doc>
<doc id="1238" url="https://en.wikipedia.org/wiki?curid=1238" title="Atomic bomb">
Atomic bomb



</doc>
<doc id="1239" url="https://en.wikipedia.org/wiki?curid=1239" title="Ashoka">
Ashoka

Ashoka (; Brāhmi: 𑀅𑀲𑁄𑀓, "Asoka", IAST: "Aśoka"), also known as Ashoka the Great, was an Indian emperor of the Maurya Dynasty, son of Bindusara, who ruled almost all of the Indian subcontinent from to 232 BCE. Ashoka promoted the spread of Buddhism across ancient Asia. Considered by many to be one of India's greatest emperors, Ashoka expanded Chandragupta's empire to reign over territory stretching from present-day Afghanistan in the west to present-day Bangladesh in the east. It covered the entire Indian subcontinent except for parts of present-day Tamil Nadu, Karnataka, and Kerala. The empire's capital was Pataliputra (in Magadha, present-day Patna), with provincial capitals at Takshashila (later Taxila) and Ujjain. Ashoka, after the war of Kalinga, got upset with the bloodshed and vowed to never fight again. He patronized Buddhism during his reign.
Ashoka waged a particularly destructive war against the state of Kalinga (modern Odisha), which he conquered in about 260 BCE. According to an interpretation of his Edicts, he converted to Buddhism after witnessing the mass deaths of the Kalinga War, which he had waged out of a desire for conquest and which reportedly directly resulted in more than 100,000 deaths and 150,000 deportations. He is remembered for erecting the Ashoka pillars and spreading his Edicts, for sending Buddhist monks to Sri Lanka and Central Asia, and for establishing monuments marking several significant sites in the life of Gautama Buddha.
Beyond the Edicts of Ashoka, biographical information about him relies on legends written centuries later, such as the 2nd-century CE "Ashokavadana" (""Narrative of Ashoka", a part of the "Divyavadana"), and in the Sri Lankan text "Mahavamsa" ("Great Chronicle"). The emblem of the modern Republic of India is an adaptation of the Lion Capital of Ashoka. His Sanskrit name " means "painless, without sorrow" (the "a" privativum and "śoka", "pain, distress"). In his edicts, he is referred to as ' (Pali ' or "the Beloved of the Gods"), and ' or Priyadarshi (Pali ' or "He who regards everyone with affection"). His fondness for a tree is the reason for his name being connected to the "Ashoka tree" or "Saraca asoca", and this is referenced in the "Ashokavadana".
In "The Outline of History" (1920), H.G. Wells wrote, "Amidst the tens of thousands of names of monarchs that crowd the columns of history, their majesties and graciousnesses and serenities and royal highnesses and the like, the name of Ashoka shines, and shines, almost alone, a star."
## Sources of information.
Information about Ashoka comes from his own inscriptions; other inscriptions that mention him or are possibly from his reign; and ancient literature, especially Buddhist texts. These sources often contradict each other, although various historians have attempted to correlate their testimony. Plenty is known or not known. So, for example, while Ashoka is often attributed with building many hospitals during his time, there is no clear evidence that any hospitals existed in ancient India during the 3rd century BC or that Ashoka was responsible for commissioning the construction of any.
Inscriptions
Ashoka's inscriptions are the earliest self-representations of imperial power in the Indian subcontinent. However, these inscriptions are focused mainly on the topic of "dhamma", and provide little information regarding other aspects of the Maurya state and society. Even on the topic of "dhamma", the content of these inscriptions cannot be taken at face value. In the words of American academic John S. Strong, it is sometimes helpful to think of Ashoka's messages as propaganda by a politician whose aim is to present a favourable image of himself and his administration, rather than record historical facts. 
A small number of other inscriptions also provide some information about Ashoka. For example, he finds a mention in the 2nd century Junagadh rock inscription of Rudradaman. An inscription discovered at Sirkap mentions a lost word beginning with "Priy", which is theorised to be Ashoka's title "Priyadarshi", although this is not certain. Some other inscriptions, such as the Sohgaura copper plate inscription, have been tentatively dated to Ashoka's period by a section of scholars, although others contest this. 
Buddhist legends
Much of the information about Ashoka comes from Buddhist legends, which present him as a great, ideal king. These legends appear in texts that are not contemporary to Ashoka and were composed by Buddhist authors, who used various stories to illustrate the impact of their faith on Ashoka. This makes it necessary to exercise caution while relying on them for historical information. Among modern scholars, opinions range from downright dismissal of these legends as mythological to acceptance of all historical portions that seem plausible.
The Buddhist legends about Ashoka exist in several languages, including Sanskrit, Pali, Tibetan, Chinese, Burmese, Sinhala, Thai, Lao, and Khotanese. All these legends can be traced to two primary traditions:
There are several significant differences between the two traditions. For example, the Sri Lankan tradition emphasises Ashoka's role in convening the Third Buddhist council, and his dispatch of several missionaries to distant regions, including his son Mahinda to Sri Lanka. However, the North Indian tradition makes no mention of these events. It describes other events not found in the Sri Lankan tradition, such as a story about another son named Kunala. 
Even while narrating the common stories, the two traditions diverge in several ways. For example, both "Ashokavadana" and "Mahavamsa" mention that Ashoka's queen Tishyarakshita had the Bodhi Tree destroyed. In "Ashokavadana", the queen manages to have the tree healed after she realises her mistake. In the "Mahavamsa", she permanently destroys the tree, but only after a branch of the tree has been transplanted in Sri Lanka. In another story, both the texts describe Ashoka's unsuccessful attempts to collect a relic of Gautama Buddha from Ramagrama. In "Ashokavadana", he fails to do so because he cannot match the devotion of the Nagas who hold the relic; however, in the "Mahavamsa", he fails to do so because the Buddha had destined the relic to be enshrined by King Dutthagamani of Sri Lanka. Using such stories, the "Mahavamsa" glorifies Sri Lanka as the new preserve of Buddhism. 
Other sources
Numismatic, sculptural, and archaeological evidence supplements research on Ashoka. Ashoka's name appears in the lists of Mauryan kings in the various Puranas. However, these texts do not provide further details about him, as their Brahmanical authors were not patronised by the Mauryans. Other texts, such as the "Arthashastra" and "Indica of Megasthenes", which provide general information about the Maurya period, can also be used to make inferences about Ashoka's reign. However, the "Arthashastra" is a normative text that focuses on an ideal rather than a historical state, and its dating to the Mauryan period is a subject of debate. The "Indica" is a lost work, and only parts of it survive in the form of paraphrases in later writings.
The 12th-century text "Rajatarangini" mentions a Kashmiri king Ashoka of Gonandiya dynasty who built several stupas: some scholars, such as Aurel Stein, have identified this king with the Maurya king Ashoka; others, such as Ananda W. P. Guruge dismiss this identification as inaccurate.
Alternative interpretation of the epigraphic evidence
For some scholars such as Christopher I. Beckwith, Ashoka, whose name only appears in the Minor Rock Edicts, should be differentiated from the ruler Piyadasi, or "Devanampiya" Piyadasi (i.e. "Beloved of the Gods Piyadasi", "Beloved of the Gods" being a fairly widespread title for "King"), who is named as the author of the Major Pillar Edicts and the Major Rock Edicts. This inscriptional evidence may suggest that these were two different rulers. According to him, Piyadasi was living in the 3rd century BCE, probably the son of Chandragupta Maurya known to the Greeks as Amitrochates, and only advocating for piety ("Dharma") in his Major Pillar Edicts and Major Rock Edicts, without ever mentioning Buddhism, the Buddha or the Samgha. Also, the geographical spread of his inscription shows that Piyadasi ruled a vast Empire, contiguous with the Seleucid Empire in the West.
On the contrary, for Beckwith, Ashoka was a later king of the 1st–2nd century CE, whose name only appears explicitly in the Minor Rock Edicts and allusively in the Minor Pillar Edicts, and who does mention the Buddha and the Samgha, explicitly promoting Buddhism. His inscriptions cover a very different and much smaller geographical area, clustering in Central India. According to Beckwith, the inscriptions of this later Ashoka were typical of the later forms of "normative Buddhism", which are well attested from inscriptions and Gandhari manuscripts dated to the turn of the millennium, and around the time of the Kushan Empire. The quality of the inscriptions of this Ashoka is significantly lower than the quality of the inscriptions of the earlier Piyadasi.
## Names and titles.
The name "A-shoka" literally means "without sorrow". According to an "Ashokavadana" legend, his mother gave him this name because his birth removed her sorrows.
The name Priyadasi is associated with Ashoka in the 3rd–4th century CE "Dipavamsa". The term literally means "he who regards amiably", or "of gracious mien" (Sanskrit: Priya-darshi). It may have been a regnal name adopted by Ashoka.
Ashoka's inscriptions mention his title "Devanampiya" (Sanskrit: "Devanampriya", "Beloved of the Gods"). The identification of Devanampiya and Ashoka as the same person is established by the Maski and Gujarra inscriptions, which use both these terms for the king. The title was adopted by other kings, including the contemporary king Devanampiya Tissa of Anuradhapura and Ashoka's descendant Dasharatha Maurya.
## Early life.
Ashoka's own inscriptions do not describe his early life, and much of the information on this topic comes from apocryphal legends written hundreds of years after him. While these legends include obviously fictitious details such as narratives of Ashoka's past lives, they have some plausible historical information about Ashoka's period.
### Date.
The exact date of Ashoka's birth is not certain, as the extant contemporary Indian texts did not record such details. It is known that he lived in the 3rd century BCE, as his inscriptions mention several contemporary rulers whose dates are known with more certainty, such as Antiochus II Theos, Ptolemy II Philadelphus, Antigonus II Gonatas, Magas of Cyrene, and Alexander (of Epirus or Corinth). Thus, Ashoka must have been born sometime in the late 4th century BCE or early 3rd century BCE (c. 304 BCE),
### Ancestry.
Ashoka's own inscriptions are fairly detailed but make no mention of his ancestors. Other sources, such as the Puranas and the "Mahavamsa" state that his father was the Mauryan emperor Bindusara, and his grandfather was Chandragupta – the founder of the Empire. The "Ashokavadana" also names his father as Bindusara, but traces his ancestry to Buddha's contemporary king Bimbisara, through Ajatashatru, Udayin, Munda, Kakavarnin, Sahalin, Tulakuchi, Mahamandala, Prasenajit, and Nanda. The 16th century Tibetan monk Taranatha, whose account is a distorted version of the earlier traditions, describes Ashoka as the illegitimate son of king Nemita of Champarana from the daughter of a merchant.
"Ashokavadana" states that Ashoka's mother was the daughter of a Brahmin from Champa, and was prophesized to marry a king. Accordingly, her father took her to Pataliputra, where she was inducted into Bindusara's harem, and ultimately, became his chief queen. The "Ashokavadana" does not mention her by name, although other legends provide different names for her. For example, the "Asokavadanamala" calls her Subhadrangi. The "Vamsatthapakasini" or "Mahavamsa-tika", a commentary on "Mahavamsa", calls her "Dharma" ("Dhamma" in Pali), and states that she belonged to the Moriya Kshatriya clan. A "Divyavadana" legend calls her Janapada-kalyani; according to scholar Ananda W. P. Guruge, this is not a name, but an epithet.
According to the 2nd-century historian Appian, Chandragupta entered into a marital alliance with the Greek ruler Seleucus I Nicator, which has led to speculation that either Chandragupta or his son Bindusara married a Greek princess. However, there is no evidence that Ashoka's mother or grandmother was Greek, and most historians have dismissed the idea.
## As a prince.
According to the "Ashokavadana", Bindusara disliked Ashoka because of his rough skin. One day, Bindusara asked the ascetic Pingala-vatsajiva to determine which of his sons was worthy of being his successor. He asked all the princes to assemble at the Garden of the Golden Pavilion on the ascetic's advice. Ashoka was reluctant to go because his father disliked him, but his mother convinced him to do so. When minister Radhagupta saw Ashoka leaving the capital for the Garden, he offered to provide the prince with a royal elephant for the travel. At the Garden, Pingala-vatsajiva examined the princes and realised that Ashoka would be the next king. To avoid annoying Bindusara, the ascetic refused to name the successor. Instead, he said that one who had the best mount, seat, drink, vessel and food would be the next king; each time, Ashoka declared that he met the criterion. Later, he told Ashoka's mother that her son would be the next king, and on her advice, left the kingdom to avoid Bindusara's wrath.
While legends suggest that Bindusara disliked Ashoka's ugly appearance, they also state that Bindusara gave him important responsibilities, such as suppressing a revolt in Takshashila (according to north Indian tradition) and governing Ujjain (according to Sri Lankan tradition). This suggests that Bindusara was impressed by the other qualities of the prince. Another possibility is that he sent Ashoka to distant regions to keep him away from the imperial capital.
### Rebellion at Takshashila.
According to the "Ashokavadana", Bindusara dispatched prince Ashoka to suppress a rebellion in the city of Takshashila (present-day Bhir Mound). This episode is not mentioned in the Sri Lankan tradition, which instead states that Bindusara sent Ashoka to govern Ujjain. Two other Buddhist texts – "Ashoka-sutra" and "Kunala-sutra" – state that Bindusara appointed Ashoka as a viceroy in Gandhara (where Takshashila was located), not Ujjain. 
The "Ashokavadana" states that Bindusara provided Ashoka with a fourfold-army (comprising cavalry, elephants, chariots and infantry) but refused to provide any weapons for this army. Ashoka declared that weapons would appear before him if he was worthy of being a king, and then, the deities emerged from the earth and provided weapons to the army. When Ashoka reached Takshashila, the citizens welcomed him and told him that their rebellion was only against the evil ministers, not the king. Sometime later, Ashoka has similarly welcomed in the Khasa territory and the gods declared that he would go on to conquer the whole earth.
Takshashila was a prosperous and geopolitically influential city, and historical evidence proves that by Ashoka's time, it was well-connected to the Mauryan capital Pataliputra by the "Uttarapatha" trade route. However, no extant contemporary source mentions the Takshashila rebellion, and none of Ashoka's records states that he ever visited the city. That said, the historicity of the legend about Ashoka's involvement in the Takshashila rebellion may be corroborated by an Aramaic-language inscription discovered at Sirkap near Taxila. The inscription includes a name that begins with the letters "prydr", and most scholars restore it as "Priyadarshi", which was the title of Ashoka. Another evidence of Ashoka's connection to the city may be the name of the Dharmarajika Stupa near Taxila; the name suggests that it was built by Ashoka ("Dharma-raja"). 
The story about the deities miraculously bringing weapons to Ashoka may be the text's way of deifying Ashoka; or indicating that Bindusara – who disliked Ashoka – wanted him to fail in Takshashila. 
### Governor of Ujjain.
According to the "Mahavamsa", Bindusara appointed Ashoka as the viceroy of present-day Ujjain (Ujjeni), which was an important administrative and commercial centre in the Avanti province of central India. This tradition is corroborated by the Saru Maru inscription discovered in central India; this inscription states that he visited the place as a prince. Ashoka's own rock edict mentions the presence of a prince viceroy at Ujjain during his reign, which further supports the tradition that he himself served as a viceroy at Ujjain.
Pataliputra was connected to Ujjain by multiple routes in Ashoka's time, and on the way, Ashoka entourage may have encamped at Rupnath, where his inscription has been found.
According to the Sri Lankan tradition, Ashoka visited Vidisha, where he fell in love with a beautiful woman on his way to Ujjain. According to the "Dipamvamsa" and "Mahamvamsa", the woman was Devi – the daughter of a merchant. According to the "Mahabodhi-vamsa", she was Vidisha-Mahadevi and belonged to the Shakya clan of Gautama Buddha. The Buddhist chroniclers may have fabricated the Shakya connection to connect Ashoka's family to Buddha. The Buddhist texts allude to her being a Buddhist in her later years but do not describe her conversion to Buddhism. Therefore, it is likely that she was already a Buddhist when she met Ashoka.
The "Mahavamsa" states that Devi gave birth to Ashoka's son Mahinda in Ujjain, and two years later, to a daughter named Sanghamitta. According to the "Mahavamsa", Ashoka's son Mahinda was ordained at the age of 20 years, during the sixth year of Ashoka's reign. That means Mahinda must have been 14 years old when Ashoka ascended the throne. Even if Mahinda was born when Ashoka was as young as 20 years old, Ashoka must have ascended the throne at 34 years, which means he must have served as a viceroy for several years.
## Ascension to the throne.
Legends suggest that Ashoka was not the crown prince, and his ascension on the throne was disputed. 
"Ashokavadana" states that Bindusara's eldest son Susima once slapped a bald minister on his head in jest. The minister worried that after ascending the throne, Susima may jokingly hurt him with a sword. Therefore, he instigated five hundred ministers to support Ashoka's claim to the throne when the time came, noting that Ashoka was predicted to become a "chakravartin" (universal ruler). Sometime later, Takshashila rebelled again, and Bindusara dispatched Susima to curb the rebellion. Shortly after, Bindusara fell ill and was expected to die soon. Susima was still in Takshashila, having been unsuccessful in suppressing the rebellion. Bindusara recalled him to the capital and asked Ashoka to march to Takshashila. However, the ministers told him that Ashoka was ill and suggested that he temporarily install Ashoka on the throne until Susmia's return from Takshashila. When Bindusara refused to do so, Ashoka declared that if the throne were rightfully his, the gods would crown him as the next king. At that instance, the gods did so, Bindusara died, and Ashoka's authority extended to the entire world, including the Yaksha territory located above the earth and the Naga territory located below the earth. When Susima returned to the capital, Ashoka's newly appointed prime minister Radhagupta tricked him into a pit of charcoal. Susima died a painful death, and his general Bhadrayudha became a Buddhist monk.
The "Mahavamsa" states that when Bindusara fell sick, Ashoka returned to Pataliputra from Ujjain and gained control of the capital. After his father's death, Ashoka had his eldest brother killed and ascended the throne. The text also states that Ashoka killed ninety-nine of his half-brothers, including Sumana. The "Dipavamsa" states that he killed a hundred of his brothers and was crowned four years later. The "Vamsatthapakasini" adds that an Ajivika ascetic had predicted this massacre based on the interpretation of a dream of Ashoka's mother. According to these accounts, only Ashoka's uterine brother Tissa was spared. Other sources name the surviving brother Vitashoka, Vigatashoka, Sudatta (So-ta-to in "A-yi-uang-chuan"), or Sugatra (Siu-ka-tu-lu in "Fen-pie-kung-te-hun").
The figures such as 99 and 100 are exaggerated and seem to be a way of stating that Ashoka killed several of his brothers. Taranatha states that Ashoka, who was an illegitimate son of his predecessor, killed six legitimate princes to ascend the throne. It is possible that Ashoka was not the rightful heir to the throne and killed a brother (or brothers) to acquire the throne. However, the Buddhist sources have exaggerated the story, which attempts to portray him as evil before his conversion to Buddhism. Ashoka's Rock Edict No. 5 mentions officers whose duties include supervising the welfare of "the families of his brothers, sisters, and other relatives". This suggests that more than one of his brothers survived his ascension. However, some scholars oppose this suggestion, arguing that the inscription talks only about the "families" of his brothers, not the brothers themselves.
### Date of ascension.
According to the Sri Lankan texts "Mahavamsa" and the "Dipavamsa", Ashoka ascended the throne 218 years after the death of Gautama Buddha and ruled for 37 years. The date of the Buddha's death is itself a matter of debate, and the North Indian tradition states that Ashoka ruled a hundred years after the Buddha's death, which has led to further debates about the date.
Assuming that the Sri Lankan tradition is correct, and assuming that the Buddha died in 483 BCE – a date proposed by several scholars – Ashoka must have ascended the throne in 265 BCE. The Puranas state that Ashoka's father Bindusara reigned for 25 years, not 28 years as specified in the Sri Lankan tradition. If this is true, Ashoka's ascension can be dated three years earlier, to 268 BCE. Alternatively, if the Sri Lankan tradition is correct, but if we assume that the Buddha died in 486 BCE (a date supported by the Cantonese Dotted Record), Ashoka's ascension can be dated to 268 BCE. The "Mahavamsa" states that Ashoka consecrated himself as the king four years after becoming a sovereign. This interregnum can be explained assuming that he fought a war of succession with other sons of Bindusara during these four years.
The "Ashokavadana" contains a story about Ashoka's minister Yashas hiding the sun with his hand. Professor P. H. L. Eggermont theorised that this story was a reference to a partial solar eclipse that was seen in northern India on 4 May 249 BCE. According to the "Ashokavadana", Ashoka went on a pilgrimage to various Buddhist sites sometime after this eclipse. Ashoka's Rummindei pillar inscription states that he visited Lumbini during his 21st regnal year. Assuming this visit was a part of the pilgrimage described in the text, and assuming that Ashoka visited Lumbini around 1–2 years after the solar eclipse, the ascension date of 268–269 BCE seems more likely. However, this theory is not universally accepted. For example, according to John S. Strong, the event described in the "Ashokavadana" has nothing to do with chronology, and Eggermont's interpretation grossly ignores the literary and religious context of the legend.
## Reign before Buddhist influence.
Both Sri Lankan and North Indian traditions assert that Ashoka was a violent person before Buddhism. Taranatha also states that Ashoka was initially called "Kamashoka" because he spent many years in pleasurable pursuits ("kama"); he was then called "Chandashoka" ("Ashoka the fierce") because he spent some years performing evil deeds; and finally, he came to be known as Dhammashoka ("Ashoka the righteous") after his conversion to Buddhism. 
The "Ashokavadana" also calls him "Chandashoka", and describes several of his cruel acts:
The 5th-century Chinese traveller Faxian states that Ashoka personally visited the underworld to study torture methods there and then invented his methods. The 7th-century traveller Xuanzang claims to have seen a pillar marking the site of Ashoka's "Hell".
The "Mahavamsa" also briefly alludes to Ashoka's cruelty, stating that Ashoka was earlier called Chandashoka because of his evil deeds but came to be called Dharmashoka because of his pious acts after his conversion to Buddhism. However, unlike the north Indian tradition, the Sri Lankan texts do not mention any specific evil deeds performed by Ashoka, except his killing of 99 of his brothers.
Such descriptions of Ashoka as an evil person before his conversion to Buddhism appear to be a fabrication of the Buddhist authors, who attempted to present the change that Buddhism brought to him as a miracle. In an attempt to dramatise this change, such legends exaggerate Ashoka's past wickedness and his piousness after the conversion.
## Kalinga war and conversion to Buddhism.
Ashoka's inscriptions mention that he conquered the Kalinga region during his 8th regnal year: the destruction caused during the war made him repent violence, and in the subsequent years, he was drawn towards Buddhism. Edict 13 of the Edicts of Ashoka Rock Inscriptions expresses the great remorse the king felt after observing the destruction of Kalinga:
On the other hand, the Sri Lankan tradition suggests that Ashoka was already a devoted Buddhist by his 8th regnal year, converted to Buddhism during his 4th regnal year, and constructed 84,000 viharas during his 5th–7th regnal years. The Buddhist legends make no mention of the Kalinga campaign.
Based on Sri Lankan tradition, some scholars, such as Eggermont, believe Ashoka converted to Buddhism "before" the Kalinga war. Critics of this theory argue that if Ashoka were already a Buddhist, he would not have waged the violent Kalinga War. Eggermont explains this anomaly by theorising that Ashoka had his interpretation of the "Middle Way".
Some earlier writers believed that Ashoka "dramatically" converted to Buddhism after seeing the suffering caused by the war since his Major Rock Edict 13 states that he became closer to the dhamma after the annexation of Kalinga. However, even if Ashoka converted to Buddhism "after" the war, epigraphic evidence suggests that his conversion was a "gradual" process rather than a dramatic event. For example, in a Minor Rock Edict issued during his 13th regnal year (five years after the Kalinga campaign), he states that he had been an "upasaka" (lay Buddhist) for more than two and a half years, but did not make much progress; in the past year, he was drawn closer to the sangha and became a more ardent follower.
### The war.
According to Ashoka's Major Rock Edict 13, he conquered Kalinga 8 years after ascending to the throne. The edict states that during his conquest of Kalinga, 100,000 men and animals were killed in action; many times that number "perished"; and 150,000 men and animals were carried away from Kalinga as captives. Ashoka states that the repentance of these sufferings caused him to devote himself to the practice and propagation of dharma. He proclaims that he now considered the slaughter, death and deportation caused during the conquest of a country painful and deplorable; and that he considered the suffering caused to the religious people and householders even more deplorable.
This edict has been inscribed at several places, including Erragudi, Girnar, Kalsi, Maneshra, Shahbazgarhi and Kandahar. However, it is omitted in Ashoka's inscriptions found in the Kalinga region, where the Rock Edicts 13 and 14 have been replaced by two separate edicts that make no mention of Ashoka's remorse. It is possible that Ashoka did not consider it politically appropriate to make such a confession to the people of Kalinga. Another possibility is the Kalinga war and its consequences, as described in Ashoka's rock edicts, are "more imaginary than real". This description is meant to impress those far removed from the scene, thus unable to verify its accuracy.
Ancient sources do not mention any other military activity of Ashoka, although the 16th-century writer Taranatha claims that Ashoka conquered the entire Jambudvipa.
### First contact with Buddhism.
Different sources give different accounts of Ashoka's conversion to Buddhism. 
According to Sri Lankan tradition, Ashoka's father, Bindusara, was a devotee of Brahmanism, and his mother Dharma was a devotee of Ajivikas. The "Samantapasadika" states that Ashoka followed non-Buddhist sects during the first three years of his reign. The Sri Lankan texts add that Ashoka was not happy with the behaviour of the Brahmins who received his alms daily. His courtiers produced some Ajivika and Nigantha teachers before him, but these also failed to impress him. 
The "Dipavamsa" states that Ashoka invited several non-Buddhist religious leaders to his palace and bestowed great gifts upon them in the hope that they would answer a question posed by the king. The text does not state what the question was but mentions that none of the invitees were able to answer it. One day, Ashoka saw a young Buddhist monk called Nigrodha (or Nyagrodha), who was looking for alms on a road in Pataliputra. He was the king's nephew, although the king was not aware of this: he was a posthumous son of Ashoka's eldest brother Sumana, whom Ashoka had killed during the conflict for the throne. Ashoka was impressed by Nigrodha's tranquil and fearless appearance, and asked him to teach him his faith. In response, Nigrodha offered him a sermon on appamada (earnestness). Impressed by the sermon, Ashoka offered Nigrodha 400,000 silver coins and 8 daily portions of rice. The king became a Buddhist upasaka, and started visiting the Kukkutarama shrine at Pataliputra. At the temple, he met the Buddhist monk Moggaliputta Tissa, and became more devoted to the Buddhist faith. The veracity of this story is not certain. This legend about Ashoka's search for a worthy teacher may be aimed at explaining why Ashoka did not adopt Jainism, another major contemporary faith that advocates non-violence and compassion. The legend suggests that Ashoka was not attracted to Buddhism because he was looking for such a faith, rather, for a competent spiritual teacher. The Sri Lankan tradition adds that during his 6th regnal year, Ashoka's son Mahinda became a Buddhist monk, and his daughter became a Buddhist nun.
A story in "Divyavadana" attributes Ashoka's conversion to the Buddhist monk Samudra, who was an ex-merchant from Shravasti. According to this account, Samudra was imprisoned in Ashoka's "Hell", but saved himself using his miraculous powers. When Ashoka heard about this, he visited the monk, and was further impressed by a series of miracles performed by the monk. He then became a Buddhist. A story in the "Ashokavadana" states that Samudra was a merchant's son, and was a 12-year-old boy when he met Ashoka; this account seems to be influenced by the Nigrodha story.
The A-yu-wang-chuan states that a 7-year-old Buddhist converted Ashoka. Another story claims that the young boy ate 500 Brahmanas who were harassing Ashoka for being interested in Buddhism; these Brahmanas later miraculously turned into Buddhist bhikkus at the Kukkutarama monastery, where Ashoka paid a visit.
Several Buddhist establishments existed in various parts of India by the time of Ashoka's ascension. It is not clear which branch of the Buddhist sangha influenced him, but the one at his capital Pataliputra is a good candidate. Another good candidate is the one at Mahabodhi: the Major Rock Edict 8 records his visit to the Bodhi Tree – the place of Buddha's enlightenment at Mahabodhi – after his 10th regnal year, and the minor rock edict issued during his 13th regnal year suggests that he had become a Buddhist around the same time.
## Reign after Buddhist influence.
### Construction of Stupas and Temples.
Both "Mahavamsa" and "Ashokavadana" state that Ashoka constructed 84,000 stupas or viharas. According to the "Mahavamsa", this activity took place during his 5th–7th regnal years.
The "Ashokavadana" states that Ashoka collected seven out of the eight relics of Gautama Buddha, and had their portions kept in 84,000 boxes made of gold, silver, cat's eye, and crystal. He ordered the construction of 84,000 stupas throughout the earth, in towns that had a population of 100,000 or more. He told Elder Yashas, a monk at the Kukkutarama monastery, that he wanted these stupas to be completed on the same day. Yashas stated that he would signal the completion time by eclipsing the sun with his hand. When he did so, the 84,000 stupas were completed at once.
The "Mahavamsa" states that Ashoka ordered construction of 84,000 viharas (monasteries) rather than the stupas to house the relics. Like "Ashokavadana", the "Mahavamsa" describes Ashoka's collection of the relics, but does not mention this episode in the context of the construction activities. It states that Ashoka decided to construct the 84,000 viharas when Moggaliputta Tissa told him that there were 84,000 sections of the Buddha's Dhamma. Ashoka himself began the construction of the Ashokarama vihara, and ordered subordinate kings to build the other viharas. Ashokarama was completed by the miraculous power of Thera Indagutta, and the news about the completion of the 84,000 viharas arrived from various cities on the same day.
The number 84,000 is an obvious exaggeration, and it appears that in the later period, the construction of almost every old stupa was attributed to Ashoka.
The construction of following stupas and viharas is credited to Ashoka:
### Propagation of dhamma.
Ashoka's rock edicts suggest that during his 8th–9th regnal years, he made a pilgrimage to the Bodhi Tree, started propagating dhamma, and performed social welfare activities. The welfare activities included establishment of medical treatment facilities for humans and animals; plantation of medicinal herbs; and digging of wells and plantation of trees along the roads. These activities were conducted in the neighbouring kingdoms, including those of the Cholas, the Pandyas, the Satiyaputras, Tamraparni, the Greek kingdom of Antiyoka.
The edicts also state that during his 10th–11th regnal years, Ashoka became closer to the Buddhist sangha, and went on a tour of the empire that lasted for at least 256 days.
By his 12th regnal year, Ashoka had started inscribing edicts to propagate dhamma, having ordered his officers ("rajjukas" and "pradesikas") to tour their jurisdictions every five years for inspection and for preaching "dhamma". By the next year, he had set up the post of the "dharma-mahamatra".
During his 14th regnal year, he commissioned the enlargement of the stupa of Buddha Kanakamuni.
### Third Buddhist Council.
The Sri Lankan tradition presents a greater role for Ashoka in the Buddhist community. In this tradition, Ashoka starts feeding monks on a large scale. His lavish patronage to the state patronage leads to many fake monks joining the sangha. The true Buddhist monks refuse to co-operate with these fake monks, and therefore, no uposatha ceremony is held for seven years. The king attempts to eradicate the fake monks, but during this attempt, an over-zealous minister ends up killing some real monks. The king then invites the elder monk Moggaliputta-Tissa, to help him expel non-Buddhists from the monastery founded by him at Pataliputra. 60,000 monks (bhikkhus) convicted of being heretical are de-frocked in the ensuing process. The uposatha ceremony is then held, and Tissa subsequently organises the Third Buddhist council, during the 17th regnal year of Ashoka. Tissa compiles "Kathavatthu", a text that reaffirms Theravadin orthodoxy on several points.
The North Indian tradition makes no mention of these events, which has led to doubts about the historicity of the Third Buddihst council.
Richard Gombrich argues that the non-corroboration of this story by inscriptional evidence cannot be used to dismiss it as completely unhistorical, as several of Ashoka's inscriptions may have been lost. Gombrich also argues that Asohka's inscriptions prove that he was interested in maintaining the "unanimity and purity" of the Sangha. For example, in his Minor Rock Edict 3, Ashoka recommends the members of the Sangha to study certain texts (most of which remain unidentified). Similarly, in an inscription found at Sanchi, Sarnath, and Kosam, Ashoka mandates that the dissident members of the sangha should be expelled, and expresses his desire to the Sangha remain united and flourish.
The 8th century Buddhist pilgrim Yijing records another story about Ashoka's involvement in the Buddhist sangha. According to this story, the earlier king Bimbisara, who was a contemporary of the Gautama Buddha, once saw 18 fragments of a cloth and a stick in a dream. The Buddha interpreted the dream to mean that his philosophy would be divided into 18 schools after his death, and predicted that a king called Ashoka would unite these schools over a hundred years later.
### Buddhist missions.
In the Sri Lankan tradition, Moggaliputta-Tissa – who is patronised by Ashoka – sends out nine Buddhist missions to spread Buddhism in the "border areas" in c. 250 BCE. This tradition does not credit Ashoka directly with sending these missions. Each mission comprises five monks, and is headed by an elder. To Sri Lanka, he sent his own son Mahinda, accompanied by four other Theras – Itthiya, Uttiya, Sambala and Bhaddasala. Next, with Moggaliputta-Tissa's help, Ashoka sent Buddhist missionaries to distant regions such as Kashmir, Gandhara, Himalayas, the land of the Yonas (Greeks), Maharashtra, Suvannabhumi, and Sri Lanka.
The Sri Lankan tradition dates these missions to Ashoka's 18th regnal year, naming the following missionaries:
The tradition adds that during his 19th regnal year, Ashoka's daughter Sanghamitta went to Sri Lanka to establish an order of nuns, taking a sapling of the sacred Bodhi Tree with her.
The North Indian tradition makes no mention of these events. Ashoka's own inscriptions also appear to omit any mention of these events, recording only one of his activities during this period: in his 19th regnal year, he donated the Khalatika Cave to ascetics to provide them a shelter during the rainy season. Ashoka's Pillar Edicts suggest that during the next year, he made pilgrimage to Lumbini – the place of Buddha's birth, and to the stupa of the Buddha Kanakamuni.
The Rock Edict XIII states that Ashoka's won a "dhamma victory" by sending messengers to five kings and several other kingdoms. Whether these missions correspond to the Buddhist missions recorded in the Buddhist chronicles is debated. Indologist Etienne Lamotte argues that the "dhamma" missionaries mentioned in Ashoka's inscriptions were probably not Buddhist monks, as this "dhamma" was not same as "Buddhism". Moreover, the lists of destinations of the missions and the dates of the missions mentioned in the inscriptions do not tally the ones mentioned in the Buddhist legends.
Other scholars, such as Erich Frauwallner and Richard Gombrich, believe that the missions mentioned in the Sri Lankan tradition are historical. According to these scholars, a part of this story is corroborated by archaeological evidence: the "Vinaya Nidana" mentions names of five monks, who are said to have gone to the Himalayan region; three of these names have been found inscribed on relic caskets found at Bhilsa (near Vidisha). These caskets have been dated to the early 2nd century BCE, and the inscription states that the monks are of the Himalayan school. The missions may have set out from Vidisha in central India, as the caskets were discovered there, and as Mahinda is said to have stayed there for a month before setting out for Sri Lanka.
According to Gombrich, the mission may have included representatives of other religions, and thus, Lamotte's objection about "dhamma" is not valid. The Buddhist chroniclers may have decided not to mention these non-Buddhists, so as not to sideline Buddhism. Frauwallner and Gombrich also believe that Ashoka was directly responsible for the missions, since only a resourceful ruler could have sponsored such activities. The Sri Lankan chronicles, which belong to the Theravada school, exaggerate the role of the Theravadin monk Moggaliputta-Tissa in order to glorify their sect.
Some historians argue that Buddhism became a major religion because of Ashoka's royal patronage. However, epigraphic evidence suggests that the spread of Buddhism in north-western India and Deccan region was less because of Ashoka's missions, and more because of merchants, traders, landowners and the artisan guilds who supported Buddhist establishments.
### Violence after conversion.
According to the "Ashokavadana", Ashoka resorted to violence even after converting to Buddhism. For example:
According to the "Ashokavadana", a non-Buddhist in Pundravardhana drew a picture showing the Buddha bowing at the feet of the Nirgrantha leader Jnatiputra. The term nirgrantha ("free from bonds") was originally used for a pre-Jaina ascetic order, but later came to be used for Jaina monks. "Jnatiputra" is identified with Mahavira, 24th Tirthankara of Jainism. The legend states that on complaint from a Buddhist devotee, Ashoka issued an order to arrest the non-Buddhist artist, and subsequently, another order to kill all the Ajivikas in Pundravardhana. Around 18,000 followers of the Ajivika sect were executed as a result of this order. Sometime later, another Nirgrantha follower in Pataliputra drew a similar picture. Ashoka burnt him and his entire family alive in their house. He also announced an award of one dinara (silver coin) to anyone who brought him the head of a Nirgrantha heretic. According to "Ashokavadana", as a result of this order, his own brother was mistaken for a heretic and killed by a cowherd. Ashoka realised his mistake, and withdrew the order.
For several reasons, scholars say, these stories of persecutions of rival sects by Ashoka appear to be clear fabrications arising out of sectarian propaganda.
## Last years.
### Tissarakkha as the queen.
Ashoka's last dated inscription - the Pillar Edict 4 is from his 26th regnal year. The only source of information about Ashoka's later years are the Buddhist legends. The Sri Lankan tradition states that Ashoka's queen Asandhamitta died during his 29th regnal year, and in his 32nd regnal year, his wife Tissarakkha was given the title of queen.
Both "Mahavamsa" and "Ashokavadana" state that Ashoka extended favours and attention to the Bodhi Tree, and a jealous Tissarakkha mistook "Bodhi" to be a mistress of Ashoka. She then used black magic to make the tree wither. According to the "Ashokavadana", she hired a sorceress to do the job, and when Ashoka explained that "Bodhi" was the name of a tree, she had the sorceress heal the tree. According to the "Mahavamsa", she completely destroyed the tree, during Ashoka's 34th regnal year.
The "Ashokavadana" states that Tissarakkha (called "Tishyarakshita" here) made sexual advances towards Ashoka's son Kunala, but Kunala rejected her. Subsequently, Ashoka granted Tissarakkha kingship for seven days, and during this period, she tortured and blinded Kunala. Ashoka then threatened to "tear out her eyes, rip open her body with sharp rakes, impale her alive on a spit, cut off her nose with a saw, cut out her tongue with a razor." Kunala regained his eyesight miraculously, and pleaded for mercy from the queen, but Ashoka had her executed anyway. Kshemendra's "Avadana-kalpa-lata" also narrates this legend, but seeks to improve Ashoka's image by stating that he forgave the queen after Kunala regained his eyesight.
### Death.
According to the Sri Lankan tradition, Ashoka died during his 37th regnal year, which suggests that he died around 232 BCE.
According to the "Ashokavadana", the emperor fell severely ill during his last days. He started using state funds to make donations to the Buddhist sangha, prompting his ministers to deny him access to the state treasury. Ashoka then started donating his personal possessions, but was similarly restricted from doing so. On his deathbed, his only possession was the half of a myrobalan fruit, which he offered to the sangha as his final donation. Such legends encourage generous donations to the "sangha" and highlight the role of the kingship in supporting the Buddhist faith.
Legend states that during his cremation, his body burned for seven days and nights.
## Family.
### Queens.
Various sources mention five consorts of Ashoka: Devi (or Vedisa-Mahadevi-Shakyakumari), Karuvaki, Asandhimitra (Pali: Asandhimitta), Padmavati, and Tishyarakshita (Pali: Tissarakkha).
Kaurvaki is the only queen of Ashoka known from his own inscriptions: she is mentioned in an edict inscribed on a pillar at Allahabad. The inscription names her as the mother of prince Tivara, and orders the royal officers (mahamattas) to record her religious and charitable donations. According to one theory, Tishyarakshita was the regnal name of Kaurvaki.
According to the "Mahavamsa", Ashoka's chief queen was Asandhimitta, who died four years before him. It states that she was born as Ashoka's queen because in a previous life, she directed a pratyekabuddha to a honey merchant (who was later reborn as Ashoka). Some later texts also state that she additionally gave the pratyekabuddha a piece of cloth made by her. These texts include the "Dasavatthuppakarana", the so-called Cambodian or Extended "Mahavamsa" (possibly from 9th–10th centuries), and the "Trai Bhumi Katha" (15th century). These texts narrate another story: one day, Ashoka mocked Asandhamitta was enjoying a tasty piece of sugarcane without having earned it through her karma. Asandhamitta replied that all her enjoyments resulted from merit resulting from her own karma. Ashoka then challenged her to prove this by procuring 60,000 robes as an offering for monks. At night, the guardian gods informed her about her past gift to the pratyekabuddha, and next day, she was able to miraculously procure the 60,000 robes. An impressed Ashoka makes her his favourite queen, and even offers to make her a sovereign ruler. Asandhamitta refuses the offer, but still invokes the jealousy of Ashoka's 16,000 other women. Ashoka proves her superiority by having 16,000 identical cakes baked with his royal seal hidden in only one of them. Each wife is asked to choose a cake, and only Asandhamitta gets the one with the royal seal. The "Trai Bhumi Katha" claims that it was Asandhamitta who encouraged her husband to become a Buddhist, and to construct 84,000 stupas and 84,000 viharas.
According to "Mahavamsa", after Asandhamitta's death, Tissarakkha became the chief queen. The Ashokavadana does not mention Asandhamitta at all, but does mention Tissarakkha as Tishyarakshita. The "Divyavadana" mentions another queen called Padmavati, who was the mother of the crown-prince Kunala.
As mentioned above, according to the Sri Lankan tradition, Ashoka fell in love with Devi (or Vidisha-Mahadevi), as a prince in central India. After Ashoka's ascention to the throne, Devi chose to remain at Vidisha than move to the royal capital Pataliputra. According to the "Mahavmsa", Ashoka's chief queen was Asandhamitta, not Devi: the text does not talk of any connection between the two women, so it is unlikely that Asandhamitta was another name for Devi. The Sri Lankan tradition uses the word "samvasa" to describe the relationship between Ashoka and Devi, which modern scholars variously interpret as sexual relations outside marriage, or co-residence as a married couple. Those who argue that Ashoka did not marry Devi argue that their theory is corroborated by the fact that Devi did not become Ashoka's chief queen in Pataliputra after his ascension. The "Dipavamsa" refers to two children of Ashoka and Devi – Mahinda and Sanghamitta.
### Sons.
Tivara, the son of Ashoka and Karuvaki, is the only of Ashoka's sons to be mentioned by name in the inscriptions.
According to North Indian tradition, Ashoka had a son named Kunala. Kunala had a son named Samprati.
The Sri Lankan tradition mentions a son called Mahinda, who was sent to Sri Lanka as a Buddhist missionary; this son is not mentioned at all in the North Indian tradition. The Chinese pilgrim Xuanzang states that Mahinda was Ashoka's younger brother (Vitashoka or Vigatashoka) rather than his illgetimate son.
The "Divyavadana" mentions the crown-prince Kunala alias Dharmavivardhana, who was a son of queen Padmavati. According to Faxian, Dharmavivardhana was appointed as the governor of Gandhara.
The "Rajatarangini" mentions Jalauka as a son of Ashoka.
### Daughters.
According to Sri Lankan tradition, Ashoka had a daughter named Sanghamitta, who became a Buddhist nun. A section of historians, such as Romila Thapar, doubt the historicity of Sanghamitta, based on the following points:
Another source mentions that Ashoka had a daughter named Charumati, who married a kshatriya named Devapala.
### Brothers.
According to the "Ashokavadana", Ashoka had an elder half-brother named Susima. According to the Sri Lankan tradition, Ashoka killed his 99 half-brothers.
Various sources mention that one of Ashoka's brothers survived his ascension, and narrate stories about his role in the Buddhist community.
## Imperial extent.
The extent of the territory controlled by Ashoka's predecessors is not certain, but it is possible that the empire of his grandfather Chandragupta extended across northern India from the western coast (Arabian Sea) to the eastern coast (Bay of Bengal), covering nearly two-thirds of the Indian subcontinent. Bindusara and Ashoka seem to have extended the empire southwards. The distribution of Ashoka's inscriptions suggests that his empire included almost the entire Indian subcontinent, except its southernmost parts. The Rock Edicts 2 and 13 suggest that these southernmost parts were controlled by the Cholas, the Pandyas, the Keralaputras, and the Satiyaputras. In the north-west, Ashoka's kingdom extended up to Kandahar, to the east of the Seleucid Empire ruled by Antiochus II. The capital of Ashoka's empire was Pataliputra in the Magadha region.
## Religion and philosophy.
### Relationship with Buddhism.
The Buddhist legends state that Ashoka converted to Buddhism, although this has been debated by a section of scholars. The Minor Rock Edict 1 leaves no doubt that Ashoka was a follower of Buddhism. In this edict, he calls himself an upasaka (a lay follower of Buddhism) and a "sakya" (i.e. Buddhist, after Gautama Buddha's title "Shakya-Muni"). This and several other edicts are evidence of his Buddhist affiliation:
### Other religions.
A legend in the Buddhist text "Vamsatthapakasini" states that an Ajivika ascetic invited to interpret a dream of Ashoka's mother had predicted that he would patronise Buddhism and destroy 96 heretical sects. However, such assertions are directly contradicted by Ashoka's own inscriptions. Ashoka's edicts, such as the Rock Edicts 6, 7, and 12, emphasise tolerance of all sects. Similarly, in his Rock Edict 12, Ashoka honours people of all faiths. In his inscriptions, Ashoka dedicates caves to non-Buddhist ascetics, and repeatedly states that both Brahmins and shramanas deserved respect. He also tells people "not to denigrate other sects, but to inform themselves about them".
In fact, there is no evidence that Buddhism was a state religion under Ashoka. None of Ashoka's extant edicts record his direct donations to the Buddhists. One inscription records donations by his queen Karuvaki, while the emperor is known to have donated the Barabar Caves to the Ajivikas. There are some indirect references to his donations to Buddhists. For example, the Nigalisagar Pillar inscription records his enlargement of the Konakamana stupa. Similarly, the Lumbini (Rumminidei) inscription states that he exempted the village of Buddha's birth from the land tax, and reduced the revenue tax to one-eighth.
Ashoka appointed the "dhamma-mahamatta" officers, whose duties included the welfare of various religious sects, including the Buddhist sangha, Brahmins, Ajivikas, and Nirgranthas. The Rock Edicts 8 and 12, and the Pillar Edict 7, mandate donations to all religious sects.
Ashoka's Minor Rock Edict 1 contains the phrase ""amissā devā". According to one interpretation, the term "amissā" derives from the word "amṛṣa"" ("false"), and thus, the phrase is a reference to Ashoak's belief in "true" and "false" gods. However, it is more likely that the term derives from the word "amiśra" ("not mingled"), and the phrase refers to celestial beings who did not mingle with humans. The inscription claims that the righteousness generated by adoption of dhamma by the humans attracted even the celestial gods who did not mingle with humans.
### Dharma.
Ashoka's various inscriptions suggest that he devoted himself to the propagation of "Dharma" (Pali: Dhamma), a term that refers to the teachings of Gautama Buddha in the Buddhist circles. However, Ashoka's own inscriptions do not mention Buddhist doctrines such as the Four Noble Truths or Nirvana. The word "Dharma" has various connotations in the Indian religions, and can be generally translated as "law, duty, or righteousness". In the Kandahar inscriptions of Ashoka, the word "Dharma" has been translated as eusebeia (Greek) and qsyt (Aramaic), which further suggests that his "Dharma" meant something more generic than Buddhism.
The inscriptions suggest that for Ashoka, Dharma meant "a moral polity of active social concern, religious tolerance, ecological awareness, the observance of common ethical precepts, and the renunciation of war." For example:
Modern scholars have variously understood this "dhamma" as a Buddhist lay ethic, a set of politico-moral ideas, a "sort of universal religion", or as an Ashokan innovation. On the other hand, it has also been interpreted as an "essentially political" ideology that sought to knit together a vast and diverse empire.
Ashoka instituted a new category of officers called the "dhamma-mahamattas", who were tasked with the welfare of the aged, the infrm, the women and children, and various religious sects. They were also sent on diplomatic missions to the Hellenistic kingdoms of west Asia, in order to propagate the dhamma.
Historically, the image of Ashoka in the global Buddhist circles was based on legends (such as those mentioned in the "Ashokavadana") rather than his rock edicts. This was because the Brahmi script in which these edicts were written was forgotten soon and remained undeciphered until its study by James Prinsep in the 19th century. The writings of the Chinese Buddhist pilgrims such as Faxian and Xuanzang suggest that Ashoka's inscriptions mark the important sites associated with Gautama Buddha. These writers attribute Buddhism-related content to Ashoka's edicts, but this content does not match with the actual text of the inscriptions as determined by modern scholars after the decipherment of the Brahmi script. It is likely that the script was forgotten by the time of Faxian, who probably relied on local guides; these guides may have made up some Buddhism-related interpretations to gratify him, or may have themselves relied on faulty translations based on oral traditions. Xuanzang may have encountered a similar situation, or may have taken the supposed content of the inscriptions from Faxian's writings. This theory is corroborated by the fact that some Brahmin scholars are known to have similarly come up with a fanciful interpretation of Ashoka pillar inscriptions, when requested to decipher them by the 14th century Muslim king Firuz Shah Tughlaq. According to Shams-i Siraj's "Tarikh-i Firoz Shahi", after the king had these pillar transported from Topra and Mirat to Delhi as war trophies, these Brahmins told him that the inscriptions prophesized that nobody would be able to remove the pillars except a king named Firuz. Moreover, by this time, there were local traditions that attributed the erection of these pillars to the legendary hero Bhima.
According to scholars such as Richard Gombrich, Ashoka's dharma shows Buddhist influence. For example, the Kalinga Separate Edict I seems to be inspired by Buddha's "Advice to Sigala" and his other sermons.
#### Animal welfare.
Ashoka's rock edicts declare that injuring living things is not good, and no animal should be slaughtered for sacrifice. However, he did not prohibit common cattle slaughter or beef eating.
He imposed a ban on killing of "all four-footed creatures that are neither useful nor edible", and of specific animal species including several birds, certain types of fish and bulls among others. He also banned killing of female goats, sheep and pigs that were nursing their young; as well as their young up to the age of six months. He also banned killing of all fish and castration of animals during certain periods such as Chaturmasa and Uposatha.
Ashoka also abolished the royal hunting of animals and restricted the slaying of animals for food in the royal residence. Because he banned hunting, created many veterinary clinics and eliminated meat eating on many holidays, the Mauryan Empire under Ashoka has been described as "one of the very few instances in world history of a government treating its animals as citizens who are as deserving of its protection as the human residents".
## Foreign relations.
It is well known that Ashoka sent "dütas" or emissaries to convey messages or letters, written or oral (rather both), to various people. The VIth Rock Edict about "oral orders" reveals this. It was later confirmed that it was not unusual to add oral messages to written ones, and the content of Ashoka's messages can be inferred likewise from the XIIIth Rock Edict: They were meant to spread his "dhammavijaya," which he considered the highest victory and which he wished to propagate everywhere (including far beyond India). There is obvious and undeniable trace of cultural contact through the adoption of the Kharosthi script, and the idea of installing inscriptions might have travelled with this script, as Achaemenid influence is seen in some of the formulations used by Ashoka in his inscriptions. This indicates to us that Ashoka was indeed in contact with other cultures, and was an active part in mingling and spreading new cultural ideas beyond his own immediate walls.
### Hellenistic world.
In his rock edicts, Ashoka states that he had encouraged the transmission of Buddhism to the Hellenistic kingdoms to the west and that the Greeks in his dominion were converts to Buddhism and recipients of his envoys:
It is possible, but not certain, that Ashoka received letters from Greek rulers and was acquainted with the Hellenistic royal orders in the same way as he perhaps knew of the inscriptions of the Achaemenid kings, given the presence of ambassadors of Hellenistic kings in India (as well as the "dütas" sent by Ashoka himself). Dionysius is reported to have been such a Greek ambassador at the court of Ashoka, sent by Ptolemy II Philadelphus, who himself is mentioned in the Edicts of Ashoka as a recipient of the Buddhist proselytism of Ashoka. Some Hellenistic philosophers, such as Hegesias of Cyrene, who probably lived under the rule of King Magas, one of the supposed recipients of Buddhist emissaries from Asoka, are sometimes thought to have been influenced by Buddhist teachings.
The Greeks in India even seem to have played an active role in the propagation of Buddhism, as some of the emissaries of Ashoka, such as Dharmaraksita, are described in Pali sources as leading Greek (Yona) Buddhist monks, active in spreading Buddhism (the "Mahavamsa", XII).
Some Greeks (Yavana) may have played an administrative role in the territories ruled by Ashoka. The Girnar inscription of Rudradaman records that during the rule of Ashoka, a Yavana Governor was in charge in the area of Girnar, Gujarat, mentioning his role in the construction of a water reservoir.
It is thought that Ashoka's palace at Patna was modelled after the Achaemenid palace of Persepolis.
## Legends about past lives.
Buddhist legends mention stories about Ashoka's past lives. According to a "Mahavamsa" story, Ashoka, Nigrodha and Devnampiya Tissa were brothers in a previous life. In that life, a pratyekabuddha was looking for honey to cure another, sick pratyekabuddha. A woman directed him to a honey shop owned by the three brothers. Ashoka generously donated honey to the pratyekabuddha, and wished to become the sovereign ruler of Jambudvipa for this act of merit. The woman wished to become his queen, and was reborn as Ashoka's wife Asandhamitta. Later Pali texts credit her with an additional act of merit: she gifted the pratyekabuddha a piece of cloth made by her. These texts include the "Dasavatthuppakarana", the so-called Cambodian or Extended "Mahavamsa" (possibly from 9th–10th centuries), and the "Trai Bhumi Katha" (15th century).
According to an "Ashokavadana" story, Ashoka was born as Jaya in a prominent family of Rajagriha. When he was a little boy, he gave the Gautama Buddha dirt imagining it to be food. The Buddha approved of the donation, and Jaya declared that he would become a king by this act of merit. The text also state that Jaya's companion Vijaya was reborn as Ashoka's prime-minister Radhagupta. In the later life, the Buddhist monk Upagupta tells Ashoka that his rough skin was caused by the impure gift of dirt in the previous life. Some later texts repeat this story, without mentioning the negative implications of gifting dirt; these texts include Kumaralata's "Kalpana-manditika", Aryashura's "Jataka-mala", and the "Maha-karma-vibhaga". The Chinese writer Pao Ch'eng's "Shih chia ju lai ying hua lu" asserts that an insignificant act like gifting dirt could not have been meritorious enough to cause Ashoka's future greatness. Instead, the text claims that in another past life, Ashoka commissioned a large number of Buddha statues as a king, and this act of merit caused him to become a great emperor in the next life.
The 14th century Pali-language fairy tale "Dasavatthuppakarana" (possibly from c. 14th century) combines the stories about the merchant's gift of honey, and the boy's gift of dirt. It narrates a slightly different version of the "Mahavamsa" story, stating that it took place before the birth of the Gautama Buddha. It then states that the merchant was reborn as the boy who gifted dirt to the Buddha; however, in this case, the Buddha his attendant to Ānanda to create plaster from the dirt, which is used repair cracks in the monastery walls.
## Legacy.
### Architecture.
Besides the various stupas attributed to Ashoka, the pillars erected by him survive at various places in the Indian subcontinent.
Ashoka is often credited with the beginning of stone architecture in India, possibly following the introduction of stone-building techniques by the Greeks after Alexander the Great. Before Ashoka's time, buildings were probably built in non-permanent material, such as wood, bamboo or thatch. Ashoka may have rebuilt his palace in Pataliputra by replacing wooden material by stone, and may also have used the help of foreign craftmen. Ashoka also innovated by using the permanent qualities of stone for his written edicts, as well as his pillars with Buddhist symbolism.
### Symbols.
Ashokan capitals were highly realistic and used a characteristic polished finish, Mauryan polish, giving a shiny appearance to the stone surface. Lion Capital of Ashoka, the capital of one of the pillars erected by Ashoka features a carving of a spoked wheel, known as the Ashoka Chakra. This wheel represents the wheel of Dhamma set in motion by the Gautama Buddha, and appears on the flag of modern India. This capital also features sculptures of lions, which appear on the seal of India.
### Inscriptions.
The edicts of Ashoka are a collection of 33 inscriptions on the Pillars of Ashoka, as well as boulders and cave walls, issued during his reign. These inscriptions are dispersed throughout modern-day Pakistan and India, and represent the first tangible evidence of Buddhism. The edicts describe in detail the first wide expansion of Buddhism through the sponsorship of one of the most powerful kings of Indian history, offering more information about Ashoka's proselytism, moral precepts, religious precepts, and his notions of social and animal welfare.
Before Ashoka, the royal communications appear to have been written on perishable materials such as palm leaves, birch barks, cotton cloth, and possibly wooden boards. While Ashoka's administration would have continued to use these materials, Ashoka also had his messages inscribed on rock edicts. Ashoka probably got the idea of putting up these inscriptions from the neighbouring Achaemenid empire. It is likely that Ashoka's messages were also inscribed on more perishable materials, such as wood, and sent to various parts of the empire. None of these records survive now.
Scholars are still attempting to analyse both the expressed and implied political ideas of the Edicts (particularly in regard to imperial vision), and make inferences pertaining to how that vision was grappling with problems and political realities of a "virtually subcontinental, and culturally and economically highly variegated, 3rd century BCE Indian empire. Nonetheless, it remains clear that Ashoka's Inscriptions represent the earliest corpus of royal inscriptions in the Indian subcontinent, and therefore prove to be a very important innovation in royal practices."
Most of Ashoka's inscriptions are written in a mixture of various Prakrit dialects, in the Brahmi script.
Several of Ashoka's inscriptions appear to have been set up near towns, on important routes, and at places of religious significance. Many of the inscriptions have been discovered in hills, rock shelters, and places of local significance. Various theories have been put forward about why Ashoka or his officials chose such places, including that they were centres of megalithic cultures, were regarded as sacred spots in Ashoka's time, or that their physical grandeur may be symbolic of spiritual dominance. Ashoka's inscriptions have not been found at major cities of the Maurya empire, such as Pataliputra, Vidisha, Ujjayini, and Taxila.
 It is possible that many of these inscriptions are lost; the 7th century Chinese pilgrim Xuanzang refers to some of Ashoka's pillar edicts, which have not been discovered by modern researchers.
It appears that Ashoka dispatched every message to his provincial governors, who in turn, relayed it to various officials in their territory. For example, the Minor Rock Edict 1 appears in several versions at multiple places: all the versions state that Ashoka issued the proclamation while on a tour, having spent 256 days on tour. The number 256 indicates that the message was dispatched simultaneously to various places. Three versions of a message, found at edicts in the neighbouring places in Karnataka (Brahmagiri, Siddapura, and Jatinga-Rameshwara), were sent from the southern province's capital Suvarnagiri to various places. All three versions contain the same message, preceded by an initial greeting from the "arya-putra" (presumably Ashoka's son and the provincial governor) and the "mahamatras" (officials) in Suvarnagiri.
### Coinage.
The caduceus appears as a symbol of the punch-marked coins of the Maurya Empire in India, in the 3rd–2nd century BCE. Numismatic research suggests that this symbol was the symbol of king Ashoka, his personal "Mudra". This symbol was not used on the pre-Mauryan punch-marked coins, but only on coins of the Maurya period, together with the three arched-hill symbol, the "peacock on the hill", the triskelis and the Taxila mark.
## Modern scholarship.
### Rediscovery.
Ashoka had almost been forgotten, but in the 19th century James Prinsep contributed in the revelation of historical sources. After deciphering the Brahmi script, Prinsep had originally identified the "Priyadasi" of the inscriptions he found with the King of Ceylon Devanampiya Tissa. However, in 1837, George Turnour discovered an important Sri Lankan manuscript (Dipavamsa, or "Island Chronicle" ) associating Piyadasi with Ashoka:
Since then, the association of "Devanampriya Priyadarsin" with Ashoka was confirmed through various inscriptions, and especially confirmed in the Minor Rock Edict inscription discovered in Maski, directly associating Ashoka with his regnal title Devanampriya ("Beloved-of-the-Gods"):
Another important historian was British archaeologist John Hubert Marshall, who was director-General of the Archaeological Survey of India. His main interests were Sanchi and Sarnath, in addition to Harappa and Mohenjodaro. Sir Alexander Cunningham, a British archaeologist and army engineer, and often known as the father of the Archaeological Survey of India, unveiled heritage sites like the Bharhut Stupa, Sarnath, Sanchi, and the Mahabodhi Temple. Mortimer Wheeler, a British archaeologist, also exposed Ashokan historical sources, especially the Taxila.
### Perceptions and historiography.
The use of Buddhist sources in reconstructing the life of Ashoka has had a strong influence on perceptions of Ashoka, as well as the interpretations of his Edicts. Building on traditional accounts, early scholars regarded Ashoka as a primarily Buddhist monarch who underwent a conversion from the Vedic religion to Buddhism and was actively engaged in sponsoring and supporting the Buddhist monastic institution. Some scholars have tended to question this assessment. Romila Thappar writes about Ashoka that "We need to see him both as a statesman in the context of inheriting and sustaining an empire in a particular historical period, and as a person with a strong commitment to changing society through what might be called the propagation of social ethics." The only source of information not attributable to Buddhist sources are the Ashokan Edicts, and these do not explicitly state that Ashoka was a Buddhist. In his edicts, Ashoka expresses support for all the major religions of his time: Buddhism, Brahmanism, Jainism, and Ajivikaism, and his edicts addressed to the population at large (there are some addressed specifically to Buddhists; this is not the case for the other religions) generally focus on moral themes members of all the religions would accept. For example, Amartya Sen writes, "The Indian Emperor Ashoka in the third century BCE presented many political inscriptions in favor of tolerance and individual freedom, both as a part of state policy and in the relation of different people to each other".
However, the edicts alone strongly "indicate" that he was a Buddhist. In one edict he belittles rituals, and he banned Vedic animal sacrifices; these strongly suggest that he at least did not look to the Vedic tradition for guidance. Furthermore, many edicts are expressed to Buddhists alone; in one, Ashoka declares himself to be an "upasaka", and in another he demonstrates a close familiarity with Buddhist texts. He erected rock pillars at Buddhist holy sites, but did not do so for the sites of other religions. He also used the word "dhamma" to refer to qualities of the heart that underlie moral action; this was an exclusively Buddhist use of the word. However, he used the word more in the spirit than as a strict code of conduct. Romila Thappar writes, "His dhamma did not derive from divine inspiration, even if its observance promised heaven. It was more in keeping with the ethic conditioned by the logic of given situations. His logic of Dhamma was intended to influence the conduct of categories of people, in relation to each other. Especially where they involved unequal relationships." Finally, he promotes ideals that correspond to the first three steps of the Buddha's graduated discourse.
Much of the knowledge about Ashoka comes from the several inscriptions that he had carved on pillars and rocks throughout the empire. All his inscriptions present him as compassionate and loving. In the Kalinga rock edits, he addresses his people as his "children" and mentions that as a father he desires their good.
### Impact of pacifism.
After Ashoka's death, the Maurya dynasty declined rapidly. The various Puranas provide different details about Ashoka's successors, but all agree that they had relatively short reigns. The empire seems to have weakened, fragmented, and suffered an invasion from the Bactrian Greeks.
Some historians, such as H. C. Raychaudhuri, have argued that Ashoka's pacifism undermined the "military backbone" of the Maurya empire. Others, such as Romila Thapar, have suggested that the extent and impact of his pacifism have been "grossly exaggerated".

</doc>
<doc id="1241" url="https://en.wikipedia.org/wiki?curid=1241" title="American (word)">
American (word)

The meaning of the word American in the English language varies according to the historical, geographical, and political context in which it is used. "American" is derived from "America", a term originally denoting all of the Americas (also called the Western Hemisphere). In some expressions, it retains this Pan-American sense, but its usage has evolved over time and, for various historical reasons, the word came to denote people or things specifically from the United States of America.
In modern English, "American" generally refers to persons or things related to the United States of America; among native English speakers this usage is almost universal, with any other use of the term requiring specification. However, some linguists in the past have argued that "American" should be widened to also include people or things from anywhere in the American continents.
The word can be used as either an adjective or a noun (viz. a demonym). In adjectival use, it means "of or relating to the United States"; for example, "Elvis Presley was an American singer" or "the man prefers American English". In its noun form, the word generally means a resident or citizen of the US, but is also used for someone whose ethnic identity is simply "American". The noun is rarely used in English to refer to people not connected to the United States when intending a geographical meaning. When used with a grammatical qualifier, the adjective "American" can mean "of or relating to the Americas", as in Latin American or Indigenous American. Less frequently, the adjective can take this meaning without a qualifier, as in "American Spanish dialects and pronunciation differ by country", or the name of the Organization of American States. A third use of the term pertains specifically to the indigenous peoples of the Americas, for instance, "In the 16th century, many Americans died from imported diseases during the European conquest", though this usage is rare, as "indigenous", "First Nations" or "Amerindian" are considered less confusing and generally more appropriate.
Compound constructions which indicate a minority ethnic group, such as "African-Americans" likewise refer exclusively to people in or from the United States of America, as does the prefix "Americo-". For instance, the Americo-Liberians and their language Merico derive their name from the fact that they are descended from African-American settlers, i.e. Blacks who were formerly enslaved in the United States of America.
## Other languages.
French, German, Italian, Japanese, Hebrew, Arabic, and Russian speakers may use cognates of "American" to refer to inhabitants of the Americas or to U.S. nationals. They generally have other terms specific to U.S. nationals, such as the German , French , Japanese , and Italian . These specific terms may be less common than the term "American".
In French, , or , from ("United States of America"), is a rarely used word that distinguishes U.S. things and persons from the adjective , which denotes persons and things from the United States, but may also refer to "the Americas".
Likewise, German's use of and observe said cultural distinction, solely denoting U.S. things and people. Note that in normal parlance, the adjective "American" and its direct cognates are usually used if the context renders the nationality of the person clear.
This differentiation is prevalent in German-speaking countries, as indicated by the style manual of the "Neue Zürcher Zeitung" (one of the leading German-language newspapers in Switzerland) which dismisses the term as both ′unnecessary′ and ′artificial′ and recommends replacing it with "amerikanisch". The respective guidelines of the foreign ministries of Austria, Germany and Switzerland all prescribe "Amerikaner" and "amerikanisch" in reference to the United States for official usage, making no mention of or .
Portuguese has , denoting both a person or thing from the Americas and a U.S. national. For referring specifically to a U.S. national and things, some words used are (also spelled , "United States person"), from , and ("Yankee")—both usages exist in Brazil, but are uncommon in Portugal—but the term most often used, and the only one in Portugal, is , even though it could, as with its Spanish equivalent, apply to Canadians and Mexicans as well.
In Spanish, denotes geographic and cultural origin in the New World, as well as (infrequently) a U.S. citizen; the more common term is ("United States person"), which derives from ("United States of America"). The Spanish term ("North American") is frequently used to refer things and persons from the United States, but this term can also denote people and things from Canada and Mexico. Among Spanish-speakers, North America generally doesn't include Central America or the Caribbean.
In other languages, however, there is no possibility for confusion. For example, the Chinese word for "U.S. national" is () is derived from a word for the United States, , where is an abbreviation for "Yàměilìjiā" ("America") and is "country". The name for the American continents is , from plus ("continent"). Thus, a is an American in the continent sense, and a is an American in the U.S. sense.
Conversely, in Czech, there is no possibility for disambiguation. "Američan" (m.) and "američanka" (f.) can refer to persons from the United States or from the continents of the Americas, and there is no specific word capable of distinguishing the two meanings. For this reason, the latter meaning is very rarely used, and word is used almost exclusively to refer to persons from the United States. The usage is exactly parallel to the English word.
Korean and Vietnamese also use unambiguous terms, with Korean having () for the country versus () for the continents, and Vietnamese having for the country versus for the continents. Japanese has such terms as well ( [ versus []), but they are found more in newspaper headlines than in speech, where predominates.
In Swahili, means specifically the United States, and is a U.S. national, whereas the international form refers to the continents, and would be an inhabitant thereof. Likewise, the Esperanto word refers to the continents. For the country there is the term . Thus, a citizen of the United States is an , whereas an is an inhabitant of the Americas.
## History.
The name "America" was coined by Martin Waldseemüller from "Americus Vespucius", the Latinized version of the name of Amerigo Vespucci (1454–1512), the Italian explorer who mapped South America's east coast and the Caribbean Sea in the early 16th century. Later, Vespucci's published letters were the basis of Waldseemüller's 1507 map, which is the first usage of "America". The adjective "American" subsequently denoted the New World.
16th-century European usage of "American" denoted the native inhabitants of the New World. The earliest recorded use of this term in English is in Thomas Hacket's 1568 translation of André Thévet's book "France Antarctique"; Thévet himself had referred to the natives as "Ameriques". In the following century, the term was extended to European settlers and their descendants in the Americas. The earliest recorded use of "English-American" dates to 1648, in Thomas Gage's "The English-American his travail by sea and land: or, a new survey of the West India's".
In English, "American" was used especially for people in British America. Samuel Johnson, the leading English lexicographer, wrote in 1775, before the United States declared independence: "That the Americans are able to bear taxation is indubitable." The Declaration of Independence of July 1776 refers to "[the] unanimous Declaration of the thirteen United States of America" adopted by the "Representatives of the United States of America" on July 4, 1776. The official name of the country was reaffirmed on November 15, 1777, when the Second Continental Congress adopted the Articles of Confederation, the first of which says, "The Stile of this Confederacy shall be 'The United States of America'". The Articles further state:
Thomas Jefferson, newly elected president in May 1801 wrote, "I am sure the measures I mean to pursue are such as would in their nature be approved by every American who can emerge from preconceived prejudices; as for those who cannot, we must take care of them as of the sick in our hospitals. The medicine of time and fact may cure some of them."
In "The Federalist Papers" (1787–88), Alexander Hamilton and James Madison used the adjective "American" with two different meanings: one political and one geographic; "the American republic" in Federalist No. 51 and in Federalist No. 70, and, in Federalist No. 24, Hamilton used "American" to denote the lands beyond the U.S.'s political borders.
Early official U.S. documents show inconsistent usage; the 1778 Treaty of Alliance with France used "the United States of North America" in the first sentence, then "the said United States" afterwards; "the United States of America" and "the United States of North America" derive from "the United Colonies of America" and "the United Colonies of North America". The Treaty of Peace and Amity of September 5, 1795, between the United States and the Barbary States contains the usages "the United States of North America", "citizens of the United States", and "American Citizens".
U.S. President George Washington, in his 1796 "Farewell Address", declaimed that "The name of American, which belongs to you in your national capacity, must always exalt the just pride of patriotism more than any appellation." Political scientist Virginia L. Arbery notes that, in his "Farewell Address": "...Washington invites his fellow citizens to view themselves now as Americans who, out of their love for the truth of liberty, have replaced their maiden names (Virginians, South Carolinians, New Yorkers, etc.) with that of “American”. Get rid of, he urges, “any appellation derived from local discriminations.” By defining himself as an American rather than as a Virginian, Washington set the national standard for all citizens. "Over and over, Washington said that America must be something set apart. As he put it to Patrick Henry, 'In a word, I want an "American" character, that the powers of Europe may be convinced we act for "ourselves" and not for "others".'" As the historian Garry Wills has noted: "This was a theme dear to Washington. He wrote to Timothy Pickering that the nation 'must never forget that we are Americans; the remembrance of which will convince us we ought not to be French or English'." Washington's countrymen subsequently embraced his exhortation with notable enthusiasm.
This semantic divergence among North American anglophones, however, remained largely unknown in the Spanish-American colonies. In 1801, the document titled "Letter to American Spaniards"—published in French (1799), in Spanish (1801), and in English (1808)—might have influenced Venezuela's Act of Independence and its 1811 constitution.
The Latter-day Saints' Articles of Faith refer to the American continents as where they are to build Zion.
Common short forms and abbreviations are the "United States", the "U.S.", the "U.S.A.", and "America"; colloquial versions include the "U.S. of A." and "the States". The term "Columbia" (from the Columbus surname) was a popular name for the U.S. and for the entire geographic Americas; its usage is present today in the District of Columbia's name. Moreover, the womanly personification of Columbia appears in some official documents, including editions of the U.S. dollar.
## Usage at the United Nations.
Use of the term "American" for U.S. nationals is common at the United Nations, and financial markets in the United States are referred to as "American financial markets".
"American Samoa" is a recognized territorial name at the United Nations.
## Cultural views.
### Spain and Hispanic America.
The use of "American" as a national demonym for U.S. nationals is challenged, primarily by Hispanic Americans. Spanish speakers in Spain and Latin America use the term to refer to people and things from the United States (from ), while refers to the continents as a whole. The term is also accepted in many parts of Latin America to refer to a person or something from the United States; however, this term may be ambiguous in certain parts. Up to and including the 1992 edition, the , published by the Real Academia Española, did not include the United States definition in the entry for ; this was added in the 2001 edition. The Real Academia Española advised against using exclusively for U.S. nationals:
### Canada.
Modern Canadians typically refer to people from the United States as "Americans", though they seldom refer to the United States as "America"; they use the terms "the United States", "the U.S.", or (informally) "the States" instead. Rarely applying the term "American" to themselves, some Canadians resent being referred to as Americans or mistaken for U.S. citizens. This is often due to others' inability, particularly overseas, to distinguish Canadians from Americans, by their accent or other cultural attributes. Some Canadians have protested the use of "American" as a national demonym. People of U.S. ethnic origin in Canada are categorized as "Other North American origins" by Statistics Canada for purposes of census counts.
### Portugal and Brazil.
Generally, denotes "U.S. citizen" in Portugal. Usage of to exclusively denote people and things of the U.S. is discouraged by the Lisbon Academy of Sciences, because the specific word (also ) clearly denotes a person from the United States. The term currently used by the Portuguese press is .
In Brazil, the term is used to address both that which pertains to both American continents and, in current speech, that which pertains to the U.S.; the particular meaning is deduced from context. Alternatively, the term ("North American") is also used in more informal contexts, while (of the U.S.) is the preferred form in academia. Use of the three terms is common in schools, government, and media. The term is used almost exclusively for the whole continent, and the U.S. is called ("United States") or ("United States of America"), often abbreviated .
The Getting Through Customs website advises business travelers not to use "in America" as a U.S. reference when conducting business in Brazil.
## In other contexts.
"American" in the 1994 "Associated Press Stylebook" was defined as, "An acceptable description for a resident of the United States. It also may be applied to any resident or citizen of nations in North or South America." Elsewhere, the "AP Stylebook" indicates that "United States" must "be spelled out when used as a noun. Use U.S. (no space) only as an adjective."
The entry for "America" in "The New York Times Manual of Style and Usage" from 1999 reads:
Media releases from the Pope and Holy See frequently use "America" to refer to the United States, and "American" to denote something or someone from the United States.
### International law.
At least one international law uses "U.S. citizen" in defining a citizen of the United States rather than "American citizen"; for example, the English version of the North American Free Trade Agreement includes:
Many international treaties use the terms "American" and "American citizen":
### U.S. commercial regulation.
Products that are labeled, advertised, and marketed in the U.S. as "Made in the USA" must be, as set by the Federal Trade Commission (FTC), "all or virtually all made in the U.S." The FTC, to prevent deception of customers and unfair competition, considers an unqualified claim of "American Made" to expressly claim exclusive manufacture in the U.S: "The FTC Act gives the Commission the power to bring law enforcement actions against false or misleading claims that a product is of U.S. origin."
## Alternatives.
There are a number of alternatives to the demonym "American" as a citizen of the United States that do not simultaneously mean any inhabitant of the Americas. One uncommon alternative is "Usonian", which usually describes a certain style of residential architecture designed by Frank Lloyd Wright. Other alternatives have also surfaced, but most have fallen into disuse and obscurity. "Merriam-Webster's Dictionary of English Usage" says:
Nevertheless, no alternative to "American" is common.

</doc>
<doc id="1242" url="https://en.wikipedia.org/wiki?curid=1242" title="Ada (programming language)">
Ada (programming language)

Ada is a structured, statically typed, imperative, and object-oriented high-level programming language, extended from Pascal and other languages. It has built-in language support for "design by contract" (DbC), extremely strong typing, explicit concurrency, tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international technical standard, jointly defined by the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). , the standard, called Ada 2012 informally, is ISO/IEC 8652:2012.
Ada was originally designed by a team led by French computer scientist Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time. Ada was named after Ada Lovelace (1815–1852), who has been credited as the first computer programmer.
## Features.
Ada was originally designed for embedded and real-time systems. The Ada 95 revision, designed by S. Tucker Taft of Intermetrics between 1992 and 1995, improved support for systems, numerical, financial, and object-oriented programming (OOP).
Features of Ada include: strong typing, modular programming mechanisms (packages), run-time checking, parallel processing (tasks, synchronous message passing, protected objects, and nondeterministic select statements), exception handling, and generics. Ada 95 added support for object-oriented programming, including dynamic dispatch.
The syntax of Ada minimizes choices of ways to perform basic operations, and prefers English keywords (such as "or else" and "and then") to symbols (such as "||" and "&amp;&amp;"). Ada uses the basic arithmetical operators "+", "-", "*", and "/", but avoids using other symbols. Code blocks are delimited by words such as "declare", "begin", and "end", where the "end" (in most cases) is followed by the identifier of the block it closes (e.g., "if ... end if", "loop ... end loop"). In the case of conditional blocks this avoids a "dangling else" that could pair with the wrong nested if-expression in other languages like C or Java.
Ada is designed for developing very large software systems. Ada packages can be compiled separately. Ada package specifications (the package interface) can also be compiled separately without the implementation to check for consistency. This makes it possible to detect problems early during the design phase, before implementation starts.
A large number of compile-time checks are supported to help avoid bugs that would not be detectable until run-time in some other languages or would require explicit checks to be added to the source code. For example, the syntax requires explicitly named closing of blocks to prevent errors due to mismatched end tokens. The adherence to strong typing allows detecting many common software errors (wrong parameters, range violations, invalid references, mismatched types, etc.) either during compile-time, or otherwise during run-time. As concurrency is part of the language specification, the compiler can in some cases detect potential deadlocks. Compilers also commonly check for misspelled identifiers, visibility of packages, redundant declarations, etc. and can provide warnings and useful suggestions on how to fix the error.
Ada also supports run-time checks to protect against access to unallocated memory, buffer overflow errors, range violations, off-by-one errors, array access errors, and other detectable bugs. These checks can be disabled in the interest of runtime efficiency, but can often be compiled efficiently. It also includes facilities to help program verification. For these reasons, Ada is widely used in critical systems, where any anomaly might lead to very serious consequences, e.g., accidental death, injury or severe financial loss. Examples of systems where Ada is used include avionics, air traffic control, railways, banking, military and space technology.
Ada's dynamic memory management is high-level and type-safe. Ada has no generic or untyped pointers; nor does it implicitly declare any pointer type. Instead, all dynamic memory allocation and deallocation must occur via explicitly declared "access types". Each access type has an associated "storage pool" that handles the low-level details of memory management; the programmer can either use the default storage pool or define new ones (this is particularly relevant for Non-Uniform Memory Access). It is even possible to declare several different access types that all designate the same type but use different storage pools. Also, the language provides for "accessibility checks", both at compile time and at run time, that ensures that an "access value" cannot outlive the type of the object it points to.
Though the semantics of the language allow automatic garbage collection of inaccessible objects, most implementations do not support it by default, as it would cause unpredictable behaviour in real-time systems. Ada does support a limited form of region-based memory management; also, creative use of storage pools can provide for a limited form of automatic garbage collection, since destroying a storage pool also destroys all the objects in the pool.
A double-dash ("--"), resembling an em dash, denotes comment text. Comments stop at end of line, to prevent unclosed comments from accidentally voiding whole sections of source code. Disabling a whole block of code now requires the prefixing of each line (or column) individually with "--". While clearly denoting disabled code with a column of repeated "--" down the page this renders the experimental dis/re-enablement of large blocks a more drawn out process.
The semicolon (";") is a statement terminator, and the null or no-operation statement is codice_1. A single codice_2 without a statement to terminate is not allowed.
Unlike most ISO standards, the Ada language definition (known as the "Ada Reference Manual" or "ARM", or sometimes the "Language Reference Manual" or "LRM") is free content. Thus, it is a common reference for Ada programmers, not only programmers implementing Ada compilers. Apart from the reference manual, there is also an extensive rationale document which explains the language design and the use of various language constructs. This document is also widely used by programmers. When the language was revised, a new rationale document was written.
One notable free software tool that is used by many Ada programmers to aid them in writing Ada source code is the GNAT Programming Studio, part of the GNU Compiler Collection.
## History.
In the 1970s the US Department of Defense (DoD) became concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and the UK Ministry of Defence's requirements. After many iterations beginning with an original Straw man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.
The HOLWG working group crafted the Steelman language requirements, a series of documents stating the requirements they felt a programming language should satisfy. Many existing languages were formally reviewed, but the team concluded in 1977 that no existing language met the specifications.
Requests for proposals for a new programming language were issued and four contractors were hired to develop their proposals under the names of Red (Intermetrics led by Benjamin Brosgol), Green (CII Honeywell Bull, led by Jean Ichbiah), Blue (SofTech, led by John Goodenough) and Yellow (SRI International, led by Jay Spitzen). In April 1978, after public scrutiny, the Red and Green proposals passed to the next phase. In May 1979, the Green proposal, designed by Jean Ichbiah at CII Honeywell Bull, was chosen and given the name Ada—after Augusta Ada, Countess of Lovelace. This proposal was influenced by the language LIS that Ichbiah and his group had developed in the 1970s. The preliminary Ada reference manual was published in ACM SIGPLAN Notices in June 1979. The Military Standard reference manual was approved on December 10, 1980 (Ada Lovelace's birthday), and given the number MIL-STD-1815 in honor of Ada Lovelace's birth year. In 1981, C. A. R. Hoare took advantage of his Turing Award speech to criticize Ada for being overly complex and hence unreliable, but subsequently seemed to recant in the foreword he wrote for an Ada textbook.
Ada attracted much attention from the programming community as a whole during its early days. Its backers and others predicted that it might become a dominant language for general purpose programming and not only defense-related work. Ichbiah publicly stated that within ten years, only two programming languages would remain: Ada and Lisp. Early Ada compilers struggled to implement the large, complex language, and both compile-time and run-time performance tended to be slow and tools primitive. Compiler vendors expended most of their efforts in passing the massive, language-conformance-testing, government-required "ACVC" validation suite that was required in another novel feature of the Ada language effort. The Jargon File, a dictionary of computer hacker slang originating in 1975–1983, notes in an entry on Ada that "it is precisely what one might expect given that kind of endorsement by fiat; designed by committee...difficult to use, and overall a disastrous, multi-billion-dollar boondoggle...Ada Lovelace...would almost certainly blanch at the use her name has been latterly put to; the kindest thing that has been said about it is that there is probably a good small language screaming to get out from inside its vast, elephantine bulk."
The first validated Ada implementation was the NYU Ada/Ed translator, certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL. Several commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, TLD Systems, and Verdix.
In 1991, the US Department of Defense began to require the use of Ada (the "Ada mandate") for all software, though exceptions to this rule were often granted. The Department of Defense Ada mandate was effectively removed in 1997, as the DoD began to embrace commercial off-the-shelf (COTS) technology. Similar requirements existed in other NATO countries: Ada was required for NATO systems involving command and control and other functions, and Ada was the mandated or preferred language for defense-related applications in countries such as Sweden, Germany, and Canada.
By the late 1980s and early 1990s, Ada compilers had improved in performance, but there were still barriers to fully exploiting Ada's abilities, including a tasking model that was different from what most real-time programmers were used to.
Because of Ada's safety-critical support features, it is now used not only for military applications, but also in commercial projects where a software bug can have severe consequences, e.g., avionics and air traffic control, commercial rockets such as the Ariane 4 and 5, satellites and other space systems, railway transport and banking.
For example, the Airplane Information Management System, the fly-by-wire system software in the Boeing 777, was written in Ada. Developed by Honeywell Air Transport Systems in collaboration with consultants from DDC-I, it became arguably the best-known of any Ada project, civilian or military. The Canadian Automated Air Traffic System was written in 1 million lines of Ada (SLOC count). It featured advanced distributed processing, a distributed Ada database, and object-oriented design. Ada is also used in other air traffic systems, e.g., the UK's next-generation Interim Future Area Control Tools Support (iFACTS) air traffic control system is designed and implemented using SPARK Ada.
It is also used in the French TVM in-cab signalling system on the TGV high-speed rail system, and the metro suburban trains in Paris, London, Hong Kong and New York City.
## Standardization.
The language became an ANSI standard in 1983 (ANSI/MIL-STD 1815A), and after translation in French and without any further changes in English became
an ISO standard in 1987 (ISO-8652:1987). This version of the language is commonly known as Ada 83, from the date of its adoption by ANSI, but is sometimes referred to also as Ada 87, from the date of its adoption by ISO.
Ada 95, the joint ISO/ANSI standard (ISO-8652:1995) was published in February 1995, making Ada 95 the first ISO standard object-oriented programming language. To help with the standard revision and future acceptance, the US Air Force funded the development of the GNAT Compiler. Presently, the GNAT Compiler is part of the GNU Compiler Collection.
Work has continued on improving and updating the technical content of the Ada language. A Technical Corrigendum to Ada 95 was published in October 2001, and a major Amendment, ISO/IEC 8652:1995/Amd 1:2007 was published on March 9, 2007. At the Ada-Europe 2012 conference in Stockholm, the Ada Resource Association (ARA) and Ada-Europe announced the completion of the design of the latest version of the Ada language and the submission of the reference manual to the International Organization for Standardization (ISO) for approval. ISO/IEC 8652:2012 was published in December 2012.
Other related standards include ISO 8651-3:1988 "Information processing systems—Computer graphics—Graphical Kernel System (GKS) language bindings—Part 3: Ada".
## Language constructs.
Ada is an ALGOL-like programming language featuring control structures with reserved words such as "if", "then", "else", "while", "for", and so on. However, Ada also has many data structuring facilities and other abstractions which were not included in the original ALGOL 60, such as type definitions, records, pointers, enumerations. Such constructs were in part inherited from or inspired by Pascal.
### "Hello, world!" in Ada.
A common example of a language's syntax is the Hello world program:
with Ada.Text_IO;
use Ada.Text_IO;
procedure Hello is
begin
 Put_Line ("Hello, world!");
end Hello;
This program can be compiled by using the freely available open source compiler GNAT, by executing
gnatmake hello.adb
### Data types.
Ada's type system is not based on a set of predefined primitive types but allows users to declare their own types. This declaration in turn is not based on the internal representation of the type but on describing the goal which should be achieved. This allows the compiler to determine a suitable memory size for the type, and to check for violations of the type definition at compile time and run time (i.e., range violations, buffer overruns, type consistency, etc.). Ada supports numerical types defined by a range, modulo types, aggregate types (records and arrays), and enumeration types. Access types define a reference to an instance of a specified type; untyped pointers are not permitted.
Special types provided by the language are task types and protected types.
For example, a date might be represented as:
type Day_type is range 1 .. 31;
type Month_type is range 1 .. 12;
type Year_type is range 1800 .. 2100;
type Hours is mod 24;
type Weekday is (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday);
type Date is
 record
 Day : Day_type;
 Month : Month_type;
 Year : Year_type;
 end record;
Types can be refined by declaring subtypes:
subtype Working_Hours is Hours range 0 .. 12; -- at most 12 Hours to work a day
subtype Working_Day is Weekday range Monday .. Friday; -- Days to work
Work_Load: constant array(Working_Day) of Working_Hours -- implicit type declaration
 := (Friday =&gt; 6, Monday =&gt; 4, others =&gt; 10); -- lookup table for working hours with initialization
Types can have modifiers such as "limited, abstract, private" etc. Private types can only be accessed and limited types can only be modified or copied within the scope of the package that defines them. Ada 95 adds further features for object-oriented extension of types.
### Control structures.
Ada is a structured programming language, meaning that the flow of control is structured into standard statements. All standard constructs and deep-level early exit are supported, so the use of the also supported "go to" commands is seldom needed.
-- while a is not equal to b, loop.
while a /= b loop
 Ada.Text_IO.Put_Line ("Waiting");
end loop;
if a &gt; b then
 Ada.Text_IO.Put_Line ("Condition met");
else
 Ada.Text_IO.Put_Line ("Condition not met");
end if;
for i in 1 .. 10 loop
 Ada.Text_IO.Put ("Iteration: ");
 Ada.Text_IO.Put (i);
 Ada.Text_IO.Put_Line;
end loop;
loop
 a := a + 1;
 exit when a = 10;
end loop;
case i is
 when 0 =&gt; Ada.Text_IO.Put ("zero");
 when 1 =&gt; Ada.Text_IO.Put ("one");
 when 2 =&gt; Ada.Text_IO.Put ("two");
 -- case statements have to cover all possible cases:
 when others =&gt; Ada.Text_IO.Put ("none of the above");
end case;
for aWeekday in Weekday'Range loop -- loop over an enumeration
 Put_Line ( Weekday'Image(aWeekday) ); -- output string representation of an enumeration
 if aWeekday in Working_Day then -- check of a subtype of an enumeration
 Put_Line ( " to work for " &amp;
 Working_Hours'Image (Work_Load(aWeekday)) ); -- access into a lookup table
 end if;
end loop;
### Packages, procedures and functions.
Among the parts of an Ada program are packages, procedures and functions.
Example:
Package specification (example.ads)
package Example is
 type Number is range 1 .. 11;
 procedure Print_and_Increment (j: in out Number);
end Example;
Package body (example.adb)
with Ada.Text_IO;
package body Example is
 i : Number := Number'First;
 procedure Print_and_Increment (j: in out Number) is
 function Next (k: in Number) return Number is
 begin
 return k + 1;
 end Next;
 begin
 Ada.Text_IO.Put_Line ( "The total is: " &amp; Number'Image(j) );
 j := Next (j);
 end Print_and_Increment;
-- package initialization executed when the package is elaborated
begin
 while i &lt; Number'Last loop
 Print_and_Increment (i);
 end loop;
end Example;
This program can be compiled, e.g., by using the freely available open-source compiler GNAT, by executing
gnatmake -z example.adb
Packages, procedures and functions can nest to any depth, and each can also be the logical outermost block.
Each package, procedure or function can have its own declarations of constants, types, variables, and other procedures, functions and packages, which can be declared in any order.
### Concurrency.
Ada has language support for task-based concurrency. The fundamental concurrent unit in Ada is a "task", which is a built-in limited type. Tasks are specified in two parts – the task declaration defines the task interface (similar to a type declaration), the task body specifies the implementation of the task. Depending on the implementation, Ada tasks are either mapped to operating system threads or processes, or are scheduled internally by the Ada runtime.
Tasks can have entries for synchronisation (a form of synchronous message passing). Task entries are declared in the task specification. Each task entry can have one or more "accept" statements within the task body. If the control flow of the task reaches an accept statement, the task is blocked until the corresponding entry is called by another task (similarly, a calling task is blocked until the called task reaches the corresponding accept statement). Task entries can have parameters similar to procedures, allowing tasks to synchronously exchange data. In conjunction with "select" statements it is possible to define "guards" on accept statements (similar to Dijkstra's guarded commands).
Ada also offers "protected objects" for mutual exclusion. Protected objects are a monitor-like construct, but use guards instead of conditional variables for signaling (similar to conditional critical regions). Protected objects combine the data encapsulation and safe mutual exclusion from monitors, and entry guards from conditional critical regions. The main advantage over classical monitors is that conditional variables are not required for signaling, avoiding potential deadlocks due to incorrect locking semantics. Like tasks, the protected object is a built-in limited type, and it also has a declaration part and a body.
A protected object consists of encapsulated private data (which can only be accessed from within the protected object), and procedures, functions and entries which are guaranteed to be mutually exclusive (with the only exception of functions, which are required to be side effect free and can therefore run concurrently with other functions). A task calling a protected object is blocked if another task is currently executing inside the same protected object, and released when this other task leaves the protected object. Blocked tasks are queued on the protected object ordered by time of arrival.
Protected object entries are similar to procedures, but additionally have "guards". If a guard evaluates to false, a calling task is blocked and added to the queue of that entry; now another task can be admitted to the protected object, as no task is currently executing inside the protected object. Guards are re-evaluated whenever a task leaves the protected object, as this is the only time when the evaluation of guards can have changed.
Calls to entries can be "requeued" to other entries with the same signature. A task that is requeued is blocked and added to the queue of the target entry; this means that the protected object is released and allows admission of another task.
The "select" statement in Ada can be used to implement non-blocking entry calls and accepts, non-deterministic selection of entries (also with guards), time-outs and aborts.
The following example illustrates some concepts of concurrent programming in Ada.
with Ada.Text_IO; use Ada.Text_IO;
procedure Traffic is
 type Airplane_ID is range 1..10; -- 10 airplanes
 task type Airplane (ID: Airplane_ID); -- task representing airplanes, with ID as initialisation parameter
 type Airplane_Access is access Airplane; -- reference type to Airplane
 protected type Runway is -- the shared runway (protected to allow concurrent access)
 entry Assign_Aircraft (ID: Airplane_ID); -- all entries are guaranteed mutually exclusive
 entry Cleared_Runway (ID: Airplane_ID);
 entry Wait_For_Clear;
 private
 Clear: Boolean := True; -- protected private data - generally more than only a flag...
 end Runway;
 type Runway_Access is access all Runway;
 -- the air traffic controller task takes requests for takeoff and landing
 task type Controller (My_Runway: Runway_Access) is
 -- task entries for synchronous message passing
 entry Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access);
 entry Request_Approach(ID: in Airplane_ID; Approach: out Runway_Access);
 end Controller;
 -- allocation of instances
 Runway1 : aliased Runway; -- instantiate a runway
 Controller1: Controller (Runway1'Access); -- and a controller to manage it
 ------ the implementations of the above types ------
 protected body Runway is
 entry Assign_Aircraft (ID: Airplane_ID)
 when Clear is -- the entry guard - calling tasks are blocked until the condition is true
 begin
 Clear := False;
 Put_Line (Airplane_ID'Image (ID) &amp; " on runway ");
 end;
 entry Cleared_Runway (ID: Airplane_ID)
 when not Clear is
 begin
 Clear := True;
 Put_Line (Airplane_ID'Image (ID) &amp; " cleared runway ");
 end;
 entry Wait_For_Clear
 when Clear is
 begin
 null; -- no need to do anything here - a task can only enter if "Clear" is true
 end;
 end Runway;
 task body Controller is
 begin
 loop
 My_Runway.Wait_For_Clear; -- wait until runway is available (blocking call)
 select -- wait for two types of requests (whichever is runnable first)
 when Request_Approach'count = 0 =&gt; -- guard statement - only accept if there are no tasks queuing on Request_Approach
 accept Request_Takeoff (ID: in Airplane_ID; Takeoff: out Runway_Access)
 do -- start of synchronized part
 My_Runway.Assign_Aircraft (ID); -- reserve runway (potentially blocking call if protected object busy or entry guard false)
 Takeoff := My_Runway; -- assign "out" parameter value to tell airplane which runway
 end Request_Takeoff; -- end of the synchronised part
 or
 accept Request_Approach (ID: in Airplane_ID; Approach: out Runway_Access) do
 My_Runway.Assign_Aircraft (ID);
 Approach := My_Runway;
 end Request_Approach;
 or -- terminate if no tasks left who could call
 terminate;
 end select;
 end loop;
 end;
 task body Airplane is
 Rwy : Runway_Access;
 begin
 Controller1.Request_Takeoff (ID, Rwy); -- This call blocks until Controller task accepts and completes the accept block
 Put_Line (Airplane_ID'Image (ID) &amp; " taking off...");
 delay 2.0;
 Rwy.Cleared_Runway (ID); -- call will not block as "Clear" in Rwy is now false and no other tasks should be inside protected object
 delay 5.0; -- fly around a bit...
 loop
 select -- try to request a runway
 Controller1.Request_Approach (ID, Rwy); -- this is a blocking call - will run on controller reaching accept block and return on completion
 exit; -- if call returned we're clear for landing - leave select block and proceed...
 or
 delay 3.0; -- timeout - if no answer in 3 seconds, do something else (everything in following block)
 Put_Line (Airplane_ID'Image (ID) &amp; " in holding pattern"); -- simply print a message
 end select;
 end loop;
 delay 4.0; -- do landing approach...
 Put_Line (Airplane_ID'Image (ID) &amp; " touched down!");
 Rwy.Cleared_Runway (ID); -- notify runway that we're done here.
 end;
 New_Airplane: Airplane_Access;
begin
 for I in Airplane_ID'Range loop -- create a few airplane tasks
 New_Airplane := new Airplane (I); -- will start running directly after creation
 delay 4.0;
 end loop;
end Traffic;
### Pragmas.
A pragma is a compiler directive that conveys information to the compiler to allow specific manipulating of compiled output. Certain pragmas are built into the language, while others are implementation-specific.
Examples of common usage of compiler pragmas would be to disable certain features, such as run-time type checking or array subscript boundary checking, or to instruct the compiler to insert object code instead of a function call (as C/C++ does with inline functions).
## References.
### Rationale.
These documents have been published in various forms, including print.

</doc>
<doc id="1245" url="https://en.wikipedia.org/wiki?curid=1245" title="Alpha ray">
Alpha ray



</doc>
<doc id="1246" url="https://en.wikipedia.org/wiki?curid=1246" title="Alfonso Aráu">
Alfonso Aráu



</doc>
<doc id="1247" url="https://en.wikipedia.org/wiki?curid=1247" title="Alfonso Cuarón">
Alfonso Cuarón

Alfonso Cuarón Orozco ( , ; born 28 November 1961) is a Mexican film director, film producer, screenwriter, cinematographer and film editor. Cuarón is the first Mexico-born filmmaker to win the Academy Award for Best Director. He has been nominated for Academy Awards in six different categories, a record he shares with Walt Disney and George Clooney.
Cuarón has received 10 Academy Award nominations, winning four including Best Director for "Gravity" (2013) and "Roma" (2018), Best Film Editing for "Gravity", and Best Cinematography for "Roma". His other notable films from a variety of genres include the family drama "A Little Princess" (1995), the romantic drama "Great Expectations" (1998), the coming of age road film "Y tu mamá también" (2001), the fantasy film "Harry Potter and the Prisoner of Azkaban" (2004), and the science fiction dystopian thriller "Children of Men" (2006).
## Early life.
Alfonso Cuarón Orozco was born in Mexico City, the son of Alfredo Cuarón, a doctor specializing in nuclear medicine, and Cristina Orozco, a pharmaceutical biochemist. He has two brothers, Carlos, also a filmmaker, and Alfredo, a conservation biologist. Cuarón studied philosophy at the National Autonomous University of Mexico (UNAM) and filmmaking at CUEC (Centro Universitario de Estudios Cinematográficos), a school within the same university. There he met the director Carlos Marcovich and cinematographer Emmanuel Lubezki, and they made what would be his first short film, "Vengeance Is Mine".
## Career.
### 1990s: Early career.
Cuarón began working on television in Mexico, first as a technician and then as a director. His television work led to assignments as an assistant director for several film productions including "La Gran Fiesta", "" and "Romero", and in 1991 he landed his first big-screen directorial assignment.
In 1991, Cuarón directed "Sólo con tu pareja", a sex comedy about a womanizing businessman (played by Daniel Giménez Cacho) who, after having sex with an attractive nurse, is fooled into believing he's contracted AIDS. In addition to writing, producing and directing, Cuarón co-edited the film with Luis Patlán. The film, which also starred cabaret singer Astrid Hadad and model/actress Claudia Ramírez (with whom Cuarón was linked between 1989 and 1993) was a big hit in Mexico. After this success, director Sydney Pollack hired Cuarón to direct an episode of "Fallen Angels", a series of neo-noir stories produced for the Showtime premium cable network in 1993; other directors who worked on the series included Steven Soderbergh, Jonathan Kaplan, Peter Bogdanovich, and Tom Hanks.
In 1995, Cuarón released his first feature film produced in the United States, "A Little Princess", an adaptation of Frances Hodgson Burnett's classic novel. Cuarón's next feature was also a literary adaptation, a modernized version of Charles Dickens's "Great Expectations" starring Ethan Hawke, Gwyneth Paltrow, and Robert De Niro.
### 2000s: International success.
In 2001, Cuarón found himself returning to Mexico with a Spanish-speaking cast to film "Y tu mamá también", starring Gael García Bernal, Diego Luna and Maribel Verdú. It was a provocative and controversial road comedy about two sexually obsessed teenagers who take an extended road trip with an attractive married woman who is much older than them. The film's open portrayal of sexuality and frequent rude humor, as well as the politically and socially relevant asides, made the film an international hit and a major success with critics. Cuarón shared an Academy Award nomination for Best Original Screenplay with co-writer and brother Carlos Cuarón.
In 2004, Cuarón directed the third film in the successful "Harry Potter" series, "Harry Potter and the Prisoner of Azkaban". Cuarón faced criticism at the time from some "Harry Potter" fans for his approach to the film, notably its tendency to take more creative liberties with the source material than its predecessors. However, author J. K. Rowling, who had seen and loved Cuarón's film "Y tu mamá también", said that it was her personal favorite from the series so far. Critically, the film was also better received than the first two installments, with some critics remarking its new tone and for being the first "Harry Potter" film to truly capture the essence of the novels. It has been subsequently rated by audience polls and critics as the best of the movie franchise series.
In 2006, Cuarón's feature "Children of Men", an adaptation of the P. D. James novel starring Clive Owen, Julianne Moore, and Michael Caine, received wide critical acclaim including three Academy Award nominations. Cuarón himself received two nominations for his work on the film in Best Film Editing (with Alex Rodríguez) and Best Adapted Screenplay (with several collaborators).
He created the production and distribution company Esperanto Filmoj ("Esperanto Films", named because of his support for the international language Esperanto), which has credits in the films "Duck Season", "Pan's Labyrinth", and "Gravity".
Cuarón also directed the controversial public service announcement "I Am Autism" for Autism Speaks that was criticized by disability rights groups for its negative portrayal of autism.
### 2010s: Awards success.
In 2010, Cuarón began to develop the film "Gravity", a drama set in space. He was joined by producer David Heyman, with whom Cuarón worked on "Harry Potter and the Prisoner of Azkaban". Starring Sandra Bullock and George Clooney, the film opened the 70th Venice International Film Festival in August. The film was then released in America in October 2013 The film became financial success earning 723.2 million of the box office against a budget of 130 million. The film also received many awards nominations. For the film, he received the Golden Globe Award in the category of Best Director. The film received ten Academy Award nominations, including Best Picture and Best Director. Cuarón won for Best Directing, becoming the first Latin American to win the award, while he and Mark Sanger received the award for Best Film Editing.
In 2013, Cuarón created "Believe", a science fiction/fantasy/adventure series that was broadcast as part of the 2013–14 United States network television schedule on NBC as a mid-season entry. The series was created by Cuarón for Bad Robot Productions and Warner Bros. Television. In 2014, "Time" placed him in its list of "100 Most Influential People in the World" – Pioneers.
In May 2015, Cuarón was announced as the President of the Jury for the 72nd Venice International Film Festival.
Production began in fall 2016 for Cuarón's eighth film, "Roma", a tale of a housekeeper for a middle class Mexican family in 1970s Mexico City, based on the life of his family's longtime maid, Liboria Rodríguez. The project was produced by Cuarón, Gabriela Rodríguez and Nicolás Celis and starred Yalitza Aparicio and Marina de Tavira both of whom received Oscar nominations. The film debuted at 75th Venice International Film Festival, where it won the Golden Lion, and was distributed to select Mexican and American theaters before its online release on Netflix. "Roma" was highly acclaimed upon release; among its accolades are two Golden Globes (Best Foreign Language Film and Best Director for Cuarón) and three Academy Awards (Best Director, Best Foreign Language Film, and Best Cinematography for Cuarón) out of a leading ten nominations. In 2019, Cuaron signed an overall TV deal at Apple. His first TV show under an overall deal with Apple was the show "Disclaimer", which was to star Cate Blanchett and Kevin Kline.
## Style.
Cuarón's films seem to seep with his many signature visual, thematic, and structural elements. Most notable is the director's use of long takes and his constantly moving camera. These tendencies create the feeling of real time and real space within the worlds that Cuarón explores in his films. Elaborating on this the director states, "For "Children of Men", we wanted to take advantage of the element of real time. It's a documentary approach. As if you were just following characters around with your own digital camera in the year 2027." This documentary approach grounds the sometimes fantastical and otherworldly settings the director traverses in films such as "Harry Potter and the Prisoner of Azkaban" and "Gravity". In his films, camera movement acts as an extension of character emotion. Whether employing handheld, steadicam or robotic arm, Cuarón uses the tools of cinematography to create an intense, symbiotic relationship between viewer and onscreen action.
## Personal life.
Cuarón is a vegetarian and has been living in London since 2000.
Cuarón's first marriage was to Mariana Elizondo with whom he has a son, Jonás Cuarón, born in 1981. Jonás Cuarón is also a film director, known for "Year of the Nail" and "Desierto". Alfonso Cuarón's second marriage, from 2001 to 2008 was to Italian actress and freelance journalist Annalisa Bugliani, with whom he has two children.
He has publicly shown his fascination for the Esperanto language and his support for the Esperanto movement. He called his production company Esperanto Filmoj.
In 2009, Cuarón signed a petition which called for the release of film director Roman Polanski, who was arrested in Switzerland in relation to his 1977 charge for drugging and raping a 13-year-old girl.

</doc>
<doc id="1252" url="https://en.wikipedia.org/wiki?curid=1252" title="Arianism">
Arianism

Arianism (, "Areianismós") is a Christological doctrine first attributed to Arius (c. AD 256–336), a Christian presbyter from Alexandria, Egypt. Arian theology holds that Jesus Christ is the Son of God, who was begotten by God the Father with the difference that the Son of God did not always exist but was begotten within time by God the Father, therefore Jesus was not co-eternal with God the Father. Arianism holds that the Son is distinct from the Father and therefore subordinate to Him. The term "Arian" is derived from the name Arius; it was not what the followers of Arius's teachings called themselves, but rather a term used by outsiders. The nature of Arius's teachings and his supporters were opposed to the theological doctrines held by Homoousian Christians, regarding the nature of the Trinity and the nature of Christ. 
There was a controversy between two interpretations of Jesus' divinity (Homoousianism and Arianism) based upon the theological orthodoxy of the time, one Trinitarian and the other also a derivative of Trinitarian orthodoxy, and both of them attempted to solve its respective theological dilemmas. Homoousianism was formally affirmed by the first two ecumenical councils; since then, Arianism has always been condemned as "the heresy or sect of Arius". As such, all mainstream branches of Christianity now consider Arianism to be heterodox and heretical. Trinitarian ("homoousian") doctrines were vigorously upheld by Patriarch Athanasius of Alexandria, who insisted that Jesus (God the Son) was "same in being" or "same in essence" with God the Father. Arius stated: "If the Father begat the Son, then he who was begotten had a beginning in existence, and from this it follows there was a time when the Son was not." The ecumenical First Council of Nicaea of 325, convened by Emperor Constantine to ensure church unity, declared Arianism to be a heresy. According to Everett Ferguson, "The great majority of Christians had no clear views about the nature of the Trinity and they did not understand what was at stake in the issues that surrounded it."
Arianism is also used to refer to other nontrinitarian theological systems of the 4th century, which regarded Jesus Christ—the Son of God, the Logos—as either a begotten creature of a similar or different substance to that of the Father, but not identical (as Homoiousian and Anomoeanism) or as neither uncreated nor created in the sense other beings are created (as in semi-Arianism).
## Origin.
Controversy over Arianism arose in the late 3rd century and persisted throughout most of the 4th century. It involved most church members—from simple believers, priests, and monks to bishops, emperors, and members of Rome's imperial family. Two Roman emperors, Constantius II and Valens, became Arians or Semi-Arians, as did prominent Gothic, Vandal, and Lombard warlords both before and after the fall of the Western Roman Empire. Such a deep controversy within the early Church during this period of its development could not have materialized without significant historical influences providing a basis for the Arian doctrines.
Arius had been a pupil of Lucian of Antioch at Lucian's private academy in Antioch and inherited from him a modified form of the teachings of Paul of Samosata. Arius taught that God the Father and the Son of God did not always exist together eternally.
## Condemnation by the Council of Nicaea.
Emperor Constantine the Great summoned the First Council of Nicaea, which defined the dogmatic fundaments of the Christian religion; these definitions served to rebut the questions posed by Arians. All the bishops who were there were in agreement with the major theological points of the proto-orthodoxy, since at that time all other forms of Christianity "had by this time already been displaced, suppressed, reformed, or destroyed". Although the proto-orthodox won the previous disputes, due to the more accurate defining of orthodoxy, they were vanquished with their own weapons, ultimately being declared heretics, not because they would have fought against ideas regarded as theologically correct, but because their positions lacked the accuracy and refinement needed by the fusion of several contradictory theses accepted at the same time by later orthodox theologians. According to Bart Ehrman that is why the Trinity is a "paradoxical affirmation".
Of the roughly three hundred bishops in attendance at the Council of Nicaea, two bishops did not sign the Nicene Creed that condemned Arianism. Constantine the Great also ordered a penalty of death for those who refused to surrender the Arian writings:
Ten years after the Council of Nicea, Constantine the Great, who was himself later baptized by the Arian bishop Eusebius of Nicomedia in 337 AD, convened another gathering of church leaders at the regional First Synod of Tyre in 335 (attended by 310 bishops), to address various charges mounted against Athanasius by his detractors, such as "murder, illegal taxation, sorcery, and treason", following his refusal to readmit Arius into fellowship. Athanasius was exiled to Trier (in modern Germany) following his conviction at Tyre of conspiracy, and Arius was, effectively, exonerated. Athanasius eventually returned to Alexandria in 346, after the deaths of both Arius and Constantine. Though Arianism had spread, Athanasius and other Nicene Christian church leaders crusaded against Arian theology, and Arius was anathemised and condemned as a heretic once more at the ecumenical First Council of Constantinople of 381 (attended by 150 bishops). The Roman Emperors Constantius II (337–361) and Valens (364–378) were Arians or Semi-Arians, as was the first King of Italy, Odoacer (433?–493), and the Lombards were also Arians or Semi-Arians until the 7th century. Visigothic Spain was Arian until 589. Many Goths adopted Arian beliefs upon their conversion to Christianity. The Vandals actively spread Arianism in North Africa.
## Beliefs.
Reconstructing what Arius actually taught, and why, is a formidable task, both because little of his own work survives except in quotations selected for polemical purposes by his opponents, and also because there is no certainty about what theological and philosophical traditions formed his thought.
Arianism taught that the Logos was a divine being begotten by God the Father before the creation of the world, made him a medium through whom everything else was created, and that the Son of God is subordinate to God the Father. A verse from Proverbs was also used: "The Lord created me at the beginning of his work." Therefore, the Son was rather the very first and the most perfect of God's creatures, and he was made "God" only by the Father's permission and power.
Arians do not believe in the traditional doctrine of the Trinity. The letter of Arian Auxentius regarding the Arian missionary Ulfilas gives a picture of Arian beliefs. Arian Ulfilas, who was ordained a bishop by Arian Eusebius of Nicomedia and returned to his people to work as a missionary, believed: God, the Father, ("unbegotten" God; Almighty God) always existing and who is the only true God. The Son of God, Jesus Christ, ("only-begotten God"), Mighty God; begotten before time began, , and who is Lord/Master. The Holy Spirit (the illuminating and sanctifying power, who is neither God the Father nor Lord/Master. 1 Corinthians 8:5–6 was cited as proof text:
The creed of Arian Ulfilas (c. 311–383), which concludes a letter praising him written by Auxentius, distinguishes God the Father ("unbegotten"), who is the only true God from Son of God ("only-begotten"), who is Lord/Master; and the Holy Spirit, the illuminating and sanctifying power, who is neither God the Father nor Lord/Master:
A letter from Arius (c. 250–336) to the Arian Eusebius of Nicomedia (died 341) succinctly states the core beliefs of the Arians:
Principally, the dispute between Trinitarianism and Arianism was about:
For the theologians of the 19th century it was already obvious that in fact Arius and Alexander/Athanasius did not have much to quarrel about, the difference between their views was very small, and that the end of the fight was by no means clear during their quarrel, both Arius and Athanasius suffering a great deal for their own views. Arius was the father of Homoiousianism and Alexander the father of Homoousianism, which was championed by Athanasius. For those theologians it was clear that Arius, Alexander and Athanasius were rather far from a true doctrine of Trinity, which developed later, historically speaking.
Berndt and Steinacher state quite clearly that the beliefs of Arius were acceptable ("not especially unusual") to a huge number of orthodox clergy; this is the reason why such a major conflict was able to develop inside the Church, since Arius's theology enjoyed widespread sympathy (or at least was not considered to be overly controversial) and could not simply be dismissed outright as individual heresy.
## Homoian Arianism.
Arianism had several different variants, including Eunomianism and Homoian Arianism. Homoian Arianism is associated with Akakius and Eudoxius. Homoian Arianism avoided the use of the word "ousia" to describe the relation of Father to Son, and described these as "like" each other. Hanson lists twelve creeds that reflect the Homoian faith:
## Struggles with orthodoxy.
### First Council of Nicaea.
In 321, Arius was denounced by a synod at Alexandria for teaching a heterodox view of the relationship of Jesus to God the Father. Because Arius and his followers had great influence in the schools of Alexandria—counterparts to modern universities or seminaries—their theological views spread, especially in the eastern Mediterranean.
By 325, the controversy had become significant enough that the Emperor Constantine called an assembly of bishops, the First Council of Nicaea, which condemned Arius's doctrine and formulated the original Nicene Creed of 325. The Nicene Creed's central term, used to describe the relationship between the Father and the Son, is Homoousios (), or Consubstantiality, meaning "of the same substance" or "of one being" (the Athanasian Creed is less often used but is a more overtly anti-Arian statement on the Trinity).
The focus of the Council of Nicaea was the nature of the Son of God and his precise relationship to God the Father (see Paul of Samosata and the Synods of Antioch). Arius taught that Jesus Christ was divine/holy and was sent to earth for the salvation of mankind but that Jesus Christ was not equal to God the Father (infinite, primordial origin) in rank "and" that God the Father and the Son of God were not equal to the Holy Spirit. Under Arianism, Christ was instead not consubstantial with God the Father since both the Father and the Son under Arius were made of "like" essence or being (see homoiousia) but not of the same essence or being (see homoousia).
In the Arian view, God the Father is a deity and is divine "and" the Son of God is not a deity but divine (I, the LORD, am Deity alone.) God the Father sent Jesus to earth for salvation of mankind. Ousia is essence or being, in Eastern Christianity, and is the aspect of God that is completely incomprehensible to mankind and human perception. It is all that subsists by itself and which has not its being in another, God the Father and God the Son and God the Holy Spirit all being uncreated.
According to the teaching of Arius, the preexistent Logos and thus the incarnate Jesus Christ was a begotten being; only the Son was directly begotten by God the Father, before ages, but was of a distinct, though similar, essence or substance from the Creator. His opponents argued that this would make Jesus less than God and that this was heretical. Much of the distinction between the differing factions was over the phrasing that Christ expressed in the New Testament to express submission to God the Father. The theological term for this submission is kenosis. This ecumenical council declared that Jesus Christ was true God, co-eternal and consubstantial (i.e., of the same substance) with God the Father.
Constantine is believed to have exiled those who refused to accept the Nicean Creed—Arius himself, the deacon Euzoios, and the Libyan bishops Theonas of Marmarica and Secundus of Ptolemais—and also the bishops who signed the creed but refused to join in condemnation of Arius, Eusebius of Nicomedia and Theognis of Nicaea. The emperor also ordered all copies of the "Thalia", the book in which Arius had expressed his teachings, to be burned. However, there is no evidence that his son and ultimate successor, Constantius II, who was a Semi-Arian Christian, was exiled.
Although he was committed to maintaining what the Great Church had defined at Nicaea, Constantine was also bent on pacifying the situation and eventually became more lenient toward those condemned and exiled at the council. First, he allowed Eusebius of Nicomedia, who was a protégé of his sister, and Theognis to return once they had signed an ambiguous statement of faith. The two, and other friends of Arius, worked for Arius's rehabilitation.
At the First Synod of Tyre in AD 335, they brought accusations against Athanasius, now bishop of Alexandria, the primary opponent of Arius. After this, Constantine had Athanasius banished since he considered him an impediment to reconciliation. In the same year, the Synod of Jerusalem under Constantine's direction readmitted Arius to communion in 336. Arius died on the way to this event in Constantinople. Some scholars suggest that Arius may have been poisoned by his opponents. Eusebius and Theognis remained in the Emperor's favor, and when Constantine, who had been a catechumen much of his adult life, accepted baptism on his deathbed, it was from Eusebius of Nicomedia.
### Aftermath of Nicaea.
The First Council of Nicaea did not end the controversy, as many bishops of the Eastern provinces disputed the "homoousios", the central term of the Nicene Creed, as it had been used by Paul of Samosata, who had advocated a monarchianist Christology. Both the man and his teaching, including the term "homoousios", had been condemned by the Synods of Antioch in 269.
Hence, after Constantine's death in 337, open dispute resumed again. Constantine's son Constantius II, who had become emperor of the eastern part of the Roman Empire, actually encouraged the Arians and set out to reverse the Nicene Creed. His advisor in these affairs was Eusebius of Nicomedia, who had already at the Council of Nicaea been the head of the Arian party, who also was made the bishop of Constantinople.
Constantius used his power to exile bishops adhering to the Nicene Creed, especially St Athanasius of Alexandria, who fled to Rome. In 355 Constantius became the sole Roman emperor and extended his pro-Arian policy toward the western provinces, frequently using force to push through his creed, even exiling Pope Liberius and installing Antipope Felix II.
The Third Council of Sirmium in 357 was the high point of Arianism. The Seventh Arian Confession (Second Sirmium Confession) held that both "homoousios" (of one substance) and "homoiousios" (of similar substance) were unbiblical and that the Father is greater than the Son. (This confession was later known as the Blasphemy of Sirmium.)
But since many persons are disturbed by questions concerning what is called in Latin "substantia", but in Greek "ousia", that is, to make it understood more exactly, as to 'coessential,' or what is called, 'like-in-essence,' there ought to be no mention of any of these at all, nor exposition of them in the Church, for this reason and for this consideration, that in divine Scripture nothing is written about them, and that they are above men's knowledge and above men's understanding;
As debates raged in an attempt to come up with a new formula, three camps evolved among the opponents of the Nicene Creed. The first group mainly opposed the Nicene terminology and preferred the term "homoiousios" (alike in substance) to the Nicene "homoousios", while they rejected Arius and his teaching and accepted the equality and co-eternality of the persons of the Trinity. Because of this centrist position, and despite their rejection of Arius, they were called "Semi-Arians" by their opponents. The second group also avoided invoking the name of Arius, but in large part followed Arius's teachings and, in another attempted compromise wording, described the Son as being like ("homoios") the Father. A third group explicitly called upon Arius and described the Son as unlike ("anhomoios") the Father. Constantius wavered in his support between the first and the second party, while harshly persecuting the third.
Epiphanius of Salamis labeled the party of Basil of Ancyra in 358 "Semi-Arianism". This is considered unfair by Kelly who states that some members of the group were virtually orthodox from the start but disliked the adjective "homoousios" while others had moved in that direction after the out-and-out Arians had come into the open.
The debates among these groups resulted in numerous synods, among them the Council of Serdica in 343, the Fourth Council of Sirmium in 358 and the double Council of Rimini and Seleucia in 359, and no fewer than fourteen further creed formulas between 340 and 360, leading the pagan observer Ammianus Marcellinus to comment sarcastically: "The highways were covered with galloping bishops." None of these attempts were acceptable to the defenders of Nicene orthodoxy; writing about the latter councils, Saint Jerome remarked that the world "awoke with a groan to find itself Arian."
After Constantius' death in 361, his successor Julian, a devotee of Rome's pagan gods, declared that he would no longer attempt to favor one church faction over another, and allowed all exiled bishops to return; this resulted in further increasing dissension among Nicene Christians. The emperor Valens, however, revived Constantius' policy and supported the "Homoian" party, exiling bishops and often using force. During this persecution many bishops were exiled to the other ends of the Roman Empire (e.g., Saint Hilary of Poitiers to the eastern provinces). These contacts and the common plight subsequently led to a rapprochement between the western supporters of the Nicene Creed and the "homoousios" and the eastern Semi-Arians.
### Council of Constantinople.
It was not until the co-reigns of Gratian and Theodosius that Arianism was effectively wiped out among the ruling class and elite of the Eastern Empire. Valens died in the Battle of Adrianople in 378 and was succeeded by Theodosius I, who adhered to the Nicene Creed. This allowed for settling the dispute. Theodosius's wife St Flacilla was instrumental in his campaign to end Arianism.
Two days after Theodosius arrived in Constantinople, 24 November 380, he expelled the Homoiousian bishop, Demophilus of Constantinople, and surrendered the churches of that city to Gregory of Nazianzus, the leader of the rather small Nicene community there, an act which provoked rioting. Theodosius had just been baptized, by bishop Acholius of Thessalonica, during a severe illness, as was common in the early Christian world. In February he and Gratian had published an edict that all their subjects should profess the faith of the bishops of Rome and Alexandria (i.e., the Nicene faith), or be handed over for punishment for not doing so.
Although much of the church hierarchy in the East had opposed the Nicene Creed in the decades leading up to Theodosius's accession, he managed to achieve unity on the basis of the Nicene Creed. In 381, at the Second Ecumenical Council in Constantinople, a group of mainly Eastern bishops assembled and accepted the Nicene Creed of 381, which was supplemented in regard to the Holy Spirit, as well as some other changes: see Comparison of Nicene Creeds of 325 and 381. This is generally considered the end of the dispute about the Trinity and the end of Arianism among the Roman, non-Germanic peoples.
## Among medieval Germanic tribes.
During the time of Arianism's flowering in Constantinople, the Gothic convert and Arian bishop Ulfilas (later the subject of the letter of Auxentius cited above) was sent as a missionary to the Gothic tribes across the Danube, a mission favored for political reasons by the Emperor Constantius II. The Homoians in the Danubian provinces played a major role in the conversion of the Goths to Arianism. Ulfilas's translation of the Bible into Gothic language and his initial success in converting the Goths to Arianism was strengthened by later events; the conversion of Goths led to a widespread diffusion of Arianism among other Germanic tribes as well (Vandals, Langobards, Svevi, and Burgundians). When the Germanic peoples entered the provinces of the Western Roman Empire and began founding their own kingdoms there, most of them were Arian Christians.
The conflict in the 4th century had seen Arian and Nicene factions struggling for control of Western Europe. In contrast, among the Arian German kingdoms established in the collapsing Western Empire in the 5th century were entirely separate Arian and Nicene Churches with parallel hierarchies, each serving different sets of believers. The Germanic elites were Arians, and the Romance majority population was Nicene.
The Arian Germanic tribes were generally tolerant towards Nicene Christians and other religious minorities, including the Jews. However, the Vandals tried for several decades to force their Arian beliefs on their North African Nicene subjects, exiling Nicene clergy, dissolving monasteries, and exercising heavy pressure on non-conforming Nicene Christians.
The apparent resurgence of Arianism after Nicaea was more an anti-Nicene reaction exploited by Arian sympathizers than a pro-Arian development. By the end of the 4th century it had surrendered its remaining ground to Trinitarianism. In Western Europe, Arianism, which had been taught by Ulfilas, the Arian missionary to the Germanic tribes, was dominant among the Goths, Langobards and Vandals. By the 8th century, it had ceased to be the tribes' mainstream belief as the tribal rulers gradually came to adopt Nicene orthodoxy. This trend began in 496 with Clovis I of the Franks, then Reccared I of the Visigoths in 587 and Aripert I of the Lombards in 653.
The Franks and the Anglo-Saxons were unlike the other Germanic peoples in that they entered the Western Roman Empire as Pagans and were converted to Chalcedonian Christianity, led by their kings, Clovis I of the Franks, and Æthelberht of Kent and others in Britain (see also Christianity in Gaul and Christianisation of Anglo-Saxon England). The remaining tribes – the Vandals and the Ostrogoths – did not convert as a people nor did they maintain territorial cohesion. Having been militarily defeated by the armies of Emperor Justinian I, the remnants were dispersed to the fringes of the empire and became lost to history. The Vandalic War of 533–534 dispersed the defeated Vandals. Following their final defeat at the Battle of Mons Lactarius in 553, the Ostrogoths went back north and (re)settled in south Austria.
## From the 5th to the 7th century.
Much of south-eastern Europe and central Europe, including many of the Goths and Vandals respectively, had embraced Arianism (the Visigoths converted to Arian Christianity in 376 through their bishop Wulfila), which led to Arianism being a religious factor in various wars in the Roman Empire. In the west, organized Arianism survived in North Africa, in Hispania, and parts of Italy until it was finally suppressed in the 6th and 7th centuries. Visigothic Spain converted to Nicene Christianity through their king Reccared I at the Third Council of Toledo in 589. Grimoald, King of the Lombards (662–671), and his young son and successor Garibald (671), were the last Arian kings in Europe.
## From the 16th to the 19th century.
Following the Protestant Reformation from 1517, it did not take long for Arian and other nontrinitarian views to resurface. The first recorded English antitrinitarian was John Assheton, who was forced to recant before Thomas Cranmer in 1548. At the Anabaptist Council of Venice 1550, the early Italian instigators of the Radical Reformation committed to the views of Michael Servetus, who was burned alive by the orders of John Calvin in 1553, and these were promulgated by Giorgio Biandrata and others into Poland and Transylvania.
The antitrinitarian wing of the Polish Reformation separated from the Calvinist "ecclesia maior" to form the "ecclesia minor" or Polish Brethren. These were commonly referred to as "Arians" due to their rejection of the Trinity, though in fact the Socinians, as they were later known, went further than Arius to the position of Photinus. The epithet "Arian" was also applied to the early Unitarians such as John Biddle, though in denial of the pre-existence of Christ they were again largely Socinians, not Arians.
In 1683, when Anthony Ashley Cooper, 1st Earl of Shaftesbury, lay dying in Amsterdam – driven into exile by his outspoken opposition to King Charles II – he spoke to the minister Robert Ferguson, and professed himself an Arian.
In the 18th century the "dominant trend" in Britain, particularly in Latitudinarianism, was towards Arianism, with which the names of Samuel Clarke, Benjamin Hoadly, William Whiston and Isaac Newton are associated. To quote the "Encyclopædia Britannica" article on Arianism: "In modern times some Unitarians are virtually Arians in that they are unwilling either to reduce Christ to a mere human being or to attribute to him a divine nature identical with that of the Father."
A similar view was held by the ancient anti-Nicene Pneumatomachi (Greek: , "breath" or "spirit" and "fighters", combining as "fighters against the spirit"), so called because they opposed the deifying of the Nicene Holy Ghost. Although the Pneumatomachi's beliefs were somewhat reminiscent of Arianism, they were a distinct group.
## Today.
The teachings of the first two ecumenical councils – which entirely reject Arianism – are held by the Catholic Church, the Eastern Orthodox Church, the Oriental Orthodox Churches, the Assyrian Church of the East and most churches founded during the Reformation in the 16th century or influenced by it (Lutheran, Reformed/Presbyterian, and Anglican). Also, nearly all Protestant groups (such as Methodists, Baptists, Evangelicals and most Pentecostals) entirely reject the teachings associated with Arianism. Modern groups which currently appear to embrace some of the principles of Arianism include Unitarians and Jehovah's Witnesses. Although the origins of their beliefs are not necessarily attributed to the teachings of Arius, many of the core beliefs of Unitarians and Jehovah's Witnesses are very similar to them.
### The Church of Jesus Christ of Latter-day Saints.
The doctrine of The Church of Jesus Christ of Latter-day Saints (LDS Church) concerning the nature of the Godhead teaches a nontrinitarian theology. The church's first Article of Faith states: "We believe in God, the Eternal Father, and in His Son, Jesus Christ, and in the Holy Ghost," while the 130th section of the its Doctrine and Covenants explains that "The Father has a body of flesh and bones as tangible as man's; the Son also; but the Holy Ghost has not a body of flesh and bones, but is a personage of Spirit. Were it not so, the Holy Ghost could not dwell in us."
Similarities between LDS doctrines and Arianism were noted as early as 1846. There are, however, a number of key differences between Arianism and Latter-day Saint theology, including the co-eternality of Jesus Christ and the Holy Ghost with the Father. Latter-day Saints deny any form of creation "ex nihilo", whereas creation "ex nihilo" and Christ's created and inferior nature are fundamental premises of Arianism. Arianism also teaches that Christ's existence is contingent on the Father, and that he is ontologically subordinate to the Father. Both of these premises are rejected by Latter-day Saint doctrine. Conversely, the LDS Church teaches that Christ is equal in nature, power, and glory with the Father, having perfectly subordinated his will to the Father's. In turn, the Father is understood to have his power by virtue of his own perfect character and subordination to eternal and uncreated principles of righteousness. The Book of Mormon prophet Alma summarizes this by saying that were God not to be perfectly just, then "God would cease to be God". Thus, Christ's subordination to the Father's will is understood as subordination to those same eternal and uncreated principles of righteousness through perfectly emulating the Father's character and example.
The LDS Church teaches that this view of the Godhead is the doctrine taught by Jesus Christ and other ancient prophets, and, by extension, that taught by the scriptures now compiled as the Bible and the Book of Mormon. Thus, Latter-day Saint doctrine does not accept the Nicene definition of Trinity (that the three are consubstantial) nor agree with the Athanasian statement that God and Christ are incomprehensible. In contrast, the Church teaches that the Biblical doctrine is self-evident: "the Father, the Son and the Holy Spirit (or Holy Ghost)... are three physically separate beings, but fully one in love, purpose and will", as illustrated in the Farewell Prayer of Jesus, his baptism at the hands of John, his transfiguration, and the martyrdom of Stephen.
### Jehovah's Witnesses.
Jehovah's Witnesses are often referred to as "modern-day Arians" or they are sometimes referred to as "Semi-Arians", usually by their opponents, although Jehovah's Witnesses themselves have denied these claims. While there are some significant similarities in matters of doctrine, Jehovah's Witnesses differ from Arians by stating that the Son can fully know the Father (something which Arius himself denied), and by their denial of personality to the Holy Spirit. The original Arians also generally prayed directly to Jesus, whereas Jehovah's Witnesses exclusively worship and pray to Jehovah God (God the Father) only through Jesus the son as a mediator.
### Others.
Other groups which oppose the belief in the Trinity are not Arian.

</doc>
<doc id="1254" url="https://en.wikipedia.org/wiki?curid=1254" title="August 1">
August 1



</doc>
<doc id="1255" url="https://en.wikipedia.org/wiki?curid=1255" title="Astronomical Units">
Astronomical Units



</doc>
<doc id="1256" url="https://en.wikipedia.org/wiki?curid=1256" title="Antoninus Pius">
Antoninus Pius

Titus Aelius Hadrianus Antoninus Pius (19 September 86 – 7 March 161) was Roman emperor from 138 to 161. He was one of the Five Good Emperors from the Nerva–Antonine dynasty.
Born into a senatorial family, Antoninus held various offices during the reign of Emperor Hadrian. He married Hadrian's niece Faustina, and Hadrian adopted him as his son and successor shortly before his death. Antoninus acquired the cognomen Pius after his accession to the throne, either because he compelled the Senate to deify his adoptive father, or because he had saved senators sentenced to death by Hadrian in his later years. His reign is notable for the peaceful state of the Empire, with no major revolts or military incursions during this time, and for his governing without ever leaving Italy. A successful military campaign in southern Scotland early in his reign resulted in the construction of the Antonine Wall.
Antoninus was an effective administrator, leaving his successors a large surplus in the treasury, expanding free access to drinking water throughout the Empire, encouraging legal conformity, and facilitating the enfranchisement of freed slaves. He died of illness in 161 and was succeeded by his adopted sons Marcus Aurelius and Lucius Verus as co-emperors.
## Early life.
### Childhood and family.
Antoninus was born near Lanuvium (modern-day Lanuvio in Italy) to Titus Aurelius Fulvus, consul in 89, and Arria Fadilla. The Aurelii Fulvi were an Aurelian family settled in Nemausus (modern Nîmes). Titus Aurelius Fulvus was the son of a senator of the same name, who, as legate of Legio III Gallica, had supported Vespasian in his bid to the Imperial office and been rewarded with a suffect consulship, plus an ordinary one under Domitian in 85. The Aurelii Fulvi were therefore a relatively new senatorial family from Gallia Narbonensis whose rise to prominence was supported by the Flavians. The link between Antoninus' family and their home province explains the increasing importance of the post of Proconsul of Gallia Narbonensis during the late Second Century.
Antoninus’ father had no other children and died shortly after his 89 ordinary consulship. Antoninus was raised by his maternal grandfather Gnaeus Arrius Antoninus, reputed by contemporaries to be a man of integrity and culture and a friend of Pliny the Younger. The Arrii Antonini were an older senatorial family from Italy, very influential during Nerva's reign. Arria Fadilla, Antoninus' mother, married afterwards Publius Julius Lupus, suffect consul in 98; from that marriage came two daughters, Arria Lupula and Julia Fadilla.
### Marriage and children.
Some time between 110 and 115, Antoninus married Annia Galeria Faustina the Elder. They are believed to have enjoyed a happy marriage. Faustina was the daughter of consul Marcus Annius Verus (II) and Rupilia Faustina (a half-sister to the Empress Vibia Sabina). Faustina was a beautiful woman, and despite (basically unproven) rumours about her character, it is clear that Antoninus cared for her deeply.
Faustina bore Antoninus four children, two sons and two daughters. They were:
When Faustina died in 141, Antoninus was greatly distressed. In honour of her memory, he asked the Senate to deify her as a goddess, and authorised the construction of a temple to be built in the Roman Forum in her name, with priestesses serving in her temple. He had various coins with her portrait struck in her honor. These coins were scripted "DIVA FAUSTINA" and were elaborately decorated. He further founded a charity, calling it "Puellae Faustinianae" or "Girls of Faustina", which assisted destitute girls of good family. Finally, Antoninus created a new "alimenta" (see Grain supply to the city of Rome).
The emperor never remarried. Instead, he lived with Galeria Lysistrate, one of Faustina's freed women. Concubinage was a form of female companionship sometimes chosen by powerful men in Ancient Rome, especially widowers like Vespasian, and Marcus Aurelius. Their union could not produce any legitimate offspring who could threaten any heirs, such as those of Antoninus. Also, as one could not have a wife and an official concubine (or two concubines) at the same time, Antoninus avoided being pressed into a marriage with a noblewoman from another family. (Later, Marcus Aurelius would also reject the advances of his former fiancée Ceionia Fabia, Lucius Verus's sister, on the grounds of protecting his children from a stepmother, and took a concubine instead.)
## Favour with Hadrian.
Having filled the offices of quaestor and praetor with more than usual success, he obtained the consulship in 120 having as his colleague Lucius Catilius Severus. He was next appointed by the Emperor Hadrian as one of the four proconsuls to administer Italia, his district including Etruria, where he had estates. He then greatly increased his reputation by his conduct as proconsul of Asia, probably during 134–135.
He acquired much favor with Hadrian, who adopted him as his son and successor on 25 February 138, after the death of his first adopted son Lucius Aelius, on the condition that Antoninus would in turn adopt Marcus Annius Verus, the son of his wife's brother, and Lucius, son of Lucius Aelius, who afterwards became the emperors Marcus Aurelius and Lucius Verus. He also adopted (briefly) the name Imperator Titus Aelius Caesar Antoninus, in preparation for his rule.There seems to have been some opposition to Antoninus' appointment on the part of other potential claimants, among them his former consular colleague Lucius Catilius Severus, then Prefect of the city. Nevertheless, Antoninus assumed power without opposition.
## Emperor.
On his accession, Antoninus' name and style became "Imperator Caesar Titus Aelius Hadrianus Antoninus Augustus Pontifex Maximus". One of his first acts as Emperor was to persuade the Senate to grant divine honours to Hadrian, which they had at first refused; his efforts to persuade the Senate to grant these honours is the most likely reason given for his title of "Pius" (dutiful in affection; compare "pietas"). Two other reasons for this title are that he would support his aged father-in-law with his hand at Senate meetings, and that he had saved those men that Hadrian, during his period of ill-health, had condemned to death.
Immediately after Hadrian's death, Antoninus approached Marcus and requested that his marriage arrangements be amended: Marcus' betrothal to Ceionia Fabia would be annulled, and he would be betrothed to Faustina, Antoninus' daughter, instead. Faustina's betrothal to Ceionia's brother Lucius Commodus would also have to be annulled. Marcus consented to Antoninus' proposal.
Antoninus built temples, theaters, and mausoleums, promoted the arts and sciences, and bestowed honours and financial rewards upon the teachers of rhetoric and philosophy. Antoninus made few initial changes when he became emperor, leaving intact as far as possible the arrangements instituted by Hadrian. Epigraphical and prosopographical research has revealed that Antoninus' imperial ruling team centered around a group of closely knit senatorial families, most of them members of the priestly congregation for the cult of Hadrian, the "sodales Hadrianales". According to the German historian H.G. Pflaum, prosopographical research of Antoninus' ruling team allows us to grasp the deeply conservative character of the ruling senatorial caste.
## A non-military reign.
There are no records of any military related acts in his time in which he participated. One modern scholar has written "It is almost certain not only that at no time in his life did he ever see, let alone command, a Roman army, but that, throughout the twenty-three years of his reign, he never went within five hundred miles of a legion".
His reign was the most peaceful in the entire history of the Principate, notwithstanding the fact that there were several military disturbances throughout the Empire in his time. Such disturbances happened in Mauretania — where a senator was named as governor of Mauretania Tingitana in place of the usual equestrian procurator and cavalry reinforcements from Pannonia were brought in, towns such as Sala and Tipasa being fortified. Similar disturbances took place in Judea, and amongst the Brigantes in Britannia, none of them being considered serious. It was however in Britain that Antoninus decided to follow a new, more aggressive path, with the appointment of a new governor in 139, Quintus Lollius Urbicus, a native of Numidia and previously governor of Germania Inferior as well as a new man.
Under instructions from the emperor, Lollius undertook an invasion of southern Scotland, winning some significant victories, and constructing the Antonine Wall from the Firth of Forth to the Firth of Clyde. The wall, however, was soon gradually decommissioned during the mid-150s and eventually abandoned late during the reign (early 160s), for reasons that are still not quite clear. Antonine's Wall is mentioned in just one literary source, Antoninus' biography in the Historia Augusta. Pausanias makes a brief and confused mention of a war in Britain. In one inscription honoring Antoninus, erected by Legio II Augusta, which participated in the building of the Wall, a relief showing four naked prisoners, one of them beheaded, seems to stand for some actual warfare.
Although Antonine's Wall was, in principle, much shorter (37 miles in length as opposed to 73) and at first sight more defensible than Hadrian's Wall, the additional area that it enclosed within the Empire was barren, with land use for grazing already in decay. This meant that supply lines to the wall were strained enough such as the costs for maintaining the additional territory outweighed the benefits of doing so. Also, in the absence of urban development and the ensuing Romanization process, the rear of the wall could not be lastingly pacified.
It has been therefore speculated that the invasion of Lowland Scotland and the building of the wall had to do mostly with internal politics, that is, offering Antoninus an opportunity to gain some modicum of necessary military prestige at the start of his reign. Actually, the campaign in Britannia was followed by an Imperial salutation — that is, by Antoninus formally taking for the second (and last) time the title of Imperator — in 142. The fact that around the same time coins were struck announcing a victory in Britain points to Antoninus' need to publicize his achievements. The orator Fronto was later to say that, although Antoninus bestowed the direction of the British campaign to others, he should be regarded as the helmsman who directed the voyage, whose glory, therefore, belonged to him.
That this quest for some military achievement responded to an actual need is proved by the fact that, although generally peaceful, Antoninus' reign was not free from attempts at usurpation: Historia Augusta mentions two, made by the senators Cornelius Priscianus ("for disturbing the peace of Spain"; Priscianus had also been Lollius Urbicus' successor as governor of Britain) and Atilius Rufius Titianus (possibly a troublemaker already exiled under Hadrian.) Both attempts are confirmed by the Fasti Ostienses as well as by the erasing of Priscianus' name from an inscription. In both cases, Antoninus was not in formal charge of the ensuing repression: Priscianus committed suicide and Titianus was found guilty by the Senate, with Antoninus abstaining from sequestering their families' properties.
There were also some troubles in Dacia Inferior which required the granting of additional powers to the procurator governor and the dispatch of additional soldiers to the province. On the Northern Black Sea coast, the Greek city of Olbia was held against the Scythians. Also during his reign the governor of Upper Germany, probably Caius Popillius Carus Pedo, built new fortifications in the Agri Decumates, advancing the Limes Germanicus fifteen miles forward in his province and neighboring Raetia. In the East, Roman suzerainty over Armenia was retained by the choice in AD 140 of Arsacid scion Sohaemus as client king.
Nevertheless, Antoninus was virtually unique among emperors in that he dealt with these crises without leaving Italy once during his reign, but instead dealt with provincial matters of war and peace through their governors or through imperial letters to the cities such as Ephesus (of which some were publicly displayed). This style of government was highly praised by his contemporaries and by later generations.
Antoninus was the last Roman Emperor recognised by the Indian Kingdoms, especially the Kushan Empire. Raoul McLaughlin quotes Aurelius Victor as saying "The Indians, the Bactrians and the Hyrcanians all sent ambassadors to Antoninus. They had all heard about the spirit of justice held by this great emperor, justice that was heightened by his handsome and grave countenance, and his slim and vigorous figure." Due to the outbreak of the Antonine epidemic and wars against northern Germanic tribes, the reign of Marcus Aurelius was forced to alter the focus of foreign policies, and matters relating to the Far East were increasingly abandoned in favour of those directly concerning the Empire's survival.
## Economy and administration.
Antoninus was regarded as a skilled administrator and as a builder. In spite of an extensive building directive — the free access of the people of Rome to drinking water was expanded with the construction of aqueducts, not only in Rome but throughout the Empire, as well as bridges and roads — the emperor still managed to leave behind a sizable public treasury of around 2.7 billion sesterces. Rome would not witness another Emperor leaving his successor with a surplus for a long time, but this treasury was depleted almost immediately after Antoninus's reign due to the Antonine plague brought back by soldiers after the Parthian victory.
The Emperor also famously suspended the collection of taxes from cities affected by natural disasters, such as when fires struck Rome and Narbona, and earthquakes affected Rhodes and the Province of Asia. He offered hefty financial grants for rebuilding and recovery of various Greek cities after two serious earthquakes: the first, "circa" 140, which affected mostly Rhodes and other islands; the second, in 152, which hit Cyzicus (where the huge and newly built Temple to Hadrian was destroyed), Ephesus, and Smyrna. Antoninus' financial help earned him praise by Greek writers such as Aelius Aristides and Pausanias. These cities received from Antoninus the usual honorific accolades, such as when he commanded that all governors of Asia should enter the province, when taking office, by way of Ephesus. Ephesus was specially favoured by Antoninus, who confirmed and upheld its distinction of having two temples for the imperial cult (neocorate), therefore having first place in the list of imperial honor titles, surpassing both Smyrna and Pergamon.
In his dealings with Greek-speaking cities, Antoninus followed the policy adopted by Hadrian of ingratiating himself with local elites, especially with local intellectuals: philosophers, teachers of literature, rhetoricians and physicians were explicitly exempted from any duties involving private spending for civic purposes — a privilege granted by Hadrian that Antoninus confirmed by means of an edict preserved in the Digest (27.1.6.8). Antoninus also created a chair for the teaching of rhetoric in Athens.
Antoninus was known as an avid observer of rites of religion and of formal celebrations — both Roman and foreign. He is known for having increasingly formalized the official cult offered to the Great Mother, which from his reign onwards included a bull sacrifice, a taurobolium, formerly only a private ritual, now being also performed for the sake of the Emperor's welfare. Antoninus also offered patronage to the worship of Mithras, to whom he erected a temple in Ostia. In 148, he presided over the celebrations of the 900th anniversary of the founding of Rome.
## Legal reforms.
Antoninus tried to portray himself as a magistrate of the "res publica", no matter how extended and ill-defined his competencies were. He is credited with the splitting of the imperial treasury, the Fiscus. This splitting had to do with the division of imperial properties into two parts. Firstly, the fiscus itself — or "patrimonium", meaning the properties of the "Crown", the hereditary properties of each succeeding person that sat on the throne, transmitted to his successors in office, regardless of their previous membership in the imperial family. Secondly, the "res privata", the "private" properties tied to the personal maintenance of the Emperor and his family, something like a Privy Purse. An anecdote in the "Historia Augusta" biography, where Antoninus replies to Faustina — who complained about his stinginess — that "we have gained an empire [and] lost even what we had before" possibly relates to Antoninus' actual concerns at the creation of the "res privata". While still a private citizen, Antoninus had increased his personal fortune greatly by mean of various legacies, the consequence — we are told — of his caring scrupulously for his relatives. Also, Antoninus left behind him a reputation for stinginess and was probably determined not to leave his personal property to be "swallowed up by the demands of the imperial throne".
The "res privata" lands could be sold and/or given away, while the "patrimonium" properties were regarded as public. It was a way of pretending that the Imperial function — and most properties attached to it — was a public one, formally subject to the authority of the Senate and the Roman people. That the distinction played no part in subsequent political history — that the "personal" power of the princeps absorbed his role as office-holder — proves that the autocratic logic of the imperial order had already subsumed the old republican institutions.
Of the public transactions of this period there is only the scantiest of information, but, to judge by what is extant, those twenty-two years were not remarkably eventful in comparison to those before and after the reign. However, Antoninus did take a great interest in the revision and practice of the law throughout the empire. One of his chief concerns was to having local communities conform their legal procedures to existing Roman norms: in a case concerning repression of banditry by local police officers ("irenarchs", Greek for "peace keepers") in Asia Minor, Antoninus ordered that these officers should not treat suspects as already condemned, and also keep a detailed copy of their interrogations, to be used in the possibility of an appeal to the Roman governor. Also, although Antoninus was not an innovator, he would not always follow the absolute letter of the law; rather he was driven by concerns over humanity and equality, and introduced into Roman law many important new principles based upon this notion.
In this, the emperor was assisted by five chief lawyers: Lucius Fulvius Aburnius Valens, an author of legal treatises; Lucius Ulpius Marcellus, a prolific writer; and three others. Of these three, the most prominent was Lucius Volusius Maecianus, a former military officer turned by Antoninus into a civil procurator, and who, in view of his subsequent career (discovered on the basis of epigraphical and prosopographical research), was the Emperor's most important legal adviser. Maecianus would eventually be chosen to occupy various prefectures (see below) as well as to conduct the legal studies of Marcus Aurelius. He was also the author of a large work on "Fidei commissa" (Testamentary Trusts). As a hallmark of the increased connection between jurists and the imperial government, Antoninus' reign also saw the appearance of the "Institutes of Gaius", an elementary legal manual for beginners (see Gaius (jurist)).
Antoninus passed measures to facilitate the enfranchisement of slaves. Mostly, he favoured the principle of "favor libertatis", giving the putative freedman the benefit of the doubt when the claim to freedom was not clearcut. Also, he punished the killing of a slave by his/her master without previous trial and determined that slaves could be forcibly sold to another master by a proconsul in cases of consistent mistreatment. Antoninus upheld the enforcement of contracts for selling of female slaves forbidding their further employment in prostitution. In criminal law, Antoninus introduced the important principle that accused persons are not to be treated as guilty before trial — as in the case of the irenarchs (see above). It was to Antonius that the Christian apologist Justin Martyr addressed his defense of the Christian faith, reminding him of his father's (Emperor Hadrian's) rule that accusations against Christians required proof. He also asserted the principle that the trial was to be held, and the punishment inflicted, in the place where the crime had been committed. He mitigated the use of torture in examining slaves by certain limitations. Thus he prohibited the application of torture to children under fourteen years, though this rule had exceptions. However, it must be stressed that Antoninus "extended", by means of a rescript, the use of torture as a means of obtaining evidence to pecuniary cases, when it had been applied up until then only in criminal cases. Also, already at the time torture of free men of low status ("humiliores") had become legal, as proved by the fact that Antoninus exempted town councillors expressly from it, and also free men of high rank ("honestiores") in general.
One highlight during his reign occurred in 148, with the nine-hundredth anniversary of the foundation of Rome being celebrated by the hosting of magnificent games in Rome. It lasted a number of days, and a host of exotic animals were killed, including elephants, giraffes, tigers, rhinoceroses, crocodiles and hippopotami. While this increased Antoninus's popularity, the frugal emperor had to debase the Roman currency. He decreased the silver purity of the denarius from 89% to 83.5% — the actual silver weight dropping from 2.88 grams to 2.68 grams.
Scholars name Antoninus Pius as the leading candidate for an individual identified as a friend of Rabbi Judah the Prince. According to the Talmud (Avodah Zarah 10a–b), Rabbi Judah was very wealthy and greatly revered in Rome. He had a close friendship with "Antoninus", possibly Antoninus Pius, who would consult Rabbi Judah on various worldly and spiritual matters.
## Death.
In 156, Antoninus Pius turned 70. He found it difficult to keep himself upright without stays. He started nibbling on dry bread to give him the strength to stay awake through his morning receptions.
Marcus Aurelius had already been created consul with Antoninus in 140, receiving the title of Caesar, i.e., heir apparent. As Antoninus aged, Marcus took on more administrative duties. Marcus's administrative duties increased again after the death — in 156 or 157 — of one of Antoninus' most trusted advisers, Marcus Gavius Maximus.
For twenty years, Gavius Maximus had been praetorian prefect, an office that was as much secretarial as military. Gavius Maximus had been awarded with the consular insignia and the honors due a senator. He had a reputation as a most strict disciplinarian ("vir severissimus", according to "Historia Augusta") and some fellow equestrian procurators held lasting grudges against him. A procurator named Gaius Censorius Niger died while Gavius Maximus was alive. In his will, Censorius Niger vilified Maximus, creating serious embarrassment for one of the heirs, the orator Fronto.
Gavius Maximus' death initiated a change in the ruling team. It has been speculated that it was the legal adviser Lucius Volusius Maecianus who assumed the role of grey eminence. Maecianus was briefly Praefect of Egypt, and subsequently Praefectus annonae in Rome. If it was Maecianus who rose to prominence, he may have risen precisely in order to prepare the incoming — and unprecedented — joint succession. In 160, Marcus and Lucius were designated joint consuls for the following year. Perhaps Antoninus was already ill; in any case, he died before the year was out, probably on 7 March.
Two days before his death, the biographer reports, Antoninus was at his ancestral estate at Lorium, in Etruria, about from Rome. He ate Alpine Gruyere cheese at dinner quite greedily. In the night he vomited; he had a fever the next day. The day after that, he summoned the imperial council, and passed the state and his daughter to Marcus. The emperor gave the keynote to his life in the last word that he uttered: when the tribune of the night-watch came to ask the password, he responded, "aequanimitas" (equanimity). He then turned over, as if going to sleep, and died. His death closed out the longest reign since Augustus (surpassing Tiberius by a couple of months). His record for the second-longest reign would be unbeaten for 168 years, until 329 when it was surpassed by Constantine the Great.
Antoninus Pius' funeral ceremonies were, in the words of the biographer, "elaborate". If his funeral followed the pattern of past funerals, his body would have been incinerated on a pyre at the Campus Martius, while his spirit would rise to the gods' home in the heavens. However, it seems that this was not the case: according to his "Historia Augusta" biography (which seems to reproduce an earlier, detailed report) Antoninus' body (and not his ashes) was buried in Hadrian's mausoleum. After a seven-day interval ("justitium"), Marcus and Lucius nominated their father for deification. In contrast to their behavior during Antoninus' campaign to deify Hadrian, the senate did not oppose the emperors' wishes. A "flamen", or cultic priest, was appointed to minister the cult of the deified Antoninus, now "Divus Antoninus".
A column was dedicated to Antoninus on the Campus Martius, and the temple he had built in the Forum in 141 to his deified wife Faustina was rededicated to the deified Faustina and the deified Antoninus. It survives as the church of San Lorenzo in Miranda.
## Diplomatic mission to China.
The first group of people claiming to be an ambassadorial mission of Romans to China was recorded in 166 AD by the "Hou Hanshu". Harper (2017) states that the embassy was likely to be a group of merchants, as many Roman merchants traveled to India and some might have gone beyond, while there are no records of official ambassadors of Rome travelling as far east. The group came to Emperor Huan of Han China and claimed to be an embassy from "Andun" (; for "Anton"-inus), "king of Daqin" (Rome). As Antoninus Pius died in 161, leaving the empire to his adoptive son Marcus Aurelius (Antoninus), and the envoy arrived in 166, confusion remains about who sent the mission, given that both Emperors were named "Antoninus". The Roman mission came from the south (therefore probably by sea), entering China by the frontier province of Jiaozhi at Rinan or Tonkin (present-day northern Vietnam). It brought presents of rhinoceros horns, ivory, and tortoise shell, probably acquired in Southern Asia. The text specifically states that it was the first time there had been direct contact between the two countries.
Furthermore, a piece of Republican-era Roman glassware has been found at a Western Han tomb in Guangzhou along the South China Sea, dated to the early 1st century BC. Roman golden medallions made during the reign of Antoninus Pius and perhaps even Marcus Aurelius have been found at Óc Eo in southern Vietnam, then part of the Kingdom of Funan near the Chinese province of Jiaozhi. This may have been the port city of Kattigara, described by Ptolemy (c. 150) as being visited by a Greek sailor named Alexander and lying beyond the Golden Chersonese (i.e., Malay Peninsula). Roman coins from the reigns of Tiberius to Aurelian have been discovered in Xi'an, China (site of the Han capital Chang'an), although the significantly greater amount of Roman coins unearthed in India suggest the Roman maritime trade for purchasing Chinese silk was centered there, not in China or even the overland Silk Road running through ancient Iran.
## Historiography.
The only intact account of his life handed down to us is that of the "Augustan History", an unreliable and mostly fabricated work. Nevertheless, it still contains information that is considered reasonably sound — for instance, it is the only source that mentions the erection of the Antonine Wall in Britain. Antoninus is unique among Roman emperors in that he has no other biographies.
### In later scholarship.
Antoninus in many ways was the ideal of the landed gentleman praised not only by ancient Romans, but also by later scholars of classical history, such as Edward Gibbon or the author of the article on Antoninus Pius in the "Encyclopædia Britannica" Eleventh Edition.
Some historians have a less positive view of his reign. According to the historian J. B. Bury,
German historian Ernst Kornemann has had it in his "Römische Geschichte" [2 vols., ed. by H. Bengtson, Stuttgart 1954] that the reign of Antoninus comprised "a succession of grossly wasted opportunities", given the upheavals that were to come. There is more to this argument, given that the Parthians in the East were themselves soon to make no small amount of mischief after Antoninus' death. Kornemann's brief is that Antoninus might have waged preventive wars to head off these outsiders. Michael Grant agrees that it is possible that had Antoninus acted decisively sooner (it appears that, on his death bed, he was preparing a large-scale action against the Parthians), the Parthians might have been unable to choose their own time, but current evidence is not conclusive. Grant opines that Antoninus and his officers did act in a resolute manner dealing with frontier disturbances of his time, although conditions for long-lasting peace were not created. On the whole, according to Grant, Marcus Aurelius' eulogistic picture of Antoninus seems deserved, and Antoninus appears to have been a conservative and nationalistic (although he respected and followed Hadrian's example of Philhellenism moderately) Emperor who was not tainted by the blood of either citizen or foe, combined and maintained Numa Pompilius' good fortune, pacific dutifulness and religious scrupulousness, and whose laws removed anomalies and softened harshnesses.
Krzysztof Ulanowski argues that the claims of military inability are exaggerated, considering that although the sources praise Antoninus' love for peace and his efforts "rather to defend, than enlarge the provinces", he could hardly be considered a pacifist, as shown by the conquest of the Lowlands, the building of the Antonine Wall and the expansion of Germania Superior. Ulianowski also praises Antoninus for being successful in deterrence by diplomatic means.
## Descendants.
Although only one of his four children survived to adulthood, Antoninus came to be ancestor to four generations of prominent Romans, including the Emperor Commodus. Hans-Georg Pflaum has identified five direct descendants of Antoninus and Faustina who were consuls in the first half of the third century.

</doc>
<doc id="1259" url="https://en.wikipedia.org/wiki?curid=1259" title="August 3">
August 3



</doc>
<doc id="1260" url="https://en.wikipedia.org/wiki?curid=1260" title="Advanced Encryption Standard">
Advanced Encryption Standard

The Advanced Encryption Standard (AES), also known by its original name Rijndael (), is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.
AES is a variant of the Rijndael block cipher developed by two Belgian cryptographers, Vincent Rijmen and Joan Daemen, who submitted a proposal to NIST during the AES selection process. Rijndael is a family of ciphers with different key and block sizes. For AES, NIST selected three members of the Rijndael family, each with a block size of 128 bits, but three different key lengths: 128, 192 and 256 bits.
AES has been adopted by the U.S. government. It supersedes the Data Encryption Standard (DES), which was published in 1977. The algorithm described by AES is a symmetric-key algorithm, meaning the same key is used for both encrypting and decrypting the data.
In the United States, AES was announced by the NIST as U.S. FIPS PUB 197 (FIPS 197) on November 26, 2001. This announcement followed a five-year standardization process in which fifteen competing designs were presented and evaluated, before the Rijndael cipher was selected as the most suitable (see Advanced Encryption Standard process for more details).
AES is included in the ISO/IEC 18033-3 standard. AES became effective as a U.S. federal government standard on May 26, 2002, after approval by the U.S. Secretary of Commerce. AES is available in many different encryption packages, and is the first (and only) publicly accessible cipher approved by the U.S. National Security Agency (NSA) for top secret information when used in an NSA approved cryptographic module (see Security of AES, below).
## Definitive standards.
The Advanced Encryption Standard (AES) is defined in each of:
## Description of the ciphers.
AES is based on a design principle known as a substitution–permutation network, and is efficient in both software and hardware. Unlike its predecessor DES, AES does not use a Feistel network. AES is a variant of Rijndael, with a fixed block size of 128 bits, and a key size of 128, 192, or 256 bits. By contrast, Rijndael "per se" is specified with block and key sizes that may be any multiple of 32 bits, with a minimum of 128 and a maximum of 256 bits.
AES operates on a 4 × 4 column-major order array of bytes, termed the "state". Most AES calculations are done in a particular finite field.
For instance, 16 bytes, formula_1 are represented as this two-dimensional array:
The key size used for an AES cipher specifies the number of transformation rounds that convert the input, called the plaintext, into the final output, called the ciphertext. The number of rounds are as follows:
Each round consists of several processing steps, including one that depends on the encryption key itself. A set of reverse rounds are applied to transform ciphertext back into the original plaintext using the same encryption key.
### The step.
In the step, each byte formula_3 in the "state" array is replaced with a formula_4 using an 8-bit substitution box. Note that before round 0, the "state" array is simply the plaintext/input. This operation provides the non-linearity in the cipher. The S-box used is derived from the multiplicative inverse over , known to have good non-linearity properties. To avoid attacks based on simple algebraic properties, the S-box is constructed by combining the inverse function with an invertible affine transformation. The S-box is also chosen to avoid any fixed points (and so is a derangement), i.e., formula_5, and also any opposite fixed points, i.e., formula_6.
While performing the decryption, the step (the inverse of ) is used, which requires first taking the inverse of the affine transformation and then finding the multiplicative inverse.
### The step.
The step operates on the rows of the state; it cyclically shifts the bytes in each row by a certain offset. For AES, the first row is left unchanged. Each byte of the second row is shifted one to the left. Similarly, the third and fourth rows are shifted by offsets of two and three respectively. In this way, each column of the output state of the step is composed of bytes from each column of the input state. The importance of this step is to avoid the columns being encrypted independently, in which case AES would degenerate into four independent block ciphers.
### The step.
In the step, the four bytes of each column of the state are combined using an invertible linear transformation. The function takes four bytes as input and outputs four bytes, where each input byte affects all four output bytes. Together with , provides diffusion in the cipher.
During this operation, each column is transformed using a fixed matrix (matrix left-multiplied by column gives new value of column in the state):
Matrix multiplication is composed of multiplication and addition of the entries. Entries are bytes treated as coefficients of polynomial of order formula_8. Addition is simply XOR. Multiplication is modulo irreducible polynomial formula_9. If processed bit by bit, then, after shifting, a conditional XOR with 1B16 should be performed if the shifted value is larger than FF16 (overflow must be corrected by subtraction of generating polynomial). These are special cases of the usual multiplication in formula_10.
In more general sense, each column is treated as a polynomial over formula_10 and is then multiplied modulo formula_12 with a fixed polynomial formula_13. The coefficients are displayed in their hexadecimal equivalent of the binary representation of bit polynomials from formula_14. The step can also be viewed as a multiplication by the shown particular MDS matrix in the finite field formula_10. This process is described further in the article Rijndael MixColumns.
### The step.
In the step, the subkey is combined with the state. For each round, a subkey is derived from the main key using Rijndael's key schedule; each subkey is the same size as the state. The subkey is added by combining each byte of the state with the corresponding byte of the subkey using bitwise XOR.
### Optimization of the cipher.
On systems with 32-bit or larger words, it is possible to speed up execution of this cipher by combining the and steps with the step by transforming them into a sequence of table lookups. This requires four 256-entry 32-bit tables (together occupying 4096 bytes). A round can then be performed with 16 table lookup operations and 12 32-bit exclusive-or operations, followed by four 32-bit exclusive-or operations in the step. Alternatively, the table lookup operation can be performed with a single 256-entry 32-bit table (occupying 1024 bytes) followed by circular rotation operations.
Using a byte-oriented approach, it is possible to combine the , , and steps into a single round operation.
## Security.
The National Security Agency (NSA) reviewed all the AES finalists, including Rijndael, and stated that all of them were secure enough for U.S. Government non-classified data. In June 2003, the U.S. Government announced that AES could be used to protect classified information:
The design and strength of all key lengths of the AES algorithm (i.e., 128, 192 and 256) are sufficient to protect classified information up to the SECRET level. TOP SECRET information will require use of either the 192 or 256 key lengths. The implementation of AES in products intended to protect national security systems and/or information must be reviewed and certified by NSA prior to their acquisition and use.
AES has 10 rounds for 128-bit keys, 12 rounds for 192-bit keys, and 14 rounds for 256-bit keys.
By 2006, the best known attacks were on 7 rounds for 128-bit keys, 8 rounds for 192-bit keys, and 9 rounds for 256-bit keys.
### Known attacks.
For cryptographers, a cryptographic "break" is anything faster than a brute-force attack – i.e., performing one trial decryption for each possible key in sequence (see Cryptanalysis). A break can thus include results that are infeasible with current technology. Despite being impractical, theoretical breaks can sometimes provide insight into vulnerability patterns. The largest successful publicly known brute-force attack against a widely implemented block-cipher encryption algorithm was against a 64-bit RC5 key by distributed.net in 2006.
The key space increases by a factor of 2 for each additional bit of key length, and if every possible value of the key is equiprobable, this translates into a doubling of the average brute-force key search time. This implies that the effort of a brute-force search increases exponentially with key length. Key length in itself does not imply security against attacks, since there are ciphers with very long keys that have been found to be vulnerable.
AES has a fairly simple algebraic framework. In 2002, a theoretical attack, named the "XSL attack", was announced by Nicolas Courtois and Josef Pieprzyk, purporting to show a weakness in the AES algorithm, partially due to the low complexity of its nonlinear components. Since then, other papers have shown that the attack, as originally presented, is unworkable; see XSL attack on block ciphers.
During the AES selection process, developers of competing algorithms wrote of Rijndael's algorithm "we are concerned about [its] use ... in security-critical applications." In October 2000, however, at the end of the AES selection process, Bruce Schneier, a developer of the competing algorithm Twofish, wrote that while he thought successful academic attacks on Rijndael would be developed someday, he "did not believe that anyone will ever discover an attack that will allow someone to read Rijndael traffic."
Until May 2009, the only successful published attacks against the full AES were side-channel attacks on some specific implementations. In 2009, a new related-key attack was discovered that exploits the simplicity of AES's key schedule and has a complexity of 2119. In December 2009 it was improved to 299.5. This is a follow-up to an attack discovered earlier in 2009 by Alex Biryukov, Dmitry Khovratovich, and Ivica Nikolić, with a complexity of 296 for one out of every 235 keys. However, related-key attacks are not of concern in any properly designed cryptographic protocol, as a properly designed protocol (i.e., implementational software) will take care not to allow related keys, essentially by constraining an attacker's means of selecting keys for relatedness.
Another attack was blogged by Bruce Schneier
on July 30, 2009, and released as a preprint
on August 3, 2009. This new attack, by Alex Biryukov, Orr Dunkelman, Nathan Keller, Dmitry Khovratovich, and Adi Shamir, is against AES-256 that uses only two related keys and 239 time to recover the complete 256-bit key of a 9-round version, or 245 time for a 10-round version with a stronger type of related subkey attack, or 270 time for an 11-round version. 256-bit AES uses 14 rounds, so these attacks are not effective against full AES.
The practicality of these attacks with stronger related keys has been criticized, for instance, by the paper on chosen-key-relations-in-the-middle attacks on AES-128 authored by Vincent Rijmen in 2010.
In November 2009, the first known-key distinguishing attack against a reduced 8-round version of AES-128 was released as a preprint.
This known-key distinguishing attack is an improvement of the rebound, or the start-from-the-middle attack, against AES-like permutations, which view two consecutive rounds of permutation as the application of a so-called Super-S-box. It works on the 8-round version of AES-128, with a time complexity of 248, and a memory complexity of 232. 128-bit AES uses 10 rounds, so this attack is not effective against full AES-128.
The first key-recovery attacks on full AES were by Andrey Bogdanov, Dmitry Khovratovich, and Christian Rechberger, and were published in 2011. The attack is a biclique attack and is faster than brute force by a factor of about four. It requires 2126.2 operations to recover an AES-128 key. For AES-192 and AES-256, 2190.2 and 2254.6 operations are needed, respectively. This result has been further improved to 2126.0 for AES-128, 2189.9 for AES-192 and 2254.3 for AES-256, which are the current best results in key recovery attack against AES.
This is a very small gain, as a 126-bit key (instead of 128-bits) would still take billions of years to brute force on current and foreseeable hardware. Also, the authors calculate the best attack using their technique on AES with a 128-bit key requires storing 288 bits of data. That works out to about 38 trillion terabytes of data, which is more than all the data stored on all the computers on the planet in 2016. As such, there are no practical implications on AES security. The space complexity has later been improved to 256 bits, which is 9007 terabytes.
According to the Snowden documents, the NSA is doing research on whether a cryptographic attack based on tau statistic may help to break AES.
At present, there is no known practical attack that would allow someone without knowledge of the key to read data encrypted by AES when correctly implemented.
### Side-channel attacks.
Side-channel attacks do not attack the cipher as a black box, and thus are not related to cipher security as defined in the classical context, but are important in practice. They attack implementations of the cipher on hardware or software systems that inadvertently leak data. There are several such known attacks on various implementations of AES.
In April 2005, D. J. Bernstein announced a cache-timing attack that he used to break a custom server that used OpenSSL's AES encryption. The attack required over 200 million chosen plaintexts. The custom server was designed to give out as much timing information as possible (the server reports back the number of machine cycles taken by the encryption operation). However, as Bernstein pointed out, "reducing the precision of the server's timestamps, or eliminating them from the server's responses, does not stop the attack: the client simply uses round-trip timings based on its local clock, and compensates for the increased noise by averaging over a larger number of samples".
In October 2005, Dag Arne Osvik, Adi Shamir and Eran Tromer presented a paper demonstrating several cache-timing attacks against the implementations in AES found in OpenSSL and Linux's codice_1 partition encryption function. One attack was able to obtain an entire AES key after only 800 operations triggering encryptions, in a total of 65 milliseconds. This attack requires the attacker to be able to run programs on the same system or platform that is performing AES.
In December 2009 an attack on some hardware implementations was published that used differential fault analysis and allows recovery of a key with a complexity of 232.
In November 2010 Endre Bangerter, David Gullasch and Stephan Krenn published a paper which described a practical approach to a "near real time" recovery of secret keys from AES-128 without the need for either cipher text or plaintext. The approach also works on AES-128 implementations that use compression tables, such as OpenSSL. Like some earlier attacks, this one requires the ability to run unprivileged code on the system performing the AES encryption, which may be achieved by malware infection far more easily than commandeering the root account.
In March 2016, Ashokkumar C., Ravi Prakash Giri and Bernard Menezes presented a side-channel attack on AES implementations that can recover the complete 128-bit AES key in just 6–7 blocks of plaintext/ciphertext, which is a substantial improvement over previous works that require between 100 and a million encryptions. The proposed attack requires standard user privilege and key-retrieval algorithms run under a minute.
Many modern CPUs have built-in hardware instructions for AES, which protect against timing-related side-channel attacks.
## NIST/CSEC validation.
The Cryptographic Module Validation Program (CMVP) is operated jointly by the United States Government's National Institute of Standards and Technology (NIST) Computer Security Division and the Communications Security Establishment (CSE) of the Government of Canada. The use of cryptographic modules validated to NIST FIPS 140-2 is required by the United States Government for encryption of all data that has a classification of Sensitive but Unclassified (SBU) or above. From NSTISSP #11, National Policy Governing the Acquisition of Information Assurance: “Encryption products for protecting classified information will be certified by NSA, and encryption products intended for protecting sensitive information will be certified in accordance with NIST FIPS 140-2.”
The Government of Canada also recommends the use of FIPS 140 validated cryptographic modules in unclassified applications of its departments.
Although NIST publication 197 (“FIPS 197”) is the unique document that covers the AES algorithm, vendors typically approach the CMVP under FIPS 140 and ask to have several algorithms (such as Triple DES or SHA1) validated at the same time. Therefore, it is rare to find cryptographic modules that are uniquely FIPS 197 validated and NIST itself does not generally take the time to list FIPS 197 validated modules separately on its public web site. Instead, FIPS 197 validation is typically just listed as an "FIPS approved: AES" notation (with a specific FIPS 197 certificate number) in the current list of FIPS 140 validated cryptographic modules.
The Cryptographic Algorithm Validation Program (CAVP) allows for independent validation of the correct implementation of the AES algorithm. Successful validation results in being listed on the NIST validations page. This testing is a pre-requisite for the FIPS 140-2 module validation described below. However, successful CAVP validation in no way implies that the cryptographic module implementing the algorithm is secure. A cryptographic module lacking FIPS 140-2 validation or specific approval by the NSA is not deemed secure by the US Government and cannot be used to protect government data.
FIPS 140-2 validation is challenging to achieve both technically and fiscally. There is a standardized battery of tests as well as an element of source code review that must be passed over a period of a few weeks. The cost to perform these tests through an approved laboratory can be significant (e.g., well over $30,000 US) and does not include the time it takes to write, test, document and prepare a module for validation. After validation, modules must be re-submitted and re-evaluated if they are changed in any way. This can vary from simple paperwork updates if the security functionality did not change to a more substantial set of re-testing if the security functionality was impacted by the change.
## Test vectors.
Test vectors are a set of known ciphers for a given input and key. NIST distributes the reference of AES test vectors as AES Known Answer Test (KAT) Vectors.
## Performance.
High speed and low RAM requirements were criteria of the AES selection process. As the chosen algorithm, AES performed well on a wide variety of hardware, from 8-bit smart cards to high-performance computers.
On a Pentium Pro, AES encryption requires 18 clock cycles per byte, equivalent to a throughput of about 11 MiB/s for a 200 MHz processor.
On Intel Core and AMD Ryzen CPUs supporting AES-NI instruction set extensions, throughput can be multiple GB/s (even over 10 GB/s).
Where the CPU hardware does not support AES acceleration, ChaCha is an alternative cipher with better performance and without sacrificing security.

</doc>
<doc id="1261" url="https://en.wikipedia.org/wiki?curid=1261" title="April 26">
April 26



</doc>
<doc id="1262" url="https://en.wikipedia.org/wiki?curid=1262" title="Argot">
Argot



</doc>
<doc id="1264" url="https://en.wikipedia.org/wiki?curid=1264" title="Anisotropy">
Anisotropy

Anisotropy () is the property of a material which allows it to change or assume different properties in different directions as opposed to isotropy. It can be defined as a difference, when measured along different axes, in a material's physical or mechanical properties (absorbance, refractive index, conductivity, tensile strength, etc.)
An example of anisotropy is light coming through a polarizer. Another is wood, which is easier to split along its grain than across it.
## Fields of interest.
### Computer graphics.
In the field of computer graphics, an anisotropic surface changes in appearance as it rotates about its geometric normal, as is the case with velvet.
Anisotropic filtering (AF) is a method of enhancing the image quality of textures on surfaces that are far away and steeply angled with respect to the point of view. Older techniques, such as bilinear and trilinear filtering, do not take into account the angle a surface is viewed from, which can result in aliasing or blurring of textures. By reducing detail in one direction more than another, these effects can be reduced.
### Chemistry.
A chemical anisotropic filter, as used to filter particles, is a filter with increasingly smaller interstitial spaces in the direction of filtration so that the proximal regions filter out larger particles and distal regions increasingly remove smaller particles, resulting in greater flow-through and more efficient filtration.
In NMR spectroscopy, the orientation of nuclei with respect to the applied magnetic field determines their chemical shift. In this context, anisotropic systems refer to the electron distribution of molecules with abnormally high electron density, like the pi system of benzene. This abnormal electron density affects the applied magnetic field and causes the observed chemical shift to change.
In fluorescence spectroscopy, the fluorescence anisotropy, calculated from the polarization properties of fluorescence from samples excited with plane-polarized light, is used, e.g., to determine the shape of a macromolecule.
Anisotropy measurements reveal the average angular displacement of the fluorophore that occurs between absorption and subsequent emission of a photon.
### Real-world imagery.
Images of a gravity-bound or man-made environment are particularly anisotropic in the orientation domain, with more image structure located at orientations parallel with or orthogonal to the direction of gravity (vertical and horizontal).
### Physics.
Physicists from University of California, Berkeley reported about their detection of the cosine anisotropy in cosmic microwave background radiation in 1977. Their experiment demonstrated the Doppler shift caused by the movement of the earth with respect to the early Universe matter, the source of the radiation. Cosmic anisotropy has also been seen in the alignment of galaxies' rotation axes and polarisation angles of quasars.
Physicists use the term anisotropy to describe direction-dependent properties of materials. Magnetic anisotropy, for example, may occur in a plasma, so that its magnetic field is oriented in a preferred direction. Plasmas may also show "filamentation" (such as that seen in lightning or a plasma globe) that is directional.
An "anisotropic liquid" has the fluidity of a normal liquid, but has an average structural order relative to each other along the molecular axis, unlike water or chloroform, which contain no structural ordering of the molecules. Liquid crystals are examples of anisotropic liquids.
Some materials conduct heat in a way that is isotropic, that is independent of spatial orientation around the heat source. Heat conduction is more commonly anisotropic, which implies that detailed geometric modeling of typically diverse materials being thermally managed is required. The materials used to transfer and reject heat from the heat source in electronics are often anisotropic.
Many crystals are anisotropic to light ("optical anisotropy"), and exhibit properties such as birefringence. Crystal optics describes light propagation in these media. An "axis of anisotropy" is defined as the axis along which isotropy is broken (or an axis of symmetry, such as normal to crystalline layers). Some materials can have multiple such optical axes.
### Geophysics and geology.
Seismic anisotropy is the variation of seismic wavespeed with direction. Seismic anisotropy is an indicator of long range order in a material, where features smaller than the seismic wavelength (e.g., crystals, cracks, pores, layers or inclusions) have a dominant alignment. This alignment leads to a directional variation of elasticity wavespeed. Measuring the effects of anisotropy in seismic data can provide important information about processes and mineralogy in the Earth; significant seismic anisotropy has been detected in the Earth's crust, mantle and inner core.
Geological formations with distinct layers of sedimentary material can exhibit electrical anisotropy; electrical conductivity in one direction (e.g. parallel to a layer), is different from that in another (e.g. perpendicular to a layer). This property is used in the gas and oil exploration industry to identify hydrocarbon-bearing sands in sequences of sand and shale. Sand-bearing hydrocarbon assets have high resistivity (low conductivity), whereas shales have lower resistivity. Formation evaluation instruments measure this conductivity/resistivity and the results are used to help find oil and gas in wells. The mechanical anisotropy measured for some of the sedimentary rocks like coal and shale can change with corresponding changes in their surface properties like sorption when gases are produced from the coal and shale reservoirs.
The hydraulic conductivity of aquifers is often anisotropic for the same reason. When calculating groundwater flow to drains or to wells, the difference between horizontal and vertical permeability must be taken into account, otherwise the results may be subject to error.
Most common rock-forming minerals are anisotropic, including quartz and feldspar. Anisotropy in minerals is most reliably seen in their optical properties. An example of an isotropic mineral is garnet.
### Medical acoustics.
Anisotropy is also a well-known property in medical ultrasound imaging describing a different resulting echogenicity of soft tissues, such as tendons, when the angle of the transducer is changed. Tendon fibers appear hyperechoic (bright) when the transducer is perpendicular to the tendon, but can appear hypoechoic (darker) when the transducer is angled obliquely. This can be a source of interpretation error for inexperienced practitioners.
### Materials science and engineering.
Anisotropy, in materials science, is a material's directional dependence of a physical property. This is a critical consideration for materials selection in engineering applications. A material with physical properties that are symmetric about an axis that is normal to a plane of isotropy is called a transversely isotropic material. Tensor descriptions of material properties can be used to determine the directional dependence of that property. For a monocrystalline material, anisotropy is associated with the crystal symmetry in the sense that more symmetric crystal types have fewer independent coefficients in the tensor description of a given property. When a material is polycrystalline, the directional dependence on properties is often related to the processing techniques it has undergone. A material with randomly oriented grains will be isotropic, whereas materials with texture will be often be anisotropic. Textured materials are often the result of processing techniques like hot rolling, wire-drawing, and heat treatment.
Mechanical properties of materials such as Young's modulus, ductility, yield strength, and high temperature creep rate, are often dependent on the direction of measurement. Fourth rank tensor properties, like the elastic constants, are anisotropic, even for materials with cubic symmetry. The Young's modulus relates stress and strain when an isotropic material is elastically deformed; to describe elasticity in an anisotropic material, stiffness (or compliance) tensors are used instead. 
In metals, anisotropic elasticity behavior is present in all single crystals with three independent coefficients for cubic crystals, for example. For face centered cubic materials such as Nickel and Copper, the stiffness is highest along the &lt;111&gt; direction, normal to the close packed planes, and smallest parallel to &lt;100&gt;. Tungsten is so nearly isotropic at room temperature that it can be considered to have only two stiffness coefficients; Aluminum is another metal that is nearly isotropic.
For an isotropic material, 
formula_1, 
where formula_2 is the shear modulus, formula_3 is the Young's modulus, and formula_4 is the material's Poisson's ratio. Therefore, for cubic materials, we can think of anisotropy, formula_5, as the ratio between the empirically determined shear modulus for the cubic material and its (isotropic) equivalent:
formula_6
The latter expression is known as the Zener ratio, formula_5, where formula_8 refers to Elastic constants in Voigt (vector-matrix) notation. For an isotropic material, the ratio is one. 
Fiber-reinforced or layered composite materials exhibit anisotropic mechanical properties, due to orientation of the reinforcement material. In many fiber-reinforced composites like carbon fiber or glass fiber based composites, the weave of the material (e.g. unidirectional or plain weave) can determine the extent of the anisotropy of the bulk material. The tunability of orientation of the fibers, allows for application-based designs of composite materials, depending on the direction of stresses applied onto the material.
Amorphous materials such as glass and polymers are typically isotropic. Due to the highly randomized orientation of macromolecules in polymeric materials, polymers are in general described as isotropic. However, polymers can be engineered to have directionally dependent properties through processing techniques or introduction of anisotropy-inducing elements. Researchers have built composite materials with aligned fibers and voids to generate anisotropic hydrogels, in order to mimic hierarchically ordered biological soft matter. 3D printing, especially Fused Deposition Modeling, can introduce anisotropy into printed parts. This is due to the fact that FDM is designed to extrude and print layers of thermoplastic materials. This creates materials that are strong when tensile stress is applied in parallel to the layers and weak when the material is perpendicular to the layers.
### Microfabrication.
Anisotropic etching techniques (such as deep reactive ion etching) are used in microfabrication processes to create well defined microscopic features with a high aspect ratio. These features are commonly used in MEMS and microfluidic devices, where the anisotropy of the features is needed to impart desired optical, electrical, or physical properties to the device. Anisotropic etching can also refer to certain chemical etchants used to etch a certain material preferentially over certain crystallographic planes (e.g., KOH etching of silicon [100] produces pyramid-like structures)
### Neuroscience.
Diffusion tensor imaging is an MRI technique that involves measuring the fractional anisotropy of the random motion (Brownian motion) of water molecules in the brain. Water molecules located in fiber tracts are more likely to be anisotropic, since they are restricted in their movement (they move more in the dimension parallel to the fiber tract rather than in the two dimensions orthogonal to it), whereas water molecules dispersed in the rest of the brain have less restricted movement and therefore display more isotropy. This difference in fractional anisotropy is exploited to create a map of the fiber tracts in the brains of the individual.
### Atmospheric radiative transfer.
Radiance fields (see BRDF) from a reflective surface are often not isotropic in nature. This makes calculations of the total energy being reflected from any scene a difficult quantity to calculate. In remote sensing applications, anisotropy functions can be derived for specific scenes, immensely simplifying the calculation of the net reflectance or (thereby) the net irradiance of a scene.
For example, let the BRDF be formula_9 where 'i' denotes incident direction and 'v' denotes viewing direction (as if from a satellite or other instrument). And let P be the Planar Albedo, which represents the total reflectance from the scene.
It is of interest because, with knowledge of the anisotropy function as defined, a measurement of the BRDF from a single viewing direction (say, formula_12) yields a measure of the total scene reflectance (Planar Albedo) for that specific incident geometry (say, formula_13).

</doc>
<doc id="1267" url="https://en.wikipedia.org/wiki?curid=1267" title="Alpha decay">
Alpha decay

Alpha decay or α-decay is a type of radioactive decay in which an atomic nucleus emits an alpha particle (helium nucleus) and thereby transforms or 'decays' into a different atomic nucleus, with a mass number that is reduced by four and an atomic number that is reduced by two. An alpha particle is identical to the nucleus of a helium-4 atom, which consists of two protons and two neutrons. It has a charge of and a mass of . For example, uranium-238 decays to form thorium-234.
Alpha particles have a charge , but as a nuclear equation describes a nuclear reaction without considering the electrons – a convention that does not imply that the nuclei necessarily occur in neutral atoms – the charge is not usually shown.
Alpha decay typically occurs in the heaviest nuclides. Theoretically, it can occur only in nuclei somewhat heavier than nickel (element 28), where the overall binding energy per nucleon is no longer a maximum and the nuclides are therefore unstable toward spontaneous fission-type processes. In practice, this mode of decay has only been observed in nuclides considerably heavier than nickel, with the lightest known alpha emitters being the lightest isotopes (mass numbers 104–109) of tellurium (element 52). Exceptionally, however, beryllium-8 decays to two alpha particles.
Alpha decay is by far the most common form of cluster decay, where the parent atom ejects a defined daughter collection of nucleons, leaving another defined product behind. It is the most common form because of the combined extremely high nuclear binding energy and a relatively small mass of the alpha particle. Like other cluster decays, alpha decay is fundamentally a quantum tunneling process. Unlike beta decay, it is governed by the interplay between both the strong nuclear force and the electromagnetic force.
Alpha particles have a typical kinetic energy of 5 MeV (or ≈ 0.13% of their total energy, 110 TJ/kg) and have a speed of about 15,000,000 m/s, or 5% of the speed of light. There is surprisingly small variation around this energy, due to the heavy dependence of the half-life of this process on the energy produced. Because of their relatively large mass, the electric charge of and relatively low velocity, alpha particles are very likely to interact with other atoms and lose their energy, and their forward motion can be stopped by a few centimeters of air.
Approximately 99% of the helium produced on Earth is the result of the alpha decay of underground deposits of minerals containing uranium or thorium. The helium is brought to the surface as a by-product of natural gas production.
## History.
Alpha particles were first described in the investigations of radioactivity by Ernest Rutherford in 1899, and by 1907 they were identified as He2+ ions.
By 1928, George Gamow had solved the theory of alpha decay via tunneling. The alpha particle is trapped inside the nucleus by an attractive nuclear potential well 
and a repulsive electromagnetic potential barrier. Classically, it is forbidden to escape, but according to the (then) newly discovered principles of quantum mechanics, it has a tiny (but non-zero) probability of "tunneling" through the barrier and appearing on the other side to escape the nucleus. Gamow solved a model potential for the nucleus and derived, from first principles, a relationship between the half-life of the decay, and the energy of the emission, which had been previously discovered empirically, and was known as the Geiger–Nuttall law.
## Mechanism.
The nuclear force holding an atomic nucleus together is very strong, in general much stronger than the repulsive electromagnetic forces between the protons. However, the nuclear force is also short-range, dropping quickly in strength beyond about 1 femtometer, while the electromagnetic force has an unlimited range. The strength of the attractive nuclear force keeping a nucleus together is thus proportional to the number of nucleons, but the total disruptive electromagnetic force trying to break the nucleus apart is roughly proportional to the square of its atomic number. A nucleus with 210 or more nucleons is so large that the strong nuclear force holding it together can just barely counterbalance the electromagnetic repulsion between the protons it contains. Alpha decay occurs in such nuclei as a means of increasing stability by reducing size.
One curiosity is why alpha particles, helium nuclei, should be preferentially emitted as opposed to other particles like a single proton or neutron or other atomic nuclei. Part of the reason is the high binding energy of the alpha particle, which means that its mass is less than the sum of the masses of two protons and two neutrons. This increases the disintegration energy. Computing the total disintegration energy given by the equation
formula_1
where is the initial mass of the nucleus, is the mass of the nucleus after particle emission, and is the mass of the emitted particle, one finds that in certain cases it is positive and so alpha particle emission is possible, whereas other decay modes would require energy to be added. For example, performing the calculation for uranium-232 shows that alpha particle emission gives 5.4 MeV of energy, while a single proton emission would "require" 6.1 MeV. Most of the disintegration energy becomes the kinetic energy of the alpha particle itself, although to maintain conservation of momentum part of the energy goes to the recoil of the nucleus itself (see Atomic recoil). However, since the mass numbers of most alpha-emitting radioisotopes exceed 210, far greater than the mass number of the alpha particle (4) the fraction of the energy going to the recoil of the nucleus is generally quite small, less than 2%, however the recoil energy (on the scale of keV) is still much larger than the strength of chemical bonds (on the scale of eV), so the daughter nuclide will break away from the chemical environment the parent was in. The energies and ratios of the alpha particles can be used to identify the radioactive parent via alpha spectrometry.
These disintegration energies, however, are substantially smaller than the repulsive potential barrier created by the electromagnetic force, which prevents the alpha particle from escaping. The energy needed to bring an alpha particle from infinity to a point near the nucleus just outside the range of the nuclear force's influence is generally in the range of about 25 MeV. An alpha particle can be thought of as being inside a potential barrier whose walls are 25 MeV above the potential at infinity. However, decay alpha particles only have energies of around 4 to 9 MeV above the potential at infinity, far less than the energy needed to escape.
Quantum mechanics, however, allows the alpha particle to escape via quantum tunneling. The quantum tunneling theory of alpha decay, independently developed by George Gamow and Ronald Wilfred Gurney and Edward Condon in 1928, was hailed as a very striking confirmation of quantum theory. Essentially, the alpha particle escapes from the nucleus not by acquiring enough energy to pass over the wall confining it, but by tunneling through the wall. Gurney and Condon made the following observation in their paper on it:
It has hitherto been necessary to postulate some special arbitrary 'instability' of the nucleus, but in the following note, it is pointed out that disintegration is a natural consequence of the laws of quantum mechanics without any special hypothesis... Much has been written of the explosive violence with which the α-particle is hurled from its place in the nucleus. But from the process pictured above, one would rather say that the α-particle almost slips away unnoticed.
The theory supposes that the alpha particle can be considered an independent particle within a nucleus, that is in constant motion but held within the nucleus by strong interaction. At each collision with the repulsive potential barrier of the electromagnetic force, there is a small non-zero probability that it will tunnel its way out. An alpha particle with a speed of 1.5×107 m/s within a nuclear diameter of approximately 10−14 m will collide with the barrier more than 1021 times per second. However, if the probability of escape at each collision is very small, the half-life of the radioisotope will be very long, since it is the time required for the total probability of escape to reach 50%. As an extreme example, the half-life of the isotope bismuth-209 is .
The isotopes in beta-decay stable isobars that are also stable with regards to double beta decay with mass number "A" = 5, "A" = 8, 143 ≤ "A" ≤ 155, 160 ≤ "A" ≤ 162, and "A" ≥ 165 are theorized to undergo alpha decay. All other mass numbers (isobars) have exactly one theoretically stable nuclide). Those with mass 5 decay to helium-4 and a proton or a neutron, and those with mass 8 decay to two helium-4 nuclei; their half-lives (helium-5, lithium-5, and beryllium-8) are very short, unlike the half-lives for all other such nuclides with "A" ≤ 209, which are very long. (Such nuclides with "A" ≤ 209 are primordial nuclides except 146Sm.)
Working out the details of the theory leads to an equation relating the half-life of a radioisotope to the decay energy of its alpha particles, a theoretical derivation of the empirical Geiger–Nuttall law.
## Uses.
Americium-241, an alpha emitter, is used in smoke detectors. The alpha particles ionize air in an open ion chamber and a small current flows through the ionized air. Smoke particles from the fire that enter the chamber reduce the current, triggering the smoke detector's alarm.
Radium-223 is also an alpha emitter. It is used in the treatment of skeletal metastases (cancers in the bones).
Alpha decay can provide a safe power source for radioisotope thermoelectric generators used for space probes and were used for artificial heart pacemakers. Alpha decay is much more easily shielded against than other forms of radioactive decay.
Static eliminators typically use polonium-210, an alpha emitter, to ionize the air, allowing the 'static cling' to dissipate more rapidly.
## Toxicity.
Highly charged and heavy, alpha particles lose their several MeV of energy within a small volume of material, along with a very short mean free path. This increases the chance of double-strand breaks to the DNA in cases of internal contamination, when ingested, inhaled, injected or introduced through the skin. Otherwise, touching an alpha source is typically not harmful, as alpha particles are effectively shielded by a few centimeters of air, a piece of paper, or the thin layer of dead skin cells that make up the epidermis; however, many alpha sources are also accompanied by beta-emitting radio daughters, and both are often accompanied by gamma photon emission.
Relative biological effectiveness (RBE) quantifies the ability of radiation to cause certain biological effects, notably either cancer or cell-death, for equivalent radiation exposure. Alpha radiation has a high linear energy transfer (LET) coefficient, which is about one ionization of a molecule/atom for every angstrom of travel by the alpha particle. The RBE has been set at the value of 20 for alpha radiation by various government regulations. The RBE is set at 10 for neutron irradiation, and at 1 for beta radiation and ionizing photons.
However, the recoil of the parent nucleus (alpha recoil) gives it a significant amount of energy, which also causes ionization damage (see ionizing radiation). This energy is roughly the weight of the alpha (4 u) divided by the weight of the parent (typically about 200 u) times the total energy of the alpha. By some estimates, this might account for most of the internal radiation damage, as the recoil nucleus is part of an atom that is much larger than an alpha particle, and causes a very dense trail of ionization; the atom is typically a heavy metal, which preferentially collect on the chromosomes. In some studies, this has resulted in an RBE approaching 1,000 instead of the value used in governmental regulations.
The largest natural contributor to public radiation dose is radon, a naturally occurring, radioactive gas found in soil and rock. If the gas is inhaled, some of the radon particles may attach to the inner lining of the lung. These particles continue to decay, emitting alpha particles, which can damage cells in the lung tissue. The death of Marie Curie at age 66 from aplastic anemia was probably caused by prolonged exposure to high doses of ionizing radiation, but it is not clear if this was due to alpha radiation or X-rays. Curie worked extensively with radium, which decays into radon, along with other radioactive materials that emit beta and gamma rays. However, Curie also worked with unshielded X-ray tubes during World War I, and analysis of her skeleton during a reburial showed a relatively low level of radioisotope burden.
The Russian dissident Alexander Litvinenko's 2006 murder by radiation poisoning is thought to have been carried out with polonium-210, an alpha emitter.

</doc>
<doc id="1268" url="https://en.wikipedia.org/wiki?curid=1268" title="AI">
AI



</doc>
<doc id="1270" url="https://en.wikipedia.org/wiki?curid=1270" title="Extreme poverty">
Extreme poverty

Extreme poverty, deep poverty, abject poverty, absolute poverty, destitution, or penury, is the most severe type of poverty, defined by the United Nations (UN) as "a condition characterized by severe deprivation of basic human needs, including food, safe drinking water, sanitation facilities, health, shelter, education and information. It depends not only on income but also on access to services" (UN 1995 report of the World Summit for Social Development). Historically, other definitions have been proposed within the United Nations.
In 2018, extreme poverty mainly refers to an income below the international poverty line of $1.90 per day (in 2011 prices, ), set by the World Bank. In October 2017, the World Bank updated the international poverty line, a global absolute minimum, to $1.90 a day. This is the equivalent of $1.00 a day in 1996 US prices, hence the widely used expression "living on less than a dollar a day". The vast majority of those in extreme poverty reside in South Asia and Sub-Saharan Africa. As of 2018, it is estimated that the country with the most people living in extreme poverty is Nigeria, at 86 million.
In the past, the vast majority of the world population lived in conditions of extreme poverty. 
The percentage of the global population living in absolute poverty fell from over 80% in 1800 to under 20% by 2015. According to UN estimates, roughly 734 million people or 10% remained under those conditions. The number had previously been measured as 1.9 billion in 1990, and 1.2 billion in 2008. Despite the significant number of individuals still below the international poverty line, these figures represent significant progress for the international community, as they reflect a decrease of more than one billion people over 15 years.
In public opinion surveys around the globe, people surveyed tend to think that extreme poverty has not decreased.
The reduction of extreme poverty and hunger was the first Millennium Development Goal (MDG1), as set by the United Nations in 2000. Specifically, the target was to reduce the extreme poverty rate by half by 2015, a goal that was met five years ahead of schedule. In the Sustainable Development Goals, which succeeded the MDGs, the goal is to end extreme poverty in all its forms everywhere. With this declaration the international community, including the UN and the World Bank have adopted the target of ending extreme poverty by 2030.
## Definition.
### Previous definitions.
In July 1993, Leandro Despouy, the then UN Special Rapporteur on extreme poverty and human rights made use of a definition he adapted from a 1987 report to the French Economic and Social Council by Fr. Joseph Wresinski, founder of the International Movement ATD Fourth World, distinguishing "lack of basic security" (poverty) and "chronic poverty" (extreme poverty), linking the eradication of extreme poverty by allowing people currently experiencing it a real opportunity to exercise all their human rights:
"The lack of basic security connotes the absence of one or more factors enabling individuals and families to assume basic responsibilities and to enjoy fundamental rights. The situation may become widespread and result in more serious and permanent consequences. The lack of basic security leads to chronic poverty when it simultaneously affects several aspects of people’s lives, when it is prolonged and when it severely compromises people’s chances of regaining their rights and of reassuming their responsibilities in the foreseeable future."
This definition was mentioned previously, in June 1989, in the preliminary report on the realization of economic, social and cultural rights by the UN Special Rapporteur Danilo Türk. It is still in use today, among others, in the current UN Guiding Principles on Extreme Poverty and Human Rights adopted by the UN Human Rights Council in September 2012
### Consumption-based definition.
Extreme poverty is defined by the international community as living below $1.90 a day, as measured in 2011 international prices (equivalent to $2.12 in 2018). This number, also known as the international poverty line, is periodically updated to account for inflation and differences in the cost of living; it was originally defined at $1.00 a day in 1996. The updates are made according to new price data to portray the costs of basic food, health services, clothing, and shelter around the world as accurately as possible. The latest revision was made in 2015 when the World Bank increased the line to international-$1.90.
Because many of the world's poorest people do not have a monetary income, the poverty measurement is based on the monetary value of a person's "consumption". Otherwise the poverty measurement would be missing the home production of subsistence farmers that consume largely their own production.
### Alternative definitions.
The $1.90/day extreme poverty line remains the most widely used metric as it highlights the reality of those in the most severe conditions. Although widely used by most international organizations, it has come under scrutiny due to a variety of factors. For example, it does not account for how far below the line people are, referred to as the depth of poverty. For this purpose, the same institutions publish data on the poverty gap.
The international poverty line is designed to stay constant over time, to allow comparisons between different years. It is therefore a measure of absolute poverty and is not measuring relative poverty. It is also not designed to capture how people view their own financial situation (known as the socially subjective poverty line). Moreover, the calculation of the poverty line relies on information about consumer prices to calculate purchasing power parity, which are very hard to measure and are necessarily debatable. As with all other metrics, there may also be missing data from the poorest and most fragile countries.
Several alternative instruments for measuring extreme poverty have been suggested which incorporate other factors such as malnutrition and lack of access to a basic education. The Multidimensional Poverty Index (MPI), based on the Alkire-Foster Method, is published by the Oxford Poverty &amp; Human Development Initiative (OPHI): it measures deprivation in basic needs and can be broken down to reflect both the incidence and the intensity of poverty. For example, under conventional measures, in both Ethiopia and Uzbekistan about 40% of the population is considered extremely poor, but based on the MPI, 90% of Ethiopians but only 2% of Uzbekistanis are in multidimensional poverty.
The MPI is useful for development officials to determine the most likely causes of poverty within a region, using the M0 measure of the method (which is calculated by multiplying the fraction of people in poverty by the fraction of dimensions they are deprived in). For example, in the Gaza Strip of Palestine, using the M0 measure of the Alkire-Foster method reveals that poverty in the region is primarily caused by a lack of access to electricity, lack of access to drinking water, and widespread overcrowding. In contrast, data from the Chhukha District of Bhutan reveals that income is a much larger contributor to poverty as opposed to other dimensions within the region. However, the MPI only presents data from 105 countries, so it cannot be used for global measurements.
## Current trends.
### Getting to zero.
Using the World Bank definition of $1.90/day, , roughly 710 million people remained in extreme poverty (or roughly 1 in 10 people worldwide). Nearly half of them live in India and China, with more than 85% living in just 20 countries. Since the mid-1990s, there has been a steady decline in both the worldwide poverty rate and the total number of extreme poor. In 1990, the percentage of the global population living in extreme poverty was 43%, but in 2011, that percentage had dropped down to 21%. This halving of the extreme poverty rate falls in line with the first Millennium Development Goal (MDG1) proposed by former UN Secretary-General Kofi Annan, who called on the international community at the turn of the century to reduce the percentage of people in extreme poverty by half by 2015.
This reduction in extreme poverty took place most notably in China, Indonesia, India, Pakistan and Vietnam. These five countries accounted for the alleviation of 715 million people out of extreme poverty between 1990 and 2010 – more than the global net total of roughly 700 million. This statistical oddity can be explained by the fact that the number of people living in extreme poverty in Sub-Saharan Africa rose from 290 million to 414 million over the same period. However, there have been many positive signs for extensive, global poverty reduction as well. Since 1999, the total number of extreme poor has declined by an average of 50 million per year. Moreover, in 2005, for the first time in recorded history, poverty rates began to fall in every region of the world, including Africa.
As aforementioned, the number of people living in extreme poverty has reduced from 1.9 billion to 766 million over the span of the last decades. If we remain on our current trajectory, many economists predict we could reach global zero by 2030–2035, thus ending extreme poverty. Global zero entails a world in which fewer than 3% of the global population lives in extreme poverty (projected under most optimistic scenarios to be fewer than 200 million people). This zero figure is set at 3% in recognition of the fact that some amount of frictional (temporary) poverty will continue to exist, whether it is caused by political conflict or unexpected economic fluctuations, at least for the foreseeable future. However, the Brookings Institution notes that any projection about poverty more than a few years into the future runs the risk of being highly uncertain. This is because changes in consumption and distribution throughout the developing world over the next two decades could result in monumental shifts in global poverty, for better or worse.
Others are more pessimistic about this possibility, predicting a range of 193 million to 660 million people still living in extreme poverty by 2035. Additionally, some believe the rate of poverty reduction will slow down in the developing world, especially in Africa, and as such it will take closer to five decades to reach global zero. Despite these reservations, several prominent international and national organizations, including the UN, the World Bank and the United States Federal Government (via USAID), have set a target of reaching global zero by the end of 2030.
### Exacerbating factors.
There are a variety of factors that may reinforce or instigate the existence of extreme poverty, such as weak institutions, cycles of violence and a low level of growth. Recent World Bank research shows that some countries can get caught in a "fragility trap", in which self-reinforcing factors prevent the poorest nations from emerging from low-level equilibrium in the long run. Moreover, most of the reduction in extreme poverty over the past twenty years has taken place in countries that have not experienced a civil conflict or have had governing institutions with a strong capacity to actually govern. Thus, to end extreme poverty, it is also important to focus on the interrelated problems of fragility and conflict.
USAID defines fragility as a government's lack of both legitimacy (the perception the government is adequate at doing its job) and effectiveness (how good the government is at maintaining law and order, in an equitable manner). As fragile nations are unable to equitably and effectively perform the functions of a state, these countries are much more prone to violent unrest and mass inequality. Additionally, in countries with high levels of inequality (a common problem in countries with inadequate governing institutions), much higher growth rates are needed to reduce the rate of poverty when compared with other nations. Additionally, if China and India are removed from the equation, up to 70% of the world's poor live in fragile states by some definitions of fragility. Some analysts project that extreme poverty will be increasingly concentrated in fragile, low-income states like Haiti, Yemen and the Central African Republic. However, some academics, such as Andy Sumner, say that extreme poverty will be increasingly concentrated in middle-income countries, creating a paradox where the world's poor do not actually live in the poorest countries.
To help low-income earners, fragile states make the transition towards peace and prosperity, the New Deal for Engagement in Fragile States, endorsed by roughly forty countries and multilateral institutions, was created in 2011. This represents an important step towards redressing the problem of fragility as it was originally articulated by self-identified fragile states who called on the international community to not only "do things differently", but to also "do different things".
Civil conflict also remains a prime cause for the perpetuation of poverty throughout the developing world. Armed conflict can have severe effects on economic growth for many reasons such as the destruction of assets, destruction of livelihoods, creation of unwanted mass migration, and diversion of public resources towards war. Significantly, a country that experienced major violence during 1981–2005 had extreme poverty rates 21 percentage points higher than a country with no violence. On average, each civil conflict will cost a country roughly 30 years of GDP growth. Therefore, a renewed commitment from the international community to address the deteriorating situation in highly fragile states is necessary to both prevent the mass loss of life, but to also prevent the vicious cycle of extreme poverty.
Population trends and dynamics (e.g. population growth) can also have a large impact on prospects for poverty reduction. According to the United Nations, "in addition to improving general health and well-being, analysis shows that meeting the reproductive health and contraceptive needs of all women in the developing world more than pays for itself").
In 2013, a prevalent finding in a report by the World Bank was that extreme poverty is most prevalent in low-income countries. In these countries, the World Bank found that progress in poverty reduction is the slowest, the poor live under the worst conditions, and the most affected persons are children age 12 and under.
## International initiatives.
### Millennium Summit and Millennium Development Goals.
In September 2000, world leaders gathered at the Millennium Summit held in New York, launching the United Nations Millennium Project suggested by then UN Secretary-General Kofi Annan. Prior to the launch of the conference, the office of Secretary-General Annan released a report entitled "We The Peoples: The Role of the United Nations in the 21st Century". In this document, now widely known as the Millennium Report, Kofi Annan called on the international community to reduce the proportion of people in extreme poverty by half by 2015, a target that would affect over 1 billion people. Citing the close correlation between economic growth and the reduction of poverty in poor countries, Annan urged international leaders to indiscriminately target the problem of extreme poverty across every region. In charge of managing the project was Jeffrey Sachs, a noted development economist, who in 2005 released a plan for action called "Investing in Development: A Practical Plan to Achieve the Millennium Development Goals". Thomas Pogge criticized the 2000 Millennium Declaration for being less ambitious than a previous declaration from the World Food Summit due to using 1990 as the benchmark rather than 1996.
Overall, there has been significant progress towards reducing extreme poverty, with the MDG1 target of reducing extreme poverty rates by half being met five years early, representing 700 million people being lifted out of extreme poverty from 1990 to 2010, with 1.2 billion people still remaining under those conditions. The notable exception to this trend was in Sub-Saharan Africa, the only region where the number of people living in extreme poverty rose from 290 million in 1990 to 414 million in 2010, comprising more than a third of those living in extreme poverty worldwide.
#### 2005 World Summit.
The 2005 World Summit, held in September which was organized to measure international progress towards fulfilling the Millennium Development Goals (MDGs). Notably, the conference brought together more than 170 Heads of State. While world leaders at the summit were encouraged by the reduction of poverty in some nations, they were concerned by the uneven decline of poverty within and among different regions of the globe. However, at the end of the summit, the conference attendees reaffirmed the UN's commitment to achieve the MDGs by 2015 and urged all supranational, national and non-governmental organizations to follow suit.
### Sustainable Development Goals.
As the expiration of the Millennium Development Goals approached in 2015, the UN convened a panel to advise on a Post-2015 Development Agenda, which led to a new set of 17 goals for 2030 titled the Sustainable Development Goals (SDGs). The first goal (SDG 1) is to "End poverty in all its forms everywhere."
The HLP report, entitled A New Global Partnership: Eradicate Poverty and Transform Economies Through Sustainable Development, was published in May 2013. In the report, the HLP wrote that:
 Ending extreme poverty is just the beginning, not the end. It is vital, but our vision must be broader: to start countries on the path of sustainable development – building on the foundations established by the 2012 UN Conference on Sustainable Development in Rio de Janeiro12, and meeting a challenge that no country, developed or developing, has met so far. We recommend to the Secretary-General that deliberations on a new development agenda must be guided by the vision of eradicating extreme poverty once and for all, in the context of sustainable development.
Therefore, the report determined that a central goal of the Post-Millennium Development agenda is to eradicate extreme poverty by 2030. However, the report also emphasized that the MDGs were not enough on their own, as they did not "focus on the devastating effects of conflict and violence on development ... the importance to development of good governance and institution ... nor the need for inclusive growth..." Consequently, there now exists synergy between the policy position papers put forward by the United States (through USAID), the World Bank and the UN itself in terms of viewing fragility and a lack of good governance as exacerbating extreme poverty. However, in a departure from the views of other organizations, the commission also proposed that the UN focus not only on extreme poverty (a line drawn at $1.25), but also on a higher target, such as $2. The report notes this change could be made to reflect the fact that escaping extreme poverty is only a first step.
In addition to the UN, a host of other supranational and national actors such as the European Union and the African Union have published their own positions or recommendations on what should be incorporated in the Post-2015 agenda. The European Commission's communication, published in A decent Life for all: from vision to collective action, affirmed the UN's commitment to "eradicate extreme poverty in our lifetime and put the world on a sustainable path to ensure a decent life for all by 2030". A unique vision of the report was the commission's environmental focus (in addition to a plethora of other goals such as combating hunger and gender inequality). Specifically, the Commission argued, "long-term poverty reduction ... requires inclusive and sustainable growth. Growth should create decent jobs, take place with resource efficiency and within planetary boundaries, and should support efforts to mitigate climate change." The African Union's report, entitled Common African Position (CAP) on the Post-2015 Development Agenda, likewise encouraged the international community to focus on eradicating the twin problems of poverty and exclusion in our lifetime. Moreover, the CAP pledged that "no person – regardless of ethnicity, gender, geography, disability, race or other status – is denied universal human rights and basic economic opportunities".
### Least developed country conferences.
The UN least developed country (LDC) conferences were a series of summits organized by the UN to promote the substantial and even development of the world's least developed countries.
The first UN LDC Conference was held between 1 and 14 September 1981, in Paris, the first UN LDC Conference was organized to finalize the UN's "Substantial New Programme of Action" for the 1980s in Least Developed Countries. This program, which was unanimously adopted by the conference attendees, argued for internal reforms in LDCs (meant to encourage economic growth) to be complemented by strong international measures. However, despite the major economic and policy reforms initiated many of these LDCs, in addition to strong international aid, the economic situation of these countries worsened as a whole in the 1980s. This prompted the organization of a 2nd UN LDC conference almost a decade later.
The second UN LDC Conference was held between 3 and 14 September 1990, once again in Paris, the second UN LDC Conference was convened to measure the progress made by the LDCs towards fulfilling their development goals during the 1980s. Recognizing the problems that plagued the LDCs over the past decade, the conference formulated a new set of national and international policies to accelerate the growth rates of the poorest nations. These new principles were embodied in the "Paris Declaration and Programme of Action for the Least Developed Countries for the 1990s".
The fourth UN LDC Conference was the most recent conference. It was held in May 2011 in Istanbul, recognized that the nature of development had fundamentally changed since the 1st conference held almost 30 years earlier. In the 21st century, the capital flow into emerging economies has increasingly become dominated by foreign direct investment and remittances, as opposed to bilateral and multilateral assistance. Moreover, since the 1980s, significant structural changes have taken place on the international stage. With the creation of the G-20 conference of the largest economic powers, including many nations in the Global South, formerly undeveloped nations are now able to have a much larger say in international relations. Furthermore, the conference recognized that in the midst of a deep global recession, coupled with multiple crises (energy, climate, food, etc.), the international community would have fewer resources to aid the LDCs. Thus, the UN considered the participation of a wide range of stakeholders (not least the LDCs themselves), crucial to the formulation of the conference.
## Organizations working to end extreme poverty.
### International organizations.
#### World Bank.
In 2013, the Board of Governors of the World Bank Group (WBG) set two overriding goals for the WBG to commit itself to in the future. First, to end extreme poverty by 2030, an objective that echoes the sentiments of the UN and the Obama administration. Additionally, the WBG set an interim target of reducing extreme poverty to below 9% by 2020. Second, to focus on growth among the bottom 40% of people, as opposed to standard GDP growth. This commitment ensures that the growth of the developing world lifts people out of poverty, rather than exacerbating inequality.
As the World Bank's primary focus is on delivering economic growth to enable equitable prosperity, its developments programs are primarily commercial-based in nature, as opposed to the UN. Since the World Bank recognizes better jobs will result in higher income, and thus less poverty, the WBG seeks to support employment training initiatives, small business development programs and strong labor protection laws. However, since much of the growth in the developing world has been inequitable, the World Bank has also begun teaming with client states to map out trends in inequality and to propose public policy changes that can level the playing field.
Moreover, the World Bank engages in a variety of nutritional, transfer payments and transport-based initiatives. Children who experience under-nutrition from conception to two years of age have a much higher risk of physical and mental disability. Thus, they are often trapped in poverty and are unable to make a full contribution to the social and economic development of their communities as adults. The WBG estimates that as much as 3% of GDP can be lost as a result of under-nutrition among the poorest nations. To combat undernutrition, the WBG has partnered with UNICEF and the WHO to ensure all small children are fully fed. The WBG also offers conditional cash transfers to poor households who meet certain requirements such as maintaining children's healthcare or ensuring school attendance. Finally, the WBG understands investment in public transportation and better roads is key to breaking rural isolation, improving access to healthcare and providing better job opportunities for the World's poor.
#### United Nations.
The UN Office for the Coordination of Humanitarian Affairs (OCHA) works to synchronize the disparate international, national and non-governmental efforts to contest poverty. OCHA seeks to prevent "confusion" in relief operations and to ensure that the humanitarian response to disaster situations has greater accountability and predictability. To do so, OCHA has begun deploying Humanitarian Coordinators and Country Teams to provide a solid architecture for the international community to work through.
The United Nation's Children's Fund (UNICEF) was created by the UN to provide food, clothing and healthcare to European children facing famine and disease in the immediate aftermath of World War II. After the UN General Assembly extended UNICEF's mandate indefinitely in 1953, it actively worked to help children in extreme poverty in more than 190 countries and territories to overcome the obstacles that poverty, violence, disease and discrimination place in a child's path. Its current focus areas are 1) Child survival &amp; development 2) Basic education &amp; gender equality 3) Children and HIV/AIDS and 4) Child protection.
The UN Refugee Agency (UNHCR) is mandated to lead and coordinate international action to protect refugees worldwide. Its primary purpose is to safeguard the rights of refugees by ensuring anyone can exercise the right to seek asylum in another state, with the option to return home voluntarily, integrate locally or resettle in a third country. The UNHCR operates in over 125 countries, helping approximately 33.9 million persons.
The World Food Programme (WFP) is the largest agency dedicated to fighting hunger worldwide. On average, the WFP brings food assistance to more than 90 million people in 75 countries. The WFP not only strives to prevent hunger in the present, but also in the future by developing stronger communities which will make food even more secure on their own. The WFP has a range of expertise from Food Security Analysis, Nutrition, Food Procurement and Logistics.
The World Health Organization (WHO) is responsible for providing leadership on global health matters, shaping the health research agenda, articulating evidence-based policy decisions and combating diseases that are induced from poverty, such as HIV/AIDS, malaria and tuberculosis. Moreover, the WHO deals with pressing issues ranging from managing water safety, to dealing with maternal and newborn health.
### Bilateral organizations.
#### USAID.
The US Agency for International Development (USAID) is the lead US government agency dedicated to ending extreme poverty. Currently the largest bilateral donor in the world, the United States channels the majority of its development assistance through USAID and the US Department of State. In President Obama's 2013 State of the Union address, he declared, "So the United States will join with our allies to eradicate such extreme poverty in the next two decades ... which is within our reach." In response to Obama's call to action, USAID has made ending extreme poverty central to its mission statement. Under its New Model of Development, USAID seeks to eradicate extreme poverty through the use of innovation in science and technology, by putting a greater emphasis on evidence based decision-making, and through leveraging the ingenuity of the private sector and global citizens.
A major initiative of the Obama Administration is Power Africa, which aims to bring energy to 20 million people in Sub-Saharan Africa. By reaching out to its international partners, whether commercial or public, the US has leveraged over $14 billion in outside commitments after investing only US$7 billion of its own. To ensure that Power Africa reaches the region's poorest, the initiative engages in a transaction based approach to create systematic change. This includes expanding access to electricity to more than 20,000 additional households which already live without power.
In terms of specific programming, USAID works in a variety of fields from preventing hunger, reducing HIV/AIDS, providing general health assistance and democracy assistance, as well as dealing with gender issues. To deal with food security, which affects roughly 842 million people (who go to bed hungry each night), USAID coordinates the Feed the Future Initiative (FtF). FtF aims to reduce poverty and undernutrition each by 20% over five years. Because of the President's Emergency Plan for AIDS Relief (PEPFAR) and a variety of congruent actors, the incidence of AIDS and HIV, which used to ravage Africa, reduced in scope and intensity. Through PEPFAR, the United States has ensured over five million people have received life-saving antiviral drugs, a significant proportion of the eight million people receiving treatment in relatively poor nations.
In terms of general health assistance, USAID has worked to reduce maternal mortality by 30%, under-five child mortality by 35%, and has accomplished a host of other goals. USAID also supports the gamut of democratic initiatives, from promoting human rights and accountable, fair governance, to supporting free and fair elections and the rule of law. In pursuit of these goals, USAID has increased global political participation bFy training more than 9,800 domestic election observers and providing civic education to more than 6.5 million people. Since 2012, the Agency has begun integrating critical gender perspectives across all aspects of its programming to ensure all USAID initiatives work to eliminate gender disparities. To do so, USAID seeks to increase the capability of women and girls to realize their rights and determine their own life outcomes. Moreover, USAID supports additional programs to improve women's access to capital and markets, builds theirs skills in agriculture, and supports women's desire to own businesses.
#### DfID.
The Department for International Development (DfID) is the UK's lead agency for eradicating extreme poverty. To do so, DfID focuses on the creation of jobs, empowering women, and rapidly responding to humanitarian emergencies.
Some specific examples of DfID projects include governance assistance, educational initiatives, and funding cutting-edge research. In 2014 alone, DfID will help to ensure free and fair elections in 13 countries. DfID will also help provide 10 million women with access to justice through strengthened judicial systems and will help 40 million people make their authorities more accountable. By 2015, DfID will have helped 9 million children attend primary school, at least half of which will be girls. Furthermore, through the Research4Development (R4D) project, DfID has funded over 35,000 projects in the name of creating new technologies to help the world's poorest. These technologies include: vaccines for diseases of African cattle, better diagnostic methods for tuberculosis, new drugs for combating malaria, and developing flood-resistant rice. In addition to technological research, the R4D is also used to fund projects that seek to understand what, specifically, about governance structures can be changed to help the world's poorest.
### Non-governmental organizations.
A multitude of non-governmental organizations operate in the field of extreme poverty, actively working to alleviate the poorest of the poor of their deprivation. To name but a few notable organizations: Save the Children, the Overseas Development Institute, Concern Worldwide, ONE, Trickle Up and Oxfam have all done a considerable amount of work in extreme poverty.
Save the Children is the leading international organization dedicated to helping the world's indigent children. In 2013, Save the Children reached over 143 million children through their work, including over 52 million children directly. Save the Children also recently released their own report titled "Getting to Zero", in which they argued the international community could feasibly do more than lift the world's poor above $1.25/day.
The Overseas Development Institute (ODI) is a UK based think tank on international development and humanitarian issues. ODI is dedicated to alleviating the suffering of the world's poor by providing high-quality research and practical policy advice to the World's development officials. ODI also recently released a paper entitled, "The Chronic Poverty Report 2014–2015: The road to zero extreme poverty", in which its authors assert that though the international communities' goal of ending extreme poverty by 2030 is laudable, much more targeted resources will be necessary to reach said target. The report states that "To eradicate extreme poverty, massive global investment is required in social assistance, education and pro-poorest economic growth".
Concern Worldwide is an international humanitarian organization whose mission is to end extreme poverty by influencing decision makers at all levels of government (from local to international). Concern has also produced a report on extreme poverty in which they explain their own conception of extreme poverty from a NGO's standpoint. In this paper, named "How Concern Understands Extreme Poverty", the report's creators write that extreme poverty entails more than just living under $1.25/day, it also includes having a small number of assets and being vulnerable to severe negative shocks (whether natural or man made).
ONE, the organization cofounded by Bono, is a non-profit organization funded almost entirely by foundations, individual philanthropists and corporations. ONE's goals include raising public awareness and working with political leaders to fight preventable diseases, increase government accountability and increase investment in nutrition. Finally, trickleUp is a microenterprise development program targeted at those living on under $1.25/day, which provides the indigent with resources to build a sustainable livelihood through both direct financing and considerable training efforts.
Oxfam is a non-governmental organization that works prominently in Africa; their mission is to improve local community organizations and it works to reduce impediments to the development of the country. Oxfam helps families suffering from poverty receive food and healthcare to survive. There are many children in Africa experiencing growth stunting, and this is one example of an issue that Oxfam targets and aims to resolve.

</doc>
<doc id="1271" url="https://en.wikipedia.org/wiki?curid=1271" title="Analytical Engine">
Analytical Engine

The Analytical Engine was a proposed mechanical general-purpose computer designed by English mathematician and computer pioneer Charles Babbage. It was first described in 1837 as the successor to Babbage's difference engine, which was a design for a simpler mechanical computer.
The Analytical Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete. In other words, the logical structure of the Analytical Engine was essentially the same as that which has dominated computer design in the electronic era. The Analytical Engine is one of the most successful achievements of Charles Babbage.
Babbage was never able to complete construction of any of his machines due to conflicts with his chief engineer and inadequate funding. It was not until 1941 that Konrad Zuse built the first general-purpose computer, Z3, more than a century after Babbage had proposed the pioneering Analytical Engine in 1837.
## Design.
Babbage's first attempt at a mechanical computing device, the Difference Engine, was a special-purpose machine designed to tabulate logarithms and trigonometric functions by evaluating finite differences to create approximating polynomials. Construction of this machine was never completed; Babbage had conflicts with his chief engineer, Joseph Clement, and ultimately the British government withdrew its funding for the project.
During this project, Babbage realised that a much more general design, the Analytical Engine, was possible. The work on the design of the Analytical Engine started in c. 1833.
The input, consisting of programs ("formulae") and data, was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter, and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.
There was to be a store (that is, a memory) capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.6 kB). An arithmetic unit (the "mill") would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially (1838) it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side. Later drawings (1858) depict a regularised grid layout. Like the central processing unit (CPU) in a modern computer, the mill would rely upon its own internal procedures, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.
The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards. Babbage developed some two dozen programs for the Analytical Engine between 1837 and 1840, and one program later. These programs treat polynomials, iterative formulas, Gaussian elimination, and Bernoulli numbers.
In 1842, the Italian mathematician Luigi Federico Menabrea published a description of the engine in French, based on lectures Babbage gave when he visited Turin in 1840. In 1843, the description was translated into English and extensively annotated by Ada Lovelace, who had become interested in the engine eight years earlier. In recognition of her additions to Menabrea's paper, which included a way to calculate Bernoulli numbers using the machine (widely considered to be the first complete computer program), she has been described as the first computer programmer.
## Construction.
Late in his life, Babbage sought ways to build a simplified version of the machine, and assembled a small part of it before his death in 1871.
In 1878, a committee of the British Association for the Advancement of Science described the Analytical Engine as "a marvel of mechanical ingenuity", but recommended against constructing it. The committee acknowledged the usefulness and value of the machine, but could not estimate the cost of building it, and were unsure whether the machine would function correctly after being built.
Intermittently from 1880 to 1910, Babbage's son Henry Prevost Babbage was constructing a part of the mill and the printing apparatus. In 1910 it was able to calculate a (faulty) list of multiples of pi. This constituted only a small part of the whole engine; it was not programmable and had no storage. (Popular images of this section have sometimes been mislabelled, implying that it was the entire mill or even the entire engine.) Henry Babbage's "Analytical Engine Mill" is on display at the Science Museum in London. Henry also proposed building a demonstration version of the full engine, with a smaller storage capacity: "perhaps for a first machine ten (columns) would do, with fifteen wheels in each". Such a version could manipulate 20 numbers of 25 digits each, and what it could be told to do with those numbers could still be impressive. "It is only a question of cards and time", wrote Henry Babbage in 1888, "... and there is no reason why (twenty thousand) cards should not be used if necessary, in an Analytical Engine for the purposes of the mathematician".
In 1991, the London Science Museum built a complete and working specimen of Babbage's Difference Engine No. 2, a design that incorporated refinements Babbage discovered during the development of the Analytical Engine. This machine was built using materials and engineering tolerances that would have been available to Babbage, quelling the suggestion that Babbage's designs could not have been produced using the manufacturing technology of his time.
In October 2010, John Graham-Cumming started a "Plan 28" campaign to raise funds by "public subscription" to enable serious historical and academic study of Babbage's plans, with a view to then build and test a fully working virtual design which will then in turn enable construction of the physical Analytical Engine. As of May 2016, actual construction had not been attempted, since no consistent understanding could yet be obtained from Babbage's original design drawings. In particular it was unclear whether it could handle the indexed variables which were required for Lovelace's Bernoulli program. By 2017, the "Plan 28" effort reported that a searchable database of all catalogued material was available, and an initial review of Babbage's voluminous Scribbling Books had been completed.
Many of Babbage's original drawings have been digitized and are publicly available online.
## Instruction set.
Babbage is not known to have written down an explicit set of instructions for the engine in the manner of a modern processor manual. Instead he showed his programs as lists of states during their execution, showing what operator was run at each step with little indication of how the control flow would be guided.
Allan G. Bromley has assumed that the card deck could be read in forwards and backwards directions as a function of conditional branching after testing for conditions, which would make the engine Turing-complete:
...the cards could be ordered to move forward and reverse (and hence to loop)...
The introduction for the first time, in 1845, of user operations for a variety of service functions including, most importantly, an effective system for user control of looping in user programs.
There is no indication how the direction of turning of the operation and variable cards is specified. In the absence of other evidence I have had to adopt the minimal default assumption that both the operation and variable cards can only be turned backward as is necessary to implement the loops used in Babbage's sample programs. There would be no mechanical or microprogramming difficulty in placing the direction of motion under the control of the user.
In their emulator of the engine, Fourmilab say:
The Engine's Card Reader is not constrained to simply process the cards in a chain one after another from start to finish. It can, in addition, directed by the very cards it reads and advised by whether the Mill's run-up lever is activated, either advance the card chain forward, skipping the intervening cards, or backward, causing previously-read cards to be processed once again.
This emulator does provide a written symbolic instruction set, though this has been constructed by its authors rather than based on Babbage's original works. For example, a factorial program would be written as:
 N0 6
 N1 1
 N2 1
 L1
 L0
 S1
 L0
 L2
 S0
 L2
 L0
 CB?11
where the CB is the conditional branch instruction or "combination card" used to make the control flow jump, in this case backward by 11 cards.
## Influence.
### Predicted influence.
Babbage understood that the existence of an automatic computer would kindle interest in the field now known as algorithmic efficiency, writing in his "Passages from the Life of a Philosopher", "As soon as an Analytical Engine exists, it will necessarily guide the future course of the science. Whenever any result is sought by its aid, the question will then arise—By what course of calculation can these results be arrived at by the machine in the "shortest time"?"
### Computer science.
From 1872 Henry continued diligently with his father's work and then intermittently in retirement in 1875.
Percy Ludgate wrote about the engine in 1914 and published his own design for an Analytical Engine in 1908. It was drawn up in detail, but never built, and the drawings have never been found. Ludgate's engine would be much smaller (about ) than Babbage's, and hypothetically would be capable of multiplying two 20-decimal-digit numbers in about six seconds.
In his "Essays on Automatics" (1913) Leonardo Torres y Quevedo designed a Babbage type of calculating machine that used electromechanical parts which included floating point number representations and built an early prototype in 1920.
Vannevar Bush's paper "Instrumental Analysis" (1936) included several references to Babbage's work. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.
Despite this groundwork, Babbage's work fell into historical obscurity, and the Analytical Engine was unknown to builders of electromechanical and electronic computing machines in the 1930s and 1940s when they began their work, resulting in the need to re-invent many of the architectural innovations Babbage had proposed. Howard Aiken, who built the quickly-obsoleted electromechanical calculator, the Harvard Mark I, between 1937 and 1945, praised Babbage's work likely as a way of enhancing his own stature, but knew nothing of the Analytical Engine's architecture during the construction of the Mark I, and considered his visit to the constructed portion of the Analytical Engine "the greatest disappointment of my life". The Mark I showed no influence from the Analytical Engine and lacked the Analytical Engine's most prescient architectural feature, conditional branching. J. Presper Eckert and John W. Mauchly similarly were not aware of the details of Babbage's Analytical Engine work prior to the completion of their design for the first electronic general-purpose computer, the ENIAC.
## Comparison to other early computers.
If the Analytical Engine had been built, it would have been digital, programmable and Turing-complete. It would, however, have been very slow. Luigi Federico Menabrea reported in "Sketch of the Analytical Engine": "Mr. Babbage believes he can, by his engine, form the product of two numbers, each containing twenty figures, in three minutes".
By comparison the Harvard Mark I could perform the same task in just six seconds. A modern PC can do the same thing in well under a billionth of a second.

</doc>
<doc id="1273" url="https://en.wikipedia.org/wiki?curid=1273" title="Augustus">
Augustus

Caesar Augustus (23 September 63 BC – 19 August AD 14), also known as Octavian, was the first Roman emperor, reigning from 27 BC until his death in AD 14. His status as the founder of the Roman Principate (the first phase of the Roman Empire) has consolidated a legacy as one of the most effective leaders in human history. The reign of Augustus initiated an era of relative peace known as the "Pax Romana". The Roman world was largely free from large-scale conflict for more than two centuries, despite continuous wars of imperial expansion on the Empire's frontiers and the year-long civil war known as the "Year of the Four Emperors" over the imperial succession.
Originally named Gaius Octavius, he was born into an old and wealthy equestrian branch of the plebeian "gens" Octavia. His maternal great-uncle Julius Caesar was assassinated in 44 BC and Octavius was named in Caesar's will as his adopted son and heir; as a result, he inherited Caesar's name, estate, and the loyalty of his legions. He, Mark Antony and Marcus Lepidus formed the Second Triumvirate to defeat the assassins of Caesar. Following their victory at the Battle of Philippi (42 BC), the Triumvirate divided the Roman Republic among themselves and ruled as "de facto" dictators. The Triumvirate was eventually torn apart by the competing ambitions of its members; Lepidus was exiled in 36 BC and Antony was defeated by Octavian at the Battle of Actium in 31 BC.
After the demise of the Second Triumvirate, Augustus restored the outward façade of the free Republic, with governmental power vested in the Roman Senate, the executive magistrates and the legislative assemblies, yet maintained autocratic authority by having the Senate grant him lifetime tenure as supreme military command, tribune and censor. A similar ambiguity is seen in his chosen names, the implied rejection of monarchical titles whereby he called himself "Princeps Civitatis" (First Citizen) juxtaposed with his adoption of the ancient title Augustus.
Augustus dramatically enlarged the Empire, annexing Egypt, Dalmatia, Pannonia, Noricum and Raetia, expanding possessions in Africa, and completing the conquest of Hispania, but suffered a major setback in Germania. Beyond the frontiers, he secured the Empire with a buffer region of client states and made peace with the Parthian Empire through diplomacy. He reformed the Roman system of taxation, developed networks of roads with an official courier system, established a standing army, established the Praetorian Guard, official police and fire-fighting services for Rome, and rebuilt much of the city during his reign. Augustus died in AD 14 at the age of 75, probably from natural causes. Persistent rumors, substantiated somewhat by deaths in the imperial family, have claimed his wife Livia poisoned him. He was succeeded as emperor by his adopted son Tiberius, Livia's son and also former husband of Augustus' only biological daughter Julia.
## Name.
As a consequence of Roman customs, society, and personal preference, Augustus ( ) was known by many names throughout his life:
## Early life.
While his paternal family was from the Volscian town of Velletri, approximately to the south-east of Rome, Augustus was born in the city of Rome on 23 September 63 BC. He was born at Ox Head, a small property on the Palatine Hill, very close to the Roman Forum. He was given the name Gaius Octavius, and in his infancy he received the cognomen Thurinus, possibly commemorating his father's victory at Thurii over a rebellious band of slaves which occurred a few years after his birth. Suetonius wrote: "There are many indications that the Octavian family was in days of old a distinguished one at Velitrae; for not only was a street in the most frequented part of town long ago called Octavian, but an altar was shown there besides, consecrated by an Octavius. This man was leader in a war with a neighbouring town ..."
Due to the crowded nature of Rome at the time, Octavius was taken to his father's home village at Velletri to be raised. Octavius mentions his father's equestrian family only briefly in his memoirs. His paternal great-grandfather Gaius Octavius was a military tribune in Sicily during the Second Punic War. His grandfather had served in several local political offices. His father, also named Gaius Octavius, had been governor of Macedonia. His mother, Atia, was the niece of Julius Caesar.
In 59 BC, when he was four years old, his father died. His mother married a former governor of Syria, Lucius Marcius Philippus. Philippus claimed descent from Alexander the Great, and was elected consul in 56 BC. Philippus never had much of an interest in young Octavius. Because of this, Octavius was raised by his grandmother, Julia, the sister of Julius Caesar. Julia died in 52 or 51 BC, and Octavius delivered the funeral oration for his grandmother. From this point, his mother and stepfather took a more active role in raising him. He donned the "toga virilis" four years later, and was elected to the College of Pontiffs in 47 BC. The following year he was put in charge of the Greek games that were staged in honor of the Temple of Venus Genetrix, built by Julius Caesar.
According to Nicolaus of Damascus, Octavius wished to join Caesar's staff for his campaign in Africa, but gave way when his mother protested. In 46 BC, she consented for him to join Caesar in Hispania, where he planned to fight the forces of Pompey, Caesar's late enemy, but Octavius fell ill and was unable to travel. When he had recovered, he sailed to the front, but was shipwrecked. After coming ashore with a handful of companions, he crossed hostile territory to Caesar's camp, which impressed his great-uncle considerably. Velleius Paterculus reports that after that time, Caesar allowed the young man to share his carriage. When back in Rome, Caesar deposited a new will with the Vestal Virgins, naming Octavius as the prime beneficiary.
## Rise to power.
### Heir to Caesar.
Octavius was studying and undergoing military training in Apollonia, Illyria, when Julius Caesar was killed on the Ides of March (15 March) 44 BC. He rejected the advice of some army officers to take refuge with the troops in Macedonia and sailed to Italy to ascertain whether he had any potential political fortunes or security. Caesar had no living legitimate children under Roman law, and so had adopted Octavius, his grand-nephew, making him his primary heir. Mark Antony later charged that Octavian had earned his adoption by Caesar through sexual favours, though Suetonius describes Antony's accusation as political slander. This form of slander was popular during this time in the Roman Republic to demean and discredit political opponents by accusing them of having an inappropriate sexual affair. After landing at Lupiae near Brundisium, Octavius learned the contents of Caesar's will, and only then did he decide to become Caesar's political heir as well as heir to two-thirds of his estate.
Upon his adoption, Octavius assumed his great-uncle's name Gaius Julius Caesar. Roman citizens adopted into a new family usually retained their old nomen in cognomen form (e.g., "Octavianus" for one who had been an Octavius, "Aemilianus" for one who had been an Aemilius, etc.). However, though some of his contemporaries did, there is no evidence that Octavius ever himself officially used the name "Octavianus", as it would have made his modest origins too obvious. Historians usually refer to the new Caesar as "Octavian" during the time between his adoption and his assumption of the name Augustus in 27 BC in order to avoid confusing the dead dictator with his heir.
Octavian could not rely on his limited funds to make a successful entry into the upper echelons of the Roman political hierarchy. After a warm welcome by Caesar's soldiers at Brundisium, Octavian demanded a portion of the funds that were allotted by Caesar for the intended war against the Parthian Empire in the Middle East. This amounted to 700 million sesterces stored at Brundisium, the staging ground in Italy for military operations in the east.
A later senatorial investigation into the disappearance of the public funds took no action against Octavian, since he subsequently used that money to raise troops against the Senate's arch enemy Mark Antony. Octavian made another bold move in 44 BC when, without official permission, he appropriated the annual tribute that had been sent from Rome's Near Eastern province to Italy.
Octavian began to bolster his personal forces with Caesar's veteran legionaries and with troops designated for the Parthian war, gathering support by emphasizing his status as heir to Caesar. On his march to Rome through Italy, Octavian's presence and newly acquired funds attracted many, winning over Caesar's former veterans stationed in Campania. By June, he had gathered an army of 3,000 loyal veterans, paying each a salary of 500 denarii.
### Growing tensions.
Arriving in Rome on 6 May 44 BC, Octavian found consul Mark Antony, Caesar's former colleague, in an uneasy truce with the dictator's assassins. They had been granted a general amnesty on 17 March, yet Antony had succeeded in driving most of them out of Rome with an inflammatory eulogy at Caesar's funeral, mounting public opinion against the assassins.
Mark Antony was amassing political support, but Octavian still had opportunity to rival him as the leading member of the faction supporting Caesar. Mark Antony had lost the support of many Romans and supporters of Caesar when he initially opposed the motion to elevate Caesar to divine status. Octavian failed to persuade Antony to relinquish Caesar's money to him. During the summer, he managed to win support from Caesarian sympathizers and also made common with the Optimates, the former enemies of Caesar, who saw him as the lesser evil and hoped to manipulate him. In September, the leading Optimate orator Marcus Tullius Cicero began to attack Antony in a series of speeches portraying him as a threat to the Republican order.
### First conflict with Antony.
With opinion in Rome turning against him and his year of consular power nearing its end, Antony attempted to pass laws that would assign him the province of Cisalpine Gaul. Octavian meanwhile built up a private army in Italy by recruiting Caesarian veterans and, on 28 November, he won over two of Antony's legions with the enticing offer of monetary gain.
In the face of Octavian's large and capable force, Antony saw the danger of staying in Rome and, to the relief of the Senate, he left Rome for Cisalpine Gaul, which was to be handed to him on 1 January. However, the province had earlier been assigned to Decimus Junius Brutus Albinus, one of Caesar's assassins, who now refused to yield to Antony. Antony besieged him at Mutina and rejected the resolutions passed by the Senate to stop the fighting. The Senate had no army to enforce their resolutions. This provided an opportunity for Octavian, who already was known to have armed forces. Cicero also defended Octavian against Antony's taunts about Octavian's lack of noble lineage and aping of Julius Caesar's name, stating "we have no more brilliant example of traditional piety among our youth."
At the urging of Cicero, the Senate inducted Octavian as senator on 1 January 43 BC, yet he also was given the power to vote alongside the former consuls. In addition, Octavian was granted "propraetor" "imperium" (commanding power) which legalized his command of troops, sending him to relieve the siege along with Hirtius and Pansa (the consuls for 43 BC). He assumed the "fasces" on 7 January, a date that he would later commemorate as the beginning of his public career. Antony's forces were defeated at the battles of Forum Gallorum (14 April) and Mutina (21 April), forcing Antony to retreat to Transalpine Gaul. Both consuls were killed, however, leaving Octavian in sole command of their armies.
The senate heaped many more rewards on Decimus Brutus than on Octavian for defeating Antony, then attempted to give command of the consular legions to Decimus Brutus. In response, Octavian stayed in the Po Valley and refused to aid any further offensive against Antony. In July, an embassy of centurions sent by Octavian entered Rome and demanded the consulship left vacant by Hirtius and Pansa and also that the decree should be rescinded which declared Antony a public enemy. When this was refused, he marched on the city with eight legions. He encountered no military opposition in Rome, and on 19 August 43 BC was elected consul with his relative Quintus Pedius as co-consul. Meanwhile, Antony formed an alliance with Marcus Aemilius Lepidus, another leading Caesarian.
### Second Triumvirate.
#### Proscriptions.
In a meeting near Bologna in October 43 BC, Octavian, Antony, and Lepidus formed the Second Triumvirate. Their powers were officialized by the Senate on 27 November. This explicit arrogation of special powers lasting five years was then legalised by law passed by the plebs, unlike the unofficial First Triumvirate formed by Pompey, Julius Caesar, and Marcus Licinius Crassus. The triumvirs then set in motion proscriptions, in which between 130 and 300 senators and 2,000 "equites" were branded as outlaws and deprived of their property and, for those who failed to escape, their lives. This decree issued by the triumvirate was motivated in part by a need to raise money to pay the salaries of their troops for the upcoming conflict against Caesar's assassins, Marcus Junius Brutus and Gaius Cassius Longinus. Rewards for their arrest gave incentive for Romans to capture those proscribed, while the assets and properties of those arrested were seized by the triumvirs.
Contemporary Roman historians provide conflicting reports as to which triumvir was most responsible for the proscriptions and killing. However, the sources agree that enacting the proscriptions was a means by all three factions to eliminate political enemies. Marcus Velleius Paterculus asserted that Octavian tried to avoid proscribing officials whereas Lepidus and Antony were to blame for initiating them. Cassius Dio defended Octavian as trying to spare as many as possible, whereas Antony and Lepidus, being older and involved in politics longer, had many more enemies to deal with.
This claim was rejected by Appian, who maintained that Octavian shared an equal interest with Lepidus and Antony in eradicating his enemies. Suetonius said that Octavian was reluctant to proscribe officials, but did pursue his enemies with more vigor than the other triumvirs. Plutarch described the proscriptions as a ruthless and cutthroat swapping of friends and family among Antony, Lepidus, and Octavian. For example, Octavian allowed the proscription of his ally Cicero, Antony the proscription of his maternal uncle Lucius Julius Caesar (the consul of 64 BC), and Lepidus his brother Paullus.
#### Battle of Philippi and division of territory.
On 1 January 42 BC, the Senate posthumously recognized Julius Caesar as a divinity of the Roman state, "Divus Iulius". Octavian was able to further his cause by emphasizing the fact that he was "divi filius", "Son of the Divine". Antony and Octavian then sent 28 legions by sea to face the armies of Brutus and Cassius, who had built their base of power in Greece. After two battles at Philippi in Macedonia in October 42, the Caesarian army was victorious and Brutus and Cassius committed suicide. Mark Antony later used the examples of these battles as a means to belittle Octavian, as both battles were decisively won with the use of Antony's forces. In addition to claiming responsibility for both victories, Antony also branded Octavian as a coward for handing over his direct military control to Marcus Vipsanius Agrippa instead.
After Philippi, a new territorial arrangement was made among the members of the Second Triumvirate. Gaul and the province of Hispania were placed in the hands of Octavian. Antony traveled east to Egypt where he allied himself with Queen Cleopatra VII, the former lover of Julius Caesar and mother of Caesar's infant son Caesarion. Lepidus was left with the province of Africa, stymied by Antony, who conceded Hispania to Octavian instead.
Octavian was left to decide where in Italy to settle the tens of thousands of veterans of the Macedonian campaign, whom the triumvirs had promised to discharge. The tens of thousands who had fought on the republican side with Brutus and Cassius could easily ally with a political opponent of Octavian if not appeased, and they also required land. There was no more government-controlled land to allot as settlements for their soldiers, so Octavian had to choose one of two options: alienating many Roman citizens by confiscating their land, or alienating many Roman soldiers who could mount a considerable opposition against him in the Roman heartland. Octavian chose the former. There were as many as eighteen Roman towns affected by the new settlements, with entire populations driven out or at least given partial evictions.
#### Rebellion and marriage alliances.
There was widespread dissatisfaction with Octavian over these settlements of his soldiers, and this encouraged many to rally at the side of Lucius Antonius, who was brother of Mark Antony and supported by a majority in the Senate. Meanwhile, Octavian asked for a divorce from Claudia, the daughter of Fulvia (Mark Antony's wife) and her first husband Publius Clodius Pulcher. He returned Claudia to her mother, claiming that their marriage had never been consummated. Fulvia decided to take action. Together with Lucius Antonius, she raised an army in Italy to fight for Antony's rights against Octavian. Lucius and Fulvia took a political and martial gamble in opposing Octavian, however, since the Roman army still depended on the triumvirs for their salaries. Lucius and his allies ended up in a defensive siege at Perusia (modern Perugia), where Octavian forced them into surrender in early 40 BC.
Lucius and his army were spared, due to his kinship with Antony, the strongman of the East, while Fulvia was exiled to Sicyon. Octavian showed no mercy, however, for the mass of allies loyal to Lucius; on 15 March, the anniversary of Julius Caesar's assassination, he had 300 Roman senators and equestrians executed for allying with Lucius. Perusia also was pillaged and burned as a warning for others. This bloody event sullied Octavian's reputation and was criticized by many, such as Augustan poet Sextus Propertius.
Sextus Pompeius, the son of Pompey and still a renegade general following Julius Caesar's victory over his father, had established himself in Sicily and Sardinia as part of an agreement reached with the Second Triumvirate in 39 BC. Both Antony and Octavian were vying for an alliance with Pompeius. Octavian succeeded in a temporary alliance in 40 BC when he married Scribonia, a sister or daughter of Pompeius's father-in-law Lucius Scribonius Libo. Scribonia gave birth to Octavian's only natural child, Julia, the same day that he divorced her to marry Livia Drusilla, little more than a year after their marriage.
While in Egypt, Antony had been engaged in an affair with Cleopatra and had fathered twin children with her. Aware of his deteriorating relationship with Octavian, Antony left Cleopatra; he sailed to Italy in 40 BC with a large force to oppose Octavian, laying siege to Brundisium. This new conflict proved untenable for both Octavian and Antony, however. Their centurions, who had become important figures politically, refused to fight due to their Caesarian cause, while the legions under their command followed suit. Meanwhile, in Sicyon, Antony's wife Fulvia died of a sudden illness while Antony was en route to meet her. Fulvia's death and the mutiny of their centurions allowed the two remaining triumvirs to effect a reconciliation.
In the autumn of 40, Octavian and Antony approved the Treaty of Brundisium, by which Lepidus would remain in Africa, Antony in the East, Octavian in the West. The Italian Peninsula was left open to all for the recruitment of soldiers, but in reality, this provision was useless for Antony in the East. To further cement relations of alliance with Mark Antony, Octavian gave his sister, Octavia Minor, in marriage to Antony in late 40 BC.
#### War with Pompeius.
Sextus Pompeius threatened Octavian in Italy by denying shipments of grain through the Mediterranean Sea to the peninsula. Pompeius's own son was put in charge as naval commander in the effort to cause widespread famine in Italy. Pompeius's control over the sea prompted him to take on the name "Neptuni filius", "son of Neptune". A temporary peace agreement was reached in 39 BC with the treaty of Misenum; the blockade on Italy was lifted once Octavian granted Pompeius Sardinia, Corsica, Sicily, and the Peloponnese, and ensured him a future position as consul for 35 BC.
The territorial agreement between the triumvirate and Sextus Pompeius began to crumble once Octavian divorced Scribonia and married Livia on 17 January 38 BC. One of Pompeius's naval commanders betrayed him and handed over Corsica and Sardinia to Octavian. Octavian lacked the resources to confront Pompeius alone, however, so an agreement was reached with the Second Triumvirate's extension for another five-year period beginning in 37 BC.
In supporting Octavian, Antony expected to gain support for his own campaign against the Parthian Empire, desiring to avenge Rome's defeat at Carrhae in 53 BC. In an agreement reached at Tarentum, Antony provided 120 ships for Octavian to use against Pompeius, while Octavian was to send 20,000 legionaries to Antony for use against Parthia. Octavian sent only a tenth of those promised, however, which Antony viewed as an intentional provocation.
Octavian and Lepidus launched a joint operation against Sextus in Sicily in 36 BC. Despite setbacks for Octavian, the naval fleet of Sextus Pompeius was almost entirely destroyed on 3 September by General Agrippa at the naval Battle of Naulochus. Sextus fled to the east with his remaining forces, where he was captured and executed in Miletus by one of Antony's generals the following year. As Lepidus and Octavian accepted the surrender of Pompeius's troops, Lepidus attempted to claim Sicily for himself, ordering Octavian to leave. Lepidus's troops deserted him, however, and defected to Octavian since they were weary of fighting and were enticed by Octavian's promises of money.
Lepidus surrendered to Octavian and was permitted to retain the office of "pontifex maximus" (head of the college of priests), but was ejected from the Triumvirate, his public career at an end, and effectively was exiled to a villa at Cape Circei in Italy. The Roman dominions were now divided between Octavian in the West and Antony in the East. Octavian ensured Rome's citizens of their rights to property in order to maintain peace and stability in his portion of the Empire. This time, he settled his discharged soldiers outside of Italy, while also returning 30,000 slaves to their former Roman owners—slaves who had fled to join Pompeius's army and navy. Octavian had the Senate grant him, his wife, and his sister tribunal immunity, or "sacrosanctitas", in order to ensure his own safety and that of Livia and Octavia once he returned to Rome.
#### War with Antony and Cleopatra.
Meanwhile, Antony's campaign turned disastrous against Parthia, tarnishing his image as a leader, and the mere 2,000 legionaries sent by Octavian to Antony were hardly enough to replenish his forces. On the other hand, Cleopatra could restore his army to full strength; he already was engaged in a romantic affair with her, so he decided to send Octavia back to Rome. Octavian used this to spread propaganda implying that Antony was becoming less than Roman because he rejected a legitimate Roman spouse for an "Oriental paramour". In 36 BC, Octavian used a political ploy to make himself look less autocratic and Antony more the villain by proclaiming that the civil wars were coming to an end, and that he would step down as triumvir—if only Antony would do the same. Antony refused.
Roman troops captured the Kingdom of Armenia in 34 BC, and Antony made his son Alexander Helios the ruler of Armenia. He also awarded the title "Queen of Kings" to Cleopatra, acts that Octavian used to convince the Roman Senate that Antony had ambitions to diminish the preeminence of Rome. Octavian became consul once again on 1 January 33 BC, and he opened the following session in the Senate with a vehement attack on Antony's grants of titles and territories to his relatives and to his queen.
The breach between Antony and Octavian prompted a large portion of the Senators, as well as both of that year's consuls, to leave Rome and defect to Antony. However, Octavian received two key deserters from Antony in the autumn of 32 BC: Munatius Plancus and Marcus Titius. These defectors gave Octavian the information that he needed to confirm with the Senate all the accusations that he made against Antony.
Octavian forcibly entered the temple of the Vestal Virgins and seized Antony's secret will, which he promptly publicized. The will would have given away Roman-conquered territories as kingdoms for his sons to rule, and designated Alexandria as the site for a tomb for him and his queen. In late 32 BC, the Senate officially revoked Antony's powers as consul and declared war on Cleopatra's regime in Egypt.
In early 31 BC, Antony and Cleopatra were temporarily stationed in Greece when Octavian gained a preliminary victory: the navy successfully ferried troops across the Adriatic Sea under the command of Agrippa. Agrippa cut off Antony and Cleopatra's main force from their supply routes at sea, while Octavian landed on the mainland opposite the island of Corcyra (modern Corfu) and marched south. Trapped on land and sea, deserters of Antony's army fled to Octavian's side daily while Octavian's forces were comfortable enough to make preparations.
Antony's fleet sailed through the bay of Actium on the western coast of Greece in a desperate attempt to break free of the naval blockade. It was there that Antony's fleet faced the much larger fleet of smaller, more maneuverable ships under commanders Agrippa and Gaius Sosius in the Battle of Actium on 2 September 31 BC. Antony and his remaining forces were spared only due to a last-ditch effort by Cleopatra's fleet that had been waiting nearby.
Octavian pursued them and defeated their forces in Alexandria on 1 August 30 BC—after which Antony and Cleopatra committed suicide. Antony fell on his own sword and was taken by his soldiers back to Alexandria where he died in Cleopatra's arms. Cleopatra died soon after, reputedly by the venomous bite of an asp or by poison. Octavian had exploited his position as Caesar's heir to further his own political career, and he was well aware of the dangers in allowing another person to do the same. He therefore followed the advice of Arius Didymus that "two Caesars are one too many", ordering Caesarion, Julius Caesar's son by Cleopatra, killed, while sparing Cleopatra's children by Antony, with the exception of Antony's older son. Octavian had previously shown little mercy to surrendered enemies and acted in ways that had proven unpopular with the Roman people, yet he was given credit for pardoning many of his opponents after the Battle of Actium.
## Sole ruler of Rome.
After Actium and the defeat of Antony and Cleopatra, Octavian was in a position to rule the entire Republic under an unofficial principate—but he had to achieve this through incremental power gains. He did so by courting the Senate and the people while upholding the republican traditions of Rome, appearing that he was not aspiring to dictatorship or monarchy. Marching into Rome, Octavian and Marcus Agrippa were elected as consuls by the Senate.
Years of civil war had left Rome in a state of near lawlessness, but the Republic was not prepared to accept the control of Octavian as a despot. At the same time, Octavian could not simply give up his authority without risking further civil wars among the Roman generals and, even if he desired no position of authority whatsoever, his position demanded that he look to the well-being of the city of Rome and the Roman provinces. Octavian's aims from this point forward were to return Rome to a state of stability, traditional legality, and civility by lifting the overt political pressure imposed on the courts of law and ensuring free elections—in name at least.
### First settlement.
In 27 BC, Octavian made a show of returning full power to the Roman Senate and relinquishing his control of the Roman provinces and their armies. Under his consulship, however, the Senate had little power in initiating legislation by introducing bills for senatorial debate. Octavian was no longer in direct control of the provinces and their armies, but he retained the loyalty of active duty soldiers and veterans alike. The careers of many clients and adherents depended on his patronage, as his financial power was unrivaled in the Roman Republic. Historian Werner Eck states:
To a large extent, the public were aware of the vast financial resources that Octavian commanded. He failed to encourage enough senators to finance the building and maintenance of networks of roads in Italy in 20 BC, but he undertook direct responsibility for them. This was publicized on the Roman currency issued in 16 BC, after he donated vast amounts of money to the "aerarium Saturni", the public treasury.
According to historian H. H. Scullard, however, Octavian's power was based on the exercise of "a predominant military power and ... the ultimate sanction of his authority was force, however much the fact was disguised." The Senate proposed to Octavian, the victor of Rome's civil wars, that he once again assume command of the provinces. The Senate's proposal was a ratification of Octavian's extra-constitutional power. Through the Senate, Octavian was able to continue the appearance of a still-functional constitution. Feigning reluctance, he accepted a ten-year responsibility of overseeing provinces that were considered chaotic.
The provinces ceded to Augustus for that ten-year period comprised much of the conquered Roman world, including all of Hispania and Gaul, Syria, Cilicia, Cyprus, and Egypt. Moreover, command of these provinces provided Octavian with control over the majority of Rome's legions.
While Octavian acted as consul in Rome, he dispatched senators to the provinces under his command as his representatives to manage provincial affairs and ensure that his orders were carried out. The provinces not under Octavian's control were overseen by governors chosen by the Roman Senate. Octavian became the most powerful political figure in the city of Rome and in most of its provinces, but he did not have a monopoly on political and martial power.
The Senate still controlled North Africa, an important regional producer of grain, as well as Illyria and Macedonia, two strategic regions with several legions. However, the Senate had control of only five or six legions distributed among three senatorial proconsuls, compared to the twenty legions under the control of Octavian, and their control of these regions did not amount to any political or military challenge to Octavian. The Senate's control over some of the Roman provinces helped maintain a republican façade for the autocratic Principate. Also, Octavian's control of entire provinces followed Republican-era precedents for the objective of securing peace and creating stability, in which such prominent Romans as Pompey had been granted similar military powers in times of crisis and instability.
### Change to Augustus.
On 16 January 27 BC the Senate gave Octavian the new titles of "Augustus" and "Princeps". "Augustus" is from the Latin word "Augere" (meaning to increase) and can be translated as "the illustrious one". It was a title of religious authority rather than political authority. His new title of Augustus was also more favorable than "Romulus", the previous one which he styled for himself in reference to the story of the legendary founder of Rome, which symbolized a second founding of Rome. The title of "Romulus" was associated too strongly with notions of monarchy and kingship, an image that Octavian tried to avoid. The title "princeps senatus" originally meant the member of the Senate with the highest precedence, but in the case of Augustus, it became an almost regnal title for a leader who was first in charge. As a result, modern historians usually regard this event as the beginning of Augustus' reign as "emperor". Augustus also styled himself as "Imperator Caesar divi filius", "Commander Caesar son of the deified one". With this title, he boasted his familial link to deified Julius Caesar, and the use of "Imperator" signified a permanent link to the Roman tradition of victory. He transformed "Caesar", a cognomen for one branch of the Julian family, into a new family line that began with him.
Augustus was granted the right to hang the "corona civica" above his door, the "civic crown" made from oak, and to have laurels drape his doorposts. However, he renounced flaunting insignia of power such as holding a scepter, wearing a diadem, or wearing the golden crown and purple toga of his predecessor Julius Caesar. If he refused to symbolize his power by donning and bearing these items on his person, the Senate nonetheless awarded him with a golden shield displayed in the meeting hall of the Curia, bearing the inscription "virtus", "pietas", "clementia", "iustitia"—"valor, piety, clemency, and justice."
### Second settlement.
By 23 BC, some of the un-Republican implications were becoming apparent concerning the settlement of 27 BC. Augustus's retention of an annual consulate drew attention to his "de facto" dominance over the Roman political system, and cut in half the opportunities for others to achieve what was still nominally the preeminent position in the Roman state. Further, he was causing political problems by desiring to have his nephew Marcus Claudius Marcellus follow in his footsteps and eventually assume the Principate in his turn, alienating his three greatest supporters – Agrippa, Maecenas, and Livia. He appointed noted Republican Calpurnius Piso (who had fought against Julius Caesar and supported Cassius and Brutus) as co-consul in 23 BC, after his choice Aulus Terentius Varro Murena died unexpectedly.
In the late spring Augustus suffered a severe illness, and on his supposed deathbed made arrangements that would ensure the continuation of the Principate in some form, while allaying senators' suspicions of his anti-republicanism. Augustus prepared to hand down his signet ring to his favored general Agrippa. However, Augustus handed over to his co-consul Piso all of his official documents, an account of public finances, and authority over listed troops in the provinces while Augustus's supposedly favored nephew Marcellus came away empty-handed. This was a surprise to many who believed Augustus would have named an heir to his position as an unofficial emperor.
Augustus bestowed only properties and possessions to his designated heirs, as an obvious system of institutionalized imperial inheritance would have provoked resistance and hostility among the republican-minded Romans fearful of monarchy. With regards to the Principate, it was obvious to Augustus that Marcellus was not ready to take on his position; nonetheless, by giving his signet ring to Agrippa, Augustus intended to signal to the legions that Agrippa was to be his successor, and that constitutional procedure notwithstanding, they should continue to obey Agrippa.
Soon after his bout of illness subsided, Augustus gave up his consulship. The only other times Augustus would serve as consul would be in the years 5 and 2 BC, both times to introduce his grandsons into public life. This was a clever ploy by Augustus; ceasing to serve as one of two annually elected consuls allowed aspiring senators a better chance to attain the consular position, while allowing Augustus to exercise wider patronage within the senatorial class. Although Augustus had resigned as consul, he desired to retain his consular "imperium" not just in his provinces but throughout the empire. This desire, as well as the Marcus Primus Affair, led to a second compromise between him and the Senate known as the Second Settlement.
### Primary reasons for the Second Settlement.
The primary reasons for the Second Settlement were as follows. First, after Augustus relinquished the annual consulship, he was no longer in an official position to rule the state, yet his dominant position remained unchanged over his Roman, 'imperial' provinces where he was still a proconsul. When he annually held the office of consul, he had the power to intervene with the affairs of the other provincial proconsuls appointed by the Senate throughout the empire, when he deemed necessary.
A second problem later arose showing the need for the Second Settlement in what became known as the "Marcus Primus Affair". In late 24 or early 23 BC, charges were brought against Marcus Primus, the former proconsul (governor) of Macedonia, for waging a war without prior approval of the Senate on the Odrysian kingdom of Thrace, whose king was a Roman ally. He was defended by Lucius Lucinius Varro Murena, who told the trial that his client had received specific instructions from Augustus, ordering him to attack the client state. Later, Primus testified that the orders came from the recently deceased Marcellus.
Such orders, had they been given, would have been considered a breach of the Senate's prerogative under the Constitutional settlement of 27 BC and its aftermath—i.e., before Augustus was granted "imperium proconsulare maius"—as Macedonia was a Senatorial province under the Senate's jurisdiction, not an imperial province under the authority of Augustus. Such an action would have ripped away the veneer of Republican restoration as promoted by Augustus, and exposed his fraud of merely being the first citizen, a first among equals. Even worse, the involvement of Marcellus provided some measure of proof that Augustus's policy was to have the youth take his place as Princeps, instituting a form of monarchy – accusations that had already played out.
The situation was so serious that Augustus himself appeared at the trial, even though he had not been called as a witness. Under oath, Augustus declared that he gave no such order. Murena disbelieved Augustus's testimony and resented his attempt to subvert the trial by using his "auctoritas". He rudely demanded to know why Augustus had turned up to a trial to which he had not been called; Augustus replied that he came in the public interest. Although Primus was found guilty, some jurors voted to acquit, meaning that not everybody believed Augustus's testimony, an insult to the 'August One'.
The Second Constitutional Settlement was completed in part to allay confusion and formalize Augustus's legal authority to intervene in Senatorial provinces. The Senate granted Augustus a form of general "imperium proconsulare", or proconsular imperium (power) that applied throughout the empire, not solely to his provinces. Moreover, the Senate augmented Augustus's proconsular imperium into "imperium proconsulare maius", or proconsular imperium applicable throughout the empire that was more (maius) or greater than that held by the other proconsuls. This in effect gave Augustus constitutional power superior to all other proconsuls in the empire. Augustus stayed in Rome during the renewal process and provided veterans with lavish donations to gain their support, thereby ensuring that his status of proconsular imperium maius was renewed in 13 BC.
### Additional powers.
During the second settlement, Augustus was also granted the power of a tribune ("tribunicia potestas") for life, though not the official title of tribune. For some years, Augustus had been awarded "tribunicia sacrosanctitas", the immunity given to a Tribune of the Plebs. Now he decided to assume the full powers of the magistracy, renewed annually, in perpetuity. Legally, it was closed to patricians, a status that Augustus had acquired some years earlier when adopted by Julius Caesar.
This power allowed him to convene the Senate and people at will and lay business before them, to veto the actions of either the Assembly or the Senate, to preside over elections, and to speak first at any meeting. Also included in Augustus's tribunician authority were powers usually reserved for the Roman censor; these included the right to supervise public morals and scrutinize laws to ensure that they were in the public interest, as well as the ability to hold a census and determine the membership of the Senate.
With the powers of a censor, Augustus appealed to virtues of Roman patriotism by banning all attire but the classic toga while entering the Forum. There was no precedent within the Roman system for combining the powers of the tribune and the censor into a single position, nor was Augustus ever elected to the office of censor. Julius Caesar had been granted similar powers, wherein he was charged with supervising the morals of the state. However, this position did not extend to the censor's ability to hold a census and determine the Senate's roster. The office of the "tribunus plebis" began to lose its prestige due to Augustus's amassing of tribunal powers, so he revived its importance by making it a mandatory appointment for any plebeian desiring the praetorship.
Augustus was granted sole "imperium" within the city of Rome itself, in addition to being granted proconsular imperium maius and tribunician authority for life. Traditionally, proconsuls (Roman province governors) lost their proconsular "imperium" when they crossed the Pomerium – the sacred boundary of Rome – and entered the city. In these situations, Augustus would have power as part of his tribunician authority but his constitutional imperium within the Pomerium would be less than that of a serving consul. That would mean that, when he was in the city, he might not be the constitutional magistrate with the most authority. Thanks to his prestige or "auctoritas", his wishes would usually be obeyed, but there might be some difficulty. To fill this power vacuum, the Senate voted that Augustus's imperium proconsulare maius (superior proconsular power) should not lapse when he was inside the city walls. All armed forces in the city had formerly been under the control of the urban praetors and consuls, but this situation now placed them under the sole authority of Augustus.
In addition, the credit was given to Augustus for each subsequent Roman military victory after this time, because the majority of Rome's armies were stationed in imperial provinces commanded by Augustus through the legatus who were deputies of the princeps in the provinces. Moreover, if a battle was fought in a Senatorial province, Augustus's proconsular imperium maius allowed him to take command of (or credit for) any major military victory. This meant that Augustus was the only individual able to receive a triumph, a tradition that began with Romulus, Rome's first King and first triumphant general. Lucius Cornelius Balbus was the last man outside Augustus's family to receive this award, in 19 BC. Tiberius, Augustus's eldest stepson by Livia, was the only other general to receive a triumph—for victories in Germania in 7 BC.
Normally during republican times, the powers Augustus held even after the Second Settlement would have been split between several people, who would each exercise them with the assistance of a colleague and for a specific period of time. Augustus held them all at once by himself, and with no time limits; even those that nominally had time limits were automatically renewed whenever they lapsed.
### Conspiracy.
Many of the political subtleties of the Second Settlement seem to have evaded the comprehension of the Plebeian class, who were Augustus's greatest supporters and clientele. This caused them to insist upon Augustus's participation in imperial affairs from time to time. Augustus failed to stand for election as consul in 22 BC, and fears arose once again that he was being forced from power by the aristocratic Senate. In 22, 21, and 19 BC, the people rioted in response, and only allowed a single consul to be elected for each of those years, ostensibly to leave the other position open for Augustus.
Likewise, there was a food shortage in Rome in 22 BC which sparked panic, while many urban plebs called for Augustus to take on dictatorial powers to personally oversee the crisis. After a theatrical display of refusal before the Senate, Augustus finally accepted authority over Rome's grain supply "by virtue of his proconsular "imperium"", and ended the crisis almost immediately. It was not until AD 8 that a food crisis of this sort prompted Augustus to establish a "praefectus annonae", a permanent prefect who was in charge of procuring food supplies for Rome.
There were some who were concerned by the expansion of powers granted to Augustus by the Second Settlement, and this came to a head with the apparent conspiracy of Fannius Caepio. Some time prior to 1 September 22 BC, a certain Castricius provided Augustus with information about a conspiracy led by Fannius Caepio. Murena, the outspoken Consul who defended Primus in the Marcus Primus Affair, was named among the conspirators. The conspirators were tried in absentia with Tiberius acting as prosecutor; the jury found them guilty, but it was not a unanimous verdict. All the accused were sentenced to death for treason and executed as soon as they were captured—without ever giving testimony in their defence. Augustus ensured that the facade of Republican government continued with an effective cover-up of the events.
In 19 BC, the Senate granted Augustus a form of 'general consular imperium', which was probably 'imperium consulare maius', like the proconsular powers that he received in 23 BC. Like his tribune authority, the consular powers were another instance of gaining power from offices that he did not actually hold. In addition, Augustus was allowed to wear the consul's insignia in public and before the Senate, as well as to sit in the symbolic chair between the two consuls and hold the fasces, an emblem of consular authority. This seems to have assuaged the populace; regardless of whether or not Augustus was a consul, the importance was that he both appeared as one before the people and could exercise consular power if necessary. On 6 March 12 BC, after the death of Lepidus, he additionally took up the position of "pontifex maximus", the high priest of the college of the Pontiffs, the most important position in Roman religion. On 5 February 2 BC, Augustus was also given the title "pater patriae", or "father of the country".
### Stability and staying power.
A final reason for the Second Settlement was to give the Principate constitutional stability and staying power in case something happened to Princeps Augustus. His illness of early 23 BC and the Caepio conspiracy showed that the regime's existence hung by the thin thread of the life of one man, Augustus himself, who suffered from several severe and dangerous illnesses throughout his life. If he were to die from natural causes or fall victim to assassination, Rome could be subjected to another round of civil war. The memories of Pharsalus, the Ides of March, the proscriptions, Philippi, and Actium, barely twenty-five years distant, were still vivid in the minds of many citizens. Proconsular imperium was conferred upon Agrippa for five years, similar to Augustus's power, in order to accomplish this constitutional stability. The exact nature of the grant is uncertain but it probably covered Augustus's imperial provinces, east and west, perhaps lacking authority over the provinces of the Senate. That came later, as did the jealously guarded tribunicia potestas. Augustus's accumulation of powers was now complete.
### War and expansion.
Augustus chose "Imperator" ("victorious commander") to be his first name, since he wanted to make an emphatically clear connection between himself and the notion of victory, and consequently became known as "Imperator Caesar Divi Filius Augustus". By the year 13, Augustus boasted 21 occasions where his troops proclaimed "imperator" as his title after a successful battle. Almost the entire fourth chapter in his publicly released memoirs of achievements known as the "Res Gestae" was devoted to his military victories and honors.
Augustus also promoted the ideal of a superior Roman civilization with a task of ruling the world (to the extent to which the Romans knew it), a sentiment embodied in words that the contemporary poet Virgil attributes to a legendary ancestor of Augustus: "tu regere imperio populos, Romane, memento"—"Roman, remember by your strength to rule the Earth's peoples!" The impulse for expansionism was apparently prominent among all classes at Rome, and it is accorded divine sanction by Virgil's Jupiter in Book 1 of the "Aeneid", where Jupiter promises Rome "imperium sine fine", "sovereignty without end".
By the end of his reign, the armies of Augustus had conquered northern Hispania (modern Spain and Portugal) and the Alpine regions of Raetia and Noricum (modern Switzerland, Bavaria, Austria, Slovenia), Illyricum and Pannonia (modern Albania, Croatia, Hungary, Serbia, etc.), and had extended the borders of the Africa Province to the east and south. Judea was added to the province of Syria when Augustus deposed Herod Archelaus, successor to client king Herod the Great (73–4 BC). Syria (like Egypt after Antony) was governed by a high prefect of the equestrian class rather than by a proconsul or legate of Augustus.
Again, no military effort was needed in 25 BC when Galatia (modern Turkey) was converted to a Roman province shortly after Amyntas of Galatia was killed by an avenging widow of a slain prince from Homonada. The rebellious tribes of Asturias and Cantabria in modern-day Spain were finally quelled in 19 BC, and the territory fell under the provinces of Hispania and Lusitania. This region proved to be a major asset in funding Augustus's future military campaigns, as it was rich in mineral deposits that could be fostered in Roman mining projects, especially the very rich gold deposits at Las Medulas.
Conquering the peoples of the Alps in 16 BC was another important victory for Rome, since it provided a large territorial buffer between the Roman citizens of Italy and Rome's enemies in Germania to the north. Horace dedicated an ode to the victory, while the monumental Trophy of Augustus near Monaco was built to honor the occasion. The capture of the Alpine region also served the next offensive in 12 BC, when Tiberius began the offensive against the Pannonian tribes of Illyricum, and his brother Nero Claudius Drusus moved against the Germanic tribes of the eastern Rhineland. Both campaigns were successful, as Drusus's forces reached the Elbe River by 9 BC—though he died shortly after by falling off his horse. It was recorded that the pious Tiberius walked in front of his brother's body all the way back to Rome.
To protect Rome's eastern territories from the Parthian Empire, Augustus relied on the client states of the east to act as territorial buffers and areas that could raise their own troops for defense. To ensure security of the Empire's eastern flank, Augustus stationed a Roman army in Syria, while his skilled stepson Tiberius negotiated with the Parthians as Rome's diplomat to the East. Tiberius was responsible for restoring Tigranes V to the throne of the Kingdom of Armenia.
Yet arguably his greatest diplomatic achievement was negotiating with Phraates IV of Parthia (37–2 BC) in 20 BC for the return of the battle standards lost by Crassus in the Battle of Carrhae, a symbolic victory and great boost of morale for Rome. Werner Eck claims that this was a great disappointment for Romans seeking to avenge Crassus's defeat by military means. However, Maria Brosius explains that Augustus used the return of the standards as propaganda symbolizing the submission of Parthia to Rome. The event was celebrated in art such as the breastplate design on the statue Augustus of Prima Porta and in monuments such as the Temple of Mars Ultor ('Mars the Avenger') built to house the standards.
Parthia had always posed a threat to Rome in the east, but the real battlefront was along the Rhine and Danube rivers. Before the final fight with Antony, Octavian's campaigns against the tribes in Dalmatia were the first step in expanding Roman dominions to the Danube. Victory in battle was not always a permanent success, as newly conquered territories were constantly retaken by Rome's enemies in Germania.
A prime example of Roman loss in battle was the Battle of Teutoburg Forest in AD 9, where three entire legions led by Publius Quinctilius Varus were destroyed by Arminius, leader of the Cherusci, an apparent Roman ally. Augustus retaliated by dispatching Tiberius and Drusus to the Rhineland to pacify it, which had some success although the battle of AD 9 brought the end to Roman expansion into Germany. Roman general Germanicus took advantage of a Cherusci civil war between Arminius and Segestes; at the Battle of Idistaviso in AD 16, they defeated Arminius, who fled but was killed later in 21 due to treachery.
## Death and succession.
The illness of Augustus in 23 BC brought the problem of succession to the forefront of political issues and the public. To ensure stability, he needed to designate an heir to his unique position in Roman society and government. This was to be achieved in small, undramatic, and incremental ways that did not stir senatorial fears of monarchy. If someone was to succeed to Augustus's unofficial position of power, he would have to earn it through his own publicly proven merits.
Some Augustan historians argue that indications pointed toward his sister's son Marcellus, who had been quickly married to Augustus's daughter Julia the Elder. Other historians dispute this due to Augustus's will being read aloud to the Senate while he was seriously ill in 23 BC, instead indicating a preference for Marcus Agrippa, who was Augustus's second in charge and arguably the only one of his associates who could have controlled the legions and held the Empire together.
After the death of Marcellus in 23 BC, Augustus married his daughter to Agrippa. This union produced five children, three sons and two daughters: Gaius Caesar, Lucius Caesar, Vipsania Julia, Agrippina, and Agrippa Postumus, so named because he was born after Marcus Agrippa died. Shortly after the Second Settlement, Agrippa was granted a five-year term of administering the eastern half of the Empire with the "imperium" of a proconsul and the same "tribunicia potestas" granted to Augustus (although not trumping Augustus's authority), his seat of governance stationed at Samos in the eastern Aegean. This granting of power showed Augustus's favor for Agrippa, but it was also a measure to please members of his Caesarian party by allowing one of their members to share a considerable amount of power with him.
Augustus's intent became apparent to make Gaius and Lucius Caesar his heirs when he adopted them as his own children. He took the consulship in 5 and 2 BC so that he could personally usher them into their political careers, and they were nominated for the consulships of AD 1 and 4. Augustus also showed favor to his stepsons, Livia's children from her first marriage Nero Claudius Drusus Germanicus (henceforth referred to as Drusus) and Tiberius Claudius (henceforth Tiberius), granting them military commands and public office, though seeming to favor Drusus. After Agrippa died in 12 BC, Tiberius was ordered to divorce his own wife Vipsania Agrippina and marry Agrippa's widow, Augustus's daughter Julia—as soon as a period of mourning for Agrippa had ended. Drusus's marriage to Augustus's niece Antonia was considered an unbreakable affair, whereas Vipsania was "only" the daughter of the late Agrippa from his first marriage.
Tiberius shared in Augustus's tribune powers as of 6 BC, but shortly thereafter went into retirement, reportedly wanting no further role in politics while he exiled himself to Rhodes. No specific reason is known for his departure, though it could have been a combination of reasons, including a failing marriage with Julia, as well as a sense of envy and exclusion over Augustus's apparent favouring of his young grandchildren-turned-sons Gaius and Lucius. (Gaius and Lucius joined the college of priests at an early age, were presented to spectators in a more favorable light, and were introduced to the army in Gaul.)
After the early deaths of both Lucius and Gaius in AD 2 and 4 respectively, and the earlier death of his brother Drusus (9 BC), Tiberius was recalled to Rome in June AD 4, where he was adopted by Augustus on the condition that he, in turn, adopt his nephew Germanicus. This continued the tradition of presenting at least two generations of heirs. In that year, Tiberius was also granted the powers of a tribune and proconsul, emissaries from foreign kings had to pay their respects to him, and by AD 13 was awarded with his second triumph and equal level of "imperium" with that of Augustus.
The only other possible claimant as heir was Postumus Agrippa, who had been exiled by Augustus in AD 7, his banishment made permanent by senatorial decree, and Augustus officially disowned him. He certainly fell out of Augustus's favor as an heir; the historian Erich S. Gruen notes various contemporary sources that state Postumus Agrippa was a "vulgar young man, brutal and brutish, and of depraved character".
On 19 August AD 14, Augustus died while visiting Nola where his father had died. Both Tacitus and Cassius Dio wrote that Livia was rumored to have brought about Augustus's death by poisoning fresh figs. This element features in many modern works of historical fiction pertaining to Augustus's life, but some historians view it as likely to have been a salacious fabrication made by those who had favoured Postumus as heir, or other of Tiberius's political enemies. Livia had long been the target of similar rumors of poisoning on the behalf of her son, most or all of which are unlikely to have been true.
Alternatively, it is possible that Livia did supply a poisoned fig (she did cultivate a variety of fig named for her that Augustus is said to have enjoyed), but did so as a means of assisted suicide rather than murder. Augustus's health had been in decline in the months immediately before his death, and he had made significant preparations for a smooth transition in power, having at last reluctantly settled on Tiberius as his choice of heir. It is likely that Augustus was not expected to return alive from Nola, but it seems that his health improved once there; it has therefore been speculated that Augustus and Livia conspired to end his life at the anticipated time, having committed all political process to accepting Tiberius, in order to not endanger that transition.
Augustus's famous last words were, "Have I played the part well? Then applaud as I exit" ()—referring to the play-acting and regal authority that he had put on as emperor. Publicly, though, his last words were, "Behold, I found Rome of clay, and leave her to you of marble" (). An enormous funerary procession of mourners traveled with Augustus's body from Nola to Rome, and on the day of his burial all public and private businesses closed for the day. Tiberius and his son Drusus delivered the eulogy while standing atop two "rostra". Augustus's body was coffin-bound and cremated on a pyre close to his mausoleum. It was proclaimed that Augustus joined the company of the gods as a member of the Roman pantheon.
Historian D. C. A. Shotter states that Augustus's policy of favoring the Julian family line over the Claudian might have afforded Tiberius sufficient cause to show open disdain for Augustus after the latter's death; instead, Tiberius was always quick to rebuke those who criticized Augustus. Shotter suggests that Augustus's deification obliged Tiberius to suppress any open resentment that he might have harbored, coupled with Tiberius's "extremely conservative" attitude towards religion. Also, historian R. Shaw-Smith points to letters of Augustus to Tiberius which display affection towards Tiberius and high regard for his military merits. Shotter states that Tiberius focused his anger and criticism on Gaius Asinius Gallus (for marrying Vipsania after Augustus forced Tiberius to divorce her), as well as toward the two young Caesars, Gaius and Lucius—instead of Augustus, the real architect of his divorce and imperial demotion.
## Legacy.
Augustus's reign laid the foundations of a regime that lasted, in one form or another, for nearly fifteen hundred years through the ultimate decline of the Western Roman Empire and until the Fall of Constantinople in 1453. Both his adoptive surname, Caesar, and his title "Augustus" became the permanent titles of the rulers of the Roman Empire for fourteen centuries after his death, in use both at Old Rome and at New Rome. In many languages, "Caesar" became the word for "emperor", as in the German "Kaiser" and in the Bulgarian and subsequently Russian "Tsar" (sometimes "Csar" or "Czar"). The cult of "Divus Augustus" continued until the state religion of the Empire was changed to Christianity in 391 by Theodosius I. Consequently, there are many excellent statues and busts of the first emperor. He had composed an account of his achievements, the "Res Gestae Divi Augusti", to be inscribed in bronze in front of his mausoleum. Copies of the text were inscribed throughout the Empire upon his death. The inscriptions in Latin featured translations in Greek beside it, and were inscribed on many public edifices, such as the temple in Ankara dubbed the "Monumentum Ancyranum", called the "queen of inscriptions" by historian Theodor Mommsen.
The "Res Gestae" is the only work to have survived from antiquity, though Augustus is also known to have composed poems entitled "Sicily", "Epiphanus", and "Ajax", an autobiography of 13 books, a philosophical treatise, and a written rebuttal to Brutus's "Eulogy of Cato". Historians are able to analyze excerpts of letters penned by Augustus, preserved in other works, to others for additional facts or clues about his personal life.
Many consider Augustus to be Rome's greatest emperor; his policies certainly extended the Empire's life span and initiated the celebrated "Pax Romana" or "Pax Augusta". The Roman Senate wished subsequent emperors to "be more fortunate than Augustus and better than Trajan". Augustus was intelligent, decisive, and a shrewd politician, but he was not perhaps as charismatic as Julius Caesar and was influenced on occasion by Livia (sometimes for the worse). Nevertheless, his legacy proved more enduring. The city of Rome was utterly transformed under Augustus, with Rome's first institutionalized police force, fire fighting force, and the establishment of the municipal prefect as a permanent office. The police force was divided into cohorts of 500 men each, while the units of firemen ranged from 500 to 1,000 men each, with 7 units assigned to 14 divided city sectors.
A "praefectus vigilum", or "Prefect of the Watch" was put in charge of the vigiles, Rome's fire brigade and police. With Rome's civil wars at an end, Augustus was also able to create a standing army for the Roman Empire, fixed at a size of 28 legions of about 170,000 soldiers. This was supported by numerous auxiliary units of 500 non-citizen soldiers each, often recruited from recently conquered areas.
With his finances securing the maintenance of roads throughout Italy, Augustus also installed an official courier system of relay stations overseen by a military officer known as the "praefectus vehiculorum". Besides the advent of swifter communication among Italian polities, his extensive building of roads throughout Italy also allowed Rome's armies to march swiftly and at an unprecedented pace across the country. In the year 6 Augustus established the "aerarium militare", donating 170 million sesterces to the new military treasury that provided for both active and retired soldiers.
One of the most enduring institutions of Augustus was the establishment of the Praetorian Guard in 27 BC, originally a personal bodyguard unit on the battlefield that evolved into an imperial guard as well as an important political force in Rome.They had the power to intimidate the Senate, install new emperors, and depose ones they disliked; the last emperor they served was Maxentius, as it was Constantine I who disbanded them in the early 4th century and destroyed their barracks, the Castra Praetoria.
Although the most powerful individual in the Roman Empire, Augustus wished to embody the spirit of Republican virtue and norms. He also wanted to relate to and connect with the concerns of the plebs and lay people. He achieved this through various means of generosity and a cutting back of lavish excess. In the year 29 BC, Augustus gave 400 sesterces (equal to 1/10 of a Roman pound of gold) each to 250,000 citizens, 1,000 sesterces each to 120,000 veterans in the colonies, and spent 700 million sesterces in purchasing land for his soldiers to settle upon. He also restored 82 different temples to display his care for the Roman pantheon of deities. In 28 BC, he melted down 80 silver statues erected in his likeness and in honor of him, an attempt of his to appear frugal and modest.
The longevity of Augustus's reign and its legacy to the Roman world should not be overlooked as a key factor in its success. As Tacitus wrote, the younger generations alive in AD 14 had never known any form of government other than the Principate. Had Augustus died earlier (in 23 BC, for instance), matters might have turned out differently. The attrition of the civil wars on the old Republican oligarchy and the longevity of Augustus, therefore, must be seen as major contributing factors in the transformation of the Roman state into a de facto monarchy in these years. Augustus's own experience, his patience, his tact, and his political acumen also played their parts. He directed the future of the Empire down many lasting paths, from the existence of a standing professional army stationed at or near the frontiers, to the dynastic principle so often employed in the imperial succession, to the embellishment of the capital at the emperor's expense. Augustus's ultimate legacy was the peace and prosperity the Empire enjoyed for the next two centuries under the system he initiated. His memory was enshrined in the political ethos of the Imperial age as a paradigm of the good emperor. Every emperor of Rome adopted his name, Caesar Augustus, which gradually lost its character as a name and eventually became a title. The Augustan era poets Virgil and Horace praised Augustus as a defender of Rome, an upholder of moral justice, and an individual who bore the brunt of responsibility in maintaining the empire.
However, for his rule of Rome and establishing the principate, Augustus has also been subjected to criticism throughout the ages. The contemporary Roman jurist Marcus Antistius Labeo (d. AD 10/11), fond of the days of pre-Augustan republican liberty in which he had been born, openly criticized the Augustan regime. In the beginning of his "Annals", the Roman historian Tacitus (c. 56–c.117) wrote that Augustus had cunningly subverted Republican Rome into a position of slavery. He continued to say that, with Augustus's death and swearing of loyalty to Tiberius, the people of Rome simply traded one slaveholder for another. Tacitus, however, records two contradictory but common views of Augustus:
According to the second opposing opinion:
In a 2006 biography on Augustus, Anthony Everitt asserts that through the centuries, judgments on Augustus's reign have oscillated between these two extremes but stresses that:
Tacitus was of the belief that Nerva (r. 96–98) successfully "mingled two formerly alien ideas, principate and liberty". The 3rd-century historian Cassius Dio acknowledged Augustus as a benign, moderate ruler, yet like most other historians after the death of Augustus, Dio viewed Augustus as an autocrat. The poet Marcus Annaeus Lucanus (AD 39–65) was of the opinion that Caesar's victory over Pompey and the fall of Cato the Younger (95 BC–46 BC) marked the end of traditional liberty in Rome; historian Chester Starr writes of his avoidance of criticizing Augustus, "perhaps Augustus was too sacred a figure to accuse directly."
The Anglo-Irish writer Jonathan Swift (1667–1745), in his "Discourse on the Contests and Dissentions in Athens and Rome", criticized Augustus for installing tyranny over Rome, and likened what he believed Great Britain's virtuous constitutional monarchy to Rome's moral Republic of the 2nd century BC. In his criticism of Augustus, the admiral and historian Thomas Gordon (1658–1741) compared Augustus to the puritanical tyrant Oliver Cromwell (1599–1658). Thomas Gordon and the French political philosopher Montesquieu (1689–1755) both remarked that Augustus was a coward in battle. In his "Memoirs of the Court of Augustus", the Scottish scholar Thomas Blackwell (1701–1757) deemed Augustus a Machiavellian ruler, "a bloodthirsty vindicative usurper", "wicked and worthless", "a mean spirit", and a "tyrant".
### Revenue reforms.
Augustus's public revenue reforms had a great impact on the subsequent success of the Empire. Augustus brought a far greater portion of the Empire's expanded land base under consistent, direct taxation from Rome, instead of exacting varying, intermittent, and somewhat arbitrary tributes from each local province as Augustus's predecessors had done. This reform greatly increased Rome's net revenue from its territorial acquisitions, stabilized its flow, and regularized the financial relationship between Rome and the provinces, rather than provoking fresh resentments with each new arbitrary exaction of tribute.
The measures of taxation in the reign of Augustus were determined by population census, with fixed quotas for each province. Citizens of Rome and Italy paid indirect taxes, while direct taxes were exacted from the provinces. Indirect taxes included a 4% tax on the price of slaves, a 1% tax on goods sold at auction, and a 5% tax on the inheritance of estates valued at over 100,000 sesterces by persons other than the next of kin.
An equally important reform was the abolition of private tax farming, which was replaced by salaried civil service tax collectors. Private contractors who collected taxes for the State were the norm in the Republican era. Some of them were powerful enough to influence the number of votes for men running for offices in Rome. These tax farmers called publicans were infamous for their depredations, great private wealth, and the right to tax local areas.
The use of Egypt's immense land rents to finance the Empire's operations resulted from Augustus's conquest of Egypt and the shift to a Roman form of government. As it was effectively considered Augustus's private property rather than a province of the Empire, it became part of each succeeding emperor's patrimonium.
Instead of a legate or proconsul, Augustus installed a prefect from the equestrian class to administer Egypt and maintain its lucrative seaports; this position became the highest political achievement for any equestrian besides becoming Prefect of the Praetorian Guard. The highly productive agricultural land of Egypt yielded enormous revenues that were available to Augustus and his successors to pay for public works and military expeditions. During his reign the circus games resulted in the killing of 3,500 elephants.
### Month of August.
The month of August (Latin: "Augustus") is named after Augustus; until his time it was called Sextilis (named so because it had been the sixth month of the original Roman calendar and the Latin word for six is "sex"). Commonly repeated lore has it that August has 31 days because Augustus wanted his month to match the length of Julius Caesar's July, but this is an invention of the 13th century scholar Johannes de Sacrobosco. Sextilis in fact had 31 days before it was renamed, and it was not chosen for its length (see Julian calendar).
According to a "senatus consultum" quoted by Macrobius, Sextilis was renamed to honor Augustus because several of the most significant events in his rise to power, culminating in the fall of Alexandria, fell in that month.
### Creation of "Italia".
Roman Italy was established by Augustus in 7 BC with the Latin name "Italia". This was the first time that the Italian peninsula was united administratively and politically under the same name. Due to this act, Augustus was called the "Father of Italy" by Italian historians such as G. Giannelli.
### Building projects.
On his deathbed, Augustus boasted "I found a Rome of bricks; I leave to you one of marble." Although there is some truth in the literal meaning of this, Cassius Dio asserts that it was a metaphor for the Empire's strength. Marble could be found in buildings of Rome before Augustus, but it was not extensively used as a building material until the reign of Augustus.
Although this did not apply to the Subura slums, which were still as rickety and fire-prone as ever, he did leave a mark on the monumental topography of the centre and of the Campus Martius, with the Ara Pacis (Altar of Peace) and monumental sundial, whose central gnomon was an obelisk taken from Egypt. The relief sculptures decorating the Ara Pacis visually augmented the written record of Augustus's triumphs in the "Res Gestae". Its reliefs depicted the imperial pageants of the praetorians, the Vestals, and the citizenry of Rome.
He also built the Temple of Caesar, the Baths of Agrippa, and the Forum of Augustus with its Temple of Mars Ultor. Other projects were either encouraged by him, such as the Theatre of Balbus, and Agrippa's construction of the Pantheon, or funded by him in the name of others, often relations (e.g. Portico of Octavia, Theatre of Marcellus). Even his Mausoleum of Augustus was built before his death to house members of his family. To celebrate his victory at the Battle of Actium, the Arch of Augustus was built in 29 BC near the entrance of the Temple of Castor and Pollux, and widened in 19 BC to include a triple-arch design.
After the death of Agrippa in 12 BC, a solution had to be found in maintaining Rome's water supply system. This came about because it was overseen by Agrippa when he served as aedile, and was even funded by him afterwards when he was a private citizen paying at his own expense. In that year, Augustus arranged a system where the Senate designated three of its members as prime commissioners in charge of the water supply and to ensure that Rome's aqueducts did not fall into disrepair.
In the late Augustan era, the commission of five senators called the "curatores locorum publicorum iudicandorum" (translated as "Supervisors of Public Property") was put in charge of maintaining public buildings and temples of the state cult. Augustus created the senatorial group of the "curatores viarum" (translated as "Supervisors for Roads") for the upkeep of roads; this senatorial commission worked with local officials and contractors to organize regular repairs.
The Corinthian order of architectural style originating from ancient Greece was the dominant architectural style in the age of Augustus and the imperial phase of Rome. Suetonius once commented that Rome was unworthy of its status as an imperial capital, yet Augustus and Agrippa set out to dismantle this sentiment by transforming the appearance of Rome upon the classical Greek model.
#### Residences.
The official residence of Augustus was the "Domus Augusti" on the Palatine which he made into a palace after buying it in 41/40 BC. He had other residences such as the "horti maecenati" in Rome where Augustus preferred to stay whenever he became ill and which Maecenas left to him in his will in 8 BC. The great villa of Vedius Pollio at Posilipo near Naples was beqeathed (probably forced) to him in 15 BC.
Augustus built the Palazzo a Mare palace on Capri. He also built the immense Villa Giulia on the island of Ventotene as a summer residence early in his reign. The family home of Augustus was probably the villa at Somma Vesuviana, Nola. This was the location where he died and where his father also died.
## Physical appearance and official images.
His biographer Suetonius, writing about a century after Augustus's death, described his appearance as: "... unusually handsome and exceedingly graceful at all periods of his life, though he cared nothing for personal adornment. He was so far from being particular about the dressing of his hair, that he would have several barbers working in a hurry at the same time, and as for his beard he now had it clipped and now shaved, while at the very same time he would either be reading or writing something ... He had clear, bright eyes ... His teeth were wide apart, small, and ill-kept; his hair was slightly curly and inclined to golden; his eyebrows met. His ears were of moderate size, and his nose projected a little at the top and then bent ever so slightly inward. His complexion was between dark and fair. He was short of stature, although Julius Marathus, his freedman and keeper of his records, says that he was five feet and nine inches (just under 5 ft. 7 in., or 1.70 meters, in modern height measurements), but this was concealed by the fine proportion and symmetry of his figure, and was noticeable only by comparison with some taller person standing beside him...", adding that "his shoes [were] somewhat high-soled, to make him look taller than he really was". Scientific analysis of traces of paint found in his official statues show that he most likely had light brown hair and eyes (his hair and eyes were depicted as the same color).
His official images were very tightly controlled and idealized, drawing from a tradition of Hellenistic royal portraiture rather than the tradition of realism in Roman portraiture. He first appeared on coins at the age of 19, and from about 29 BC "the explosion in the number of Augustan portraits attests a concerted propaganda campaign aimed at dominating all aspects of civil, religious, economic and military life with Augustus's person." The early images did indeed depict a young man, but although there were gradual changes his images remained youthful until he died in his seventies, by which time they had "a distanced air of ageless majesty". Among the best known of many surviving portraits are the Augustus of Prima Porta, the image on the Ara Pacis, and the Via Labicana Augustus, which depicts him in his role as "pontifex maximus". Several cameo portraits include the Blacas Cameo and "Gemma Augustea".

</doc>
<doc id="1274" url="https://en.wikipedia.org/wiki?curid=1274" title="Geography of Antarctica">
Geography of Antarctica

The geography of Antarctica is dominated by its south polar location and, thus, by ice. The Antarctic continent, located in the Earth's southern hemisphere, is centered asymmetrically around the South Pole and largely south of the Antarctic Circle. It is washed by the Southern (or Antarctic) Ocean or, depending on definition, the southern Pacific, Atlantic, and Indian Oceans. It has an area of more than 14 million km2.
Some 98% of Antarctica is covered by the Antarctic ice sheet, the world's largest ice sheet and also its largest reservoir of fresh water. Averaging at least 1.6 km thick, the ice is so massive that it has depressed the continental bedrock in some areas more than 2.5 km below sea level; subglacial lakes of liquid water also occur (e.g., Lake Vostok). Ice shelves and rises populate the ice sheet on the periphery.
In September 2018, researchers at the National Geospatial-Intelligence Agency released a high resolution terrain map (detail down to the size of a car, and less in some areas) of Antarctica, named the "Reference Elevation Model of Antarctica" (REMA).
## Regions.
Physically, Antarctica is divided in two by Transantarctic Mountains close to the neck between the Ross Sea and the Weddell Sea. Western Antarctica and Eastern Antarctica correspond roughly to the eastern and western hemispheres relative to the Greenwich meridian. This usage has been regarded as Eurocentric by some, and the alternative terms Lesser Antarctica and Greater Antarctica (respectively) are sometimes preferred.
Lesser Antarctica is covered by the West Antarctic Ice Sheet. There has been some concern about this ice sheet, because there is a small chance that it will collapse. If it does, ocean levels would rise by a few metres in a very short period of time.
## Volcanoes.
Volcanoes that occur underneath glacial ice sheets are known by the term "Glaciovolcanism", or subglacial volcanoes. An article published in 2017 claims that researchers from Edinburgh University recently discovered 91 new volcanoes below the Antarctic ice sheet, adding to the 47 volcanoes that were already known. As of today, there have been 138 possible volcanoes identified in West Antarctica. There is limited knowledge about West Antarctic Volcanoes due to the presence of the West Antarctic Ice Sheet, which heavily covers the West Antarctic Rift System -- a likely hub for volcanic activity. Researchers find it difficult to properly identify volcanic activity due to the comprehensive ice covering.
East Antarctica is significantly larger than West Antarctica, and similarly remains widely unexplored in terms of its volcanic potential. While there are some indications that there is volcanic activity under the East Antarctic Ice Sheet, there is not a significant amount of present information on the subject.
Mount Erebus is one of the most notable sites in the study of Antarctic Volcanism, in that it is the southernmost historically active volcanic site on the planet.
Deception Island is another active Antarctic volcano. It is one of the most protected areas in the Antarctic, given its situation between the South Shetland Islands and the Antarctic Peninsula. As the most active volcano in the Antarctic peninsula, it has been studied closely since its initial discovery in 1820.
There are four volcanoes on the mainland of Antarctica that are
considered to be active on the basis of observed fumarolic activity or 
"recent" tephra deposits: 
Mount Melbourne (2,730 m) (74°21'S., 164°42'E.), a stratovolcano; 
Mount Berlin (3,500 m) (76°03'S., 135°52'W.), a stratovolcano; 
Mount Kauffman (2,365 m) (75°37'S., 132°25'W.), a stratovolcano; and 
Mount Hampton (3,325 m) (76°29'S., 125°48'W.), a volcanic caldera.
Mount Rittmann (2,600 m) (73.45°S 165.5° E), a volcanic caldera.
Several volcanoes on offshore islands have records of historic activity.
Mount Erebus (3,795 m), a stratovolcano on
Ross Island with 10 known eruptions and 1 suspected eruption.
On the opposite side of the continent, 
Deception Island
(62°57'S., 60°38'W.), a volcanic caldera with 10 known
and 4 suspected eruptions, have been the most active.
Buckle Island in the Balleny Islands (66°50'S., 163°12'E.), 
Penguin Island (62°06'S., 57°54'W.), 
Paulet Island (63°35'S., 55°47'W.), and 
Lindenberg Island (64°55'S., 59°40'W.) are also 
considered to be active. In 2017, the researchers of Edinburgh University discovered 91 underwater volcanoes under West Antarctica.
### Glaciovolcanism.
The definition of Glaciovolcanism is “the interactions of magma with ice in all its forms, including snow, firn and any meltwater.” It defines a special field of volcanic that is specifically centered around ice and ice melt. This field of science is less than 100 years old, and thus continuously makes new discoveries. Glaciovolcanism is characterized by three kinds of eruptions: sub-glacial eruptions, supraglacial volcanism, and ice-marginal volcanism.
The study of glaciovolcanism is vital to the understanding of ice sheet formation. It is also a valuable tool to predict volcanic hazards, such as the ash hazard following the Eyjafjallajökull eruption in Iceland.
### Marie Byrd Land.
The Marie Byrd Land is an incredibly large portion of West Antarctica, consisting of the Area below the Antarctic Peninsula. The Marie Byrd land is a large formation of volcanic rock, characterized by 18 exposed and subglacial volcanoes. 16 of the 18 volcanoes are entirely covered by the antarctic ice sheet. There have been no eruptions recorded from any of the volcanoes in this area, however scientists believe that some of the volcanoes may be potentially active.
### Activity.
Scientists and researchers debate whether or not the 138 identified possible volcanoes are active or dormant. It is very hard to definitively say, given that many of these volcanic structures are buried underneath several kilometers of ice. However, ash layers within the West Antarctic Ice Sheet, as well as deformations in the ice surface indicate that the West Antarctic Rift System could be active and contain erupting volcanoes. Additionally, seismic activity in the region hints at magma movement beneath the crust, a sign of volcanic activity. Despite this, however, there is not yet definitive evidence of presently active volcanoes.
Subglacial volcanism is often characterized by ice melt and subglacial water. Though there are other sources of subglacial water, such as geothermal heat, it almost always is a condition of volcanism. Scientists remain uncertain about the presence of water underneath the West Antarctic Ice Sheet, with some claiming to have found evidence indicating the existence.
### Conditions of Formation.
In West Antarctica's Marie Byrd Land, volcanoes are typically composed of alkaline and basaltic lava. Sometimes, the volcanoes are entirely basaltic in composition. Due to the geographic similarity of the Marie Byrd Land, it is believed that the volcanoes in the West African Rift System are also composed of basalt.
Above-ice basaltic volcanoes, also known as subaerial basaltic volcanoes, generally form in tall, broad cone shapes. Since they are formed from repeated piling of liquid magma sourced from the center, they spread widely and grow upwards relatively slowly. However, West Antarctic Volcanoes form underneath ice sheets, and are thus categorized as subglacial volcanoes. Subglacial volcanoes that are monogenetic are far more narrow, steeper, flat topped structures. Polygenetic subglacial volcanoes have a wider variety of shapes and sizes due to being made up of many different eruptions. Often, they look more cone shaped, like stratovolcanoes.
### Hazards.
#### Hazardous ash.
Little has been studied about the implications of volcanic ash from eruptions within the Antarctic Circle. It is likely that an eruption at lower latitudes would cause global health and aviation hazards due to ash disbursement. The clockwise air circulation around the low pressure system at the South Pole forces air upwards, hypothetically sending ash upwards towards the Stratospheric jet streams, and thus quickly dispersing it throughout the globe.
#### Melting ice.
Recently, in 2017, a study found evidence of subglacial volcanic activity within the West Antarctic Ice Sheet. This activity poses a threat to the stability of the Ice Sheet, as volcanic activity leads to increased melting. This could possibly plunge the West Antarctic Ice Sheet into a positive feedback loop of rising temperatures and increased melting.
## Canyons.
There are three vast canyons that run for hundreds of kilometers, cutting through tall mountains. None of the canyons are visible at the snow-covered surface of the continent since they are buried under hundreds of meters of ice. The largest of the canyons is called Foundation Trough and is over 350km long and 35km wide. The Patuxent Trough is more than 300km long and over 15km wide, while the Offset Rift Basin is 150km long and 30km wide. These three troughs all lie under and cross the so-called "ice divide" - the high ice ridge that runs all the way from the South Pole out towards the coast of West Antarctica.
## West Antarctica.
West Antarctica is the smaller part of the continent, , divided into:
### Ice shelves.
Larger ice shelves are:
For all ice shelves see List of Antarctic ice shelves.
### Islands.
For a list of all Antarctic islands see List of Antarctic and sub-Antarctic islands.
## East Antarctica.
East Antarctica is the larger part of the continent, , both the South Magnetic Pole and geographic South Pole are situated here. Divided into:
### Ice shelves.
Larger ice shelves are:
For all ice shelves see List of Antarctic ice shelves.
### Islands.
For a list of all Antarctic islands see List of Antarctic and sub-Antarctic islands.
## Territorial landclaims.
Seven nations have made official Territorial claims in Antarctica.

</doc>
<doc id="1276" url="https://en.wikipedia.org/wiki?curid=1276" title="Economy of Antarctica">
Economy of Antarctica



</doc>
<doc id="1277" url="https://en.wikipedia.org/wiki?curid=1277" title="Government of Antarctica">
Government of Antarctica



</doc>
